{
  "hash": "0d2317e0b07e12d4cb37547abc3eabde",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Making a \\\"Visible\\\" Neural Network\"\nauthor: \"Daniel Kick\"\ndate: \"2023-09-15\"\ncategories: \n  - code\n  - advanced\ndraft: False\nfreeze: true\n---\n\n\n## What's a visual neural network?\n\nIn most neural networks, neurons are not parametric in the same way that linear models are. In a image recognition model there may be neuron which *functions* to detects edges but when the model is set up initially one can't point to a neuron and say what it will do or represent. This can make interpreting the weights in a model tricky.\n\nVisible neural networks (VNN) are one way to get around this problem by making the structure of a model reflect the process being modeled. In a VNN, influential sub-components may be interpreted as implicating the process they represent as being important. Within biology VNNs have been used by [Ma et al. 2018](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5882547/) and [Hilten et al. 2021](https://www.nature.com/articles/s42003-021-02622-z) working in yeast and humans respectively (in the later mixed performance, seemingly based on trait complexity).\n\n## A hypothetical gene network\n\nBefore scaling to representing gene networks, I built a simple test case and will walk through it below, with all the necessary code (but some of it hidden[^1] for brevity).\n\n[^1]: Have a look at the page source.\n\n![](GraphBasic.png)\n\nHere we have a hypothetical network which involves two genes (`a1_input`, `a2_input`), variants of which affect some initial processes (`b1`, `b2`), which in turn affect a second set of processes (`c1`, `c2`). I'll use these last processes to predict my trait of interest (`y_hat`).\n\nThis is a directed acyclic graph, meaning that processes have an order (the arrows) and there are no loops (`c1` doesn't some how change `a1_input`). The model I'd like to end up with is a neural network with a structure that mirrors this graph [^2] with each node representing one or more layers of neurons.\n\n[^2]: This will not be a graph neural network, although they may be effective here.\n\nBeginning with the end in mind, I need a way to specify: 1. The data the graph operates on 1. The process graph and each node's attributes 1. How to \"move\" through the graph\n\n## 1. The data itself\n\nMy example trait, $y$ is either 0 or 1 (plus a little noise). It's controlled by two genes which are represented as tensor[^3] containing values for each possible nucleotide (ACGT) for each SNP measured in the gene. Conveniently, both genes either contain all 0's or all 1's and when there are only 0's $y$ will be around 0 (and the same for 1).\n\n[^3]: A box of numbers that can have multiple dimensions. A matrix is a \"rank-2\" tensor.\n\nThis of course means that in this population *no nucleotides* (all 0s) were observed or *all nucleotides* (all 1s) were simultaneously observed. Don't ask me how this is possible ü§î. For real data these values would be probability of seeing a given nucelotide so \"A\" might be `[1, 0, 0, 0]`[^4].\n\n[^4]: Technically, with 4 possibilities you only need 3 binary digits where `[0, 0, 0]` would be 100% probability of the fourth nucleotide\n\n\n\n::: {#840ea23f .cell execution_count=2}\n``` {.python .cell-code}\nn_obs = 100 # 100 obs for each group\ny_true = torch.from_numpy(np.concatenate([\n        np.zeros((n_obs, )),\n        np.ones( (n_obs, ))], 0)) + .1* torch.rand(2*n_obs,)\n        \ninput_tensor_dict = {\n    'a1_input': torch.from_numpy(np.concatenate([\n        np.zeros((n_obs, 4, 3)),\n        np.ones( (n_obs, 4, 3))], 0)),\n    'a2_input': torch.from_numpy(np.concatenate([\n        np.zeros((n_obs, 4, 2)),  \n        np.ones( (n_obs, 4, 2))], 0))}\n\nx_list_temp = [input_tensor_dict[key].to(torch.float) for key in input_tensor_dict.keys()]\nx_list_temp\n# output\n                        # Probability of\n[tensor([[[0., 0., 0.], # A\n          [0., 0., 0.], # C\n          [0., 0., 0.], # G\n          [0., 0., 0.]],# T\n\n         ...,\n\n         [[1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.]]]),\n\n tensor([[[0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.]],\n\n         ...,\n\n         [[1., 1.],\n          [1., 1.],\n          [1., 1.],\n          [1., 1.]]])]\n```\n:::\n\n\nThen this data can be packaged nicely in a `DataLoader`[^5]. This will retrieve the trait (`y`) and SNPs for each gene (in `x_list`) for 20 observations at a time.\n\n[^5]: I'm using a custom `Dataset` subclass. See source for details.\n\n\n\n::: {#73d023f3 .cell execution_count=4}\n``` {.python .cell-code}\ntraining_dataloader = DataLoader(\n  ListDataset(\n    y = y_true[:, None].to(torch.float32), # Set as 32 bit float to match network\n    x_list = [e.to(torch.float32) for e in x_list_temp]),\n    batch_size = 20,\n    shuffle = True)\n```\n:::\n\n\n## 2. Defining the graph\n\nThe structure of a graph can be nicely represented as a python dictionary so I'll begin with that:\n\n::: {#85766d75 .cell execution_count=5}\n``` {.python .cell-code}\nnode_connections = {\n  'y_hat':['c1', 'c2'],\n  'c1':['b1'],\n  'c2':['b2'],\n  'b1':['a1_input', 'b2'],\n  'b2':['a2_input'],\n  'a1_input': [],\n  'a2_input': []\n}\n```\n:::\n\n\nEach node will have an input and output size stored in a dictionary. The output sizes are easy, all nodes will have the same size except for the last node, which has predicts y, which will have a size of 1.\n\n\n\n::: {#33024a92 .cell execution_count=7}\n``` {.python .cell-code}\nnode_list = list(node_connections.keys())\n\ndefault_output_size = 20\noutput_size_dict = dict(zip(node_list, \n                        [default_output_size for i in range(len(node_list))]))\noutput_size_dict['y_hat'] = 1 \noutput_size_dict\n# output\n{'a1_input': 20,\n 'a2_input': 20,\n 'b1': 20,\n 'b2': 20,\n 'c1': 20,\n 'c2': 20,\n 'y_hat': 1}\n```\n:::\n\n\nThe input sizes are a little trickier. A node's input should be the number of SNPs in a gene (if it's an input node) or the sum of the outputs of the nodes on which it depends (e.g. `y_hat`'s input size is the sum of `c1` and `c2`'s outputs). To do this, I'm going to copy the dictionary with all the connections between nodes, then swap the node names for their output sizes. Summing the list of these output values will be the required input size. Data nodes don't depend on input from other nodes, so those will have an input shape of 0.\n\n::: {#73e8dd5f .cell execution_count=8}\n``` {.python .cell-code}\ninput_size_dict = node_connections.copy()\n\nno_dependants = [e for e in node_connections.keys() if node_connections[e] == []]\n\n# use the expected output sizes from `output_size_dict` to fill in the non-data sizes\ntensor_ndim = len(input_tensor_dict[list(input_tensor_dict.keys())[0]].shape)\nfor e in tqdm(input_size_dict.keys()):\n    # overwrite named connections with the output size of those connections\n    # if the entry is in no_dependants it's data so it's size needs to be grabbed from the input_tensor_dict\n    input_size_dict[e] = [\n        (list(input_tensor_dict[ee].shape)[1]*list(input_tensor_dict[ee].shape)[2]) \n        if ee in no_dependants\n        else output_size_dict[ee] for ee in input_size_dict[e]]\n\n# Now walk over entries and overwrite with the sum of the inputs\nfor e in tqdm(input_size_dict.keys()):\n    input_size_dict[e] = np.sum(input_size_dict[e])\n    \ninput_size_dict\n# output\n{'y_hat': 40,\n 'c1': 20,\n 'c2': 20,\n 'b1': 32,\n 'b2': 8,\n 'a1_input': 0.0,\n 'a2_input': 0.0}\n```\n:::\n\n\nNow we can update the graph from above adding in the input/output sizes.\n\n\n\n![](GraphWithUnits.png)\n\n## 3. How to move through the graph\n\nTo calculate the prediction for an observation each node in the graph needs to be run after all it's input nodes have been run. Specifically, I need a list of nodes, ordered such that each node comes after all the nodes on which it depends.\n\nThis takes little doing. Here I use some custom helper function to find the unique entries in a dictionary, the \"top\" nodes (those on which no other nodes depend).\n\n::: {#6863b5e1 .cell execution_count=10}\n``` {.python .cell-code}\n# start by finding the top level -- all those keys which are themselves not values\n# helper function to get all keys and all value from a dict. Useful for when keys don't have unique values.\ndef find_uniq_keys_values(input_dict):\n    all_keys = list(input_dict.keys())\n    all_values = []\n    for e in all_keys:\n        all_values.extend(input_dict[e])\n    all_values = list(set(all_values))\n\n    return({'all_keys': all_keys,\n           'all_values': all_values})\n\n# find the dependencies for run order from many dependencies to none\n# wrapper function to find the nodes that aren't any other nodes dependencies.\ndef find_top_nodes(all_key_value_dict):\n    return([e for e in all_key_value_dict['all_keys'] if e not in all_key_value_dict['all_values']])\n```\n:::\n\n\nSimilar to how I calculated each node's output size, here I copy the connection dictionary and then manipulate it. I repeatedly identify the top-most nodes in the graph, add them to a list, and then *remove them* from the dictionary. Repeating this \"peels\" of the top layer over and over until there are nodes left. The resulting list is ordered from top most to most basal, so reversing it is all that need be done to get the order nodes should be run in.\n\n::: {#1c10b20e .cell execution_count=11}\n``` {.python .cell-code}\n# find the dependencies for run order from many dependencies to none\ntemp = node_connections.copy()\n\ndependancy_order = []\n# Then iterate\nfor ith in range(100): \n    top_nodes = find_top_nodes(all_key_value_dict = find_uniq_keys_values(input_dict = temp))\n    if top_nodes == []:\n        break\n    else:\n        dependancy_order += top_nodes    \n        # remove nodes from the graph that are at the 'top' level and haven't already been removed\n        for key in [e for e in dependancy_order if e in temp.keys()]:\n             temp.pop(key)\n\n                \n# reverse to get the order that the nodes should be called\ndependancy_order.reverse()                \ndependancy_order\n# output\n['a2_input', 'a1_input', 'b2', 'b1', 'c2', 'c1', 'y_hat']\n```\n:::\n\n\n## 4. Turn the graph into a neural network\n\nSo far, we have the data in a useful format (`training_dataloader`), a description of what the network should look like (`node_connections`, `input_size_dict`, `output_size_dict`), and the order that nodes in the network should be run in (`dependancy_order`). With this, we can build the network. I'll start by defining a node as a linear layer (`nn.Linear`) that is passed into a `ReLU`. By creating a function[^6] for making nodes, changing *every* node in the network is as easy as editing this function.\n\n[^6]: Technically a method since it's in a class.\n\n::: {#fc1a5832 .cell execution_count=12}\n``` {.python .cell-code}\ndef Linear_block(in_size, out_size, drop_pr):\n    block = nn.Sequential(\n        nn.Linear(in_size, out_size),\n        nn.ReLU())\n    return(block)  \n```\n:::\n\n\nNow, I can go through each node in order of it's dependencies and have it return the data (if it's an input node), process inputs with a `Linear_block` (if it's not an input node or the output node), or use a linear function to predict the trait[^7].\n\n[^7]: As an aside, the first time I wrote this I had all non-input nodes be Linear_blocks. This resulted in fair bit frusterated debugging as the network would either train perfectly or fail to train depending on how the last ReLU was initializedü§¶üèº‚Äç‚ôÇÔ∏è.\n\n::: {#f04f92b7 .cell execution_count=13}\n``` {.python .cell-code}\n# fill in the list in dependency order. \nlayer_list = []\nfor key in dependancy_order:\n    if key in input_tensor_names:\n        layer_list += [\n            nn.Flatten()\n        ]\n    elif key != 'y_hat':\n        layer_list += [\n            Linear_block(in_size=example_dict_input_size[key], \n                         out_size=example_dict_output_size[key])\n                      ]\n    else:\n        layer_list += [\n            nn.Linear(example_dict_input_size[key], \n                      example_dict_output_size[key])\n                      ]\n```\n:::\n\n\n\n\n## Double checking the model structure\n\nUsing the lovely library [torchviz](https://github.com/szagoruyko/pytorchviz), we can visualize every computational step in this model.\n\n\n\n![](GraphTorchViz.png){width=\"340\"}\n\nThis is a lot to look at, but if we compare it to the earlier graph we can spot the same loop.\n\n![](CompareGraphs.png)\n\n## The moment of truth...\n\nNow all that is left is to see if the model trains. Using the objects describing the graph, the names of the input tensors, and the order nodes should be run it I'll initialze the network, train it for 200 epochs aaaaaannnnddd....\n\n::: {#f22beb2c .cell execution_count=16}\n``` {.python .cell-code}\nmodel = NeuralNetwork(example_dict = node_connections, \n                      example_dict_input_size = input_size_dict,\n                      example_dict_output_size = output_size_dict,\n                      input_tensor_names = list(input_tensor_dict.keys()),\n                      dependancy_order = dependancy_order) \n\n\nmodel, loss_df = train_nn_yx(\n    training_dataloader,\n    training_dataloader, # For demo, the training and testing data are the same.\n    model,\n    learning_rate = 1e-3,\n    batch_size = 20,\n    epochs = 200\n)\n\n```\n:::\n\n\nIt works! \n\n\n\n![](TrainingHistory.png)\n\nNow all that's left is to scale it up to a full genome and all the connections between the genes in it üòÖ.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}