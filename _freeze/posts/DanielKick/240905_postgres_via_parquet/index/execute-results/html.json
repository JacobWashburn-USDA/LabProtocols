{
  "hash": "7d5afcd0c4996f3624b6fb522d4e1dd3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Save only what you need\"\nsubtitle: \"Saving 88% disk space with a few lines of `R`\"\nauthor: \"Daniel Kick\"\ndate: \"2024-09-05\"\nimage: \"IBM_card_storage.NARA.jpg\"\ncategories: \n  - code\n  - debugging\n  - tacit knowledge\n  - beginner\nfreeze: true\n---\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7424.908\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 735.05\n```\n\n\n:::\n:::\n\n\n[link](./posts/DanielKick/230913_filtering_to_save_88pr_space/index.html)\n\n\n\n\n## How we got to this point:\n\nCollecting data from many sites is expensive but using a biophysical model, many sites and years can be 'harvested' in minutes. I'm building a dataset with many cultivars planted across the united states. The problem is that I'm being greedy -- I want to have the a day by day account of plant's growth at \\~1,300 locations from 1984-2022, varying cultivar, and planting date.\n\nIn my initial implementation the results from the model are written to a csv for each location...\n\n![Oh *no*.](Picture1.PNG)\n\nThis file has a boat load of data. It's a table of 25,243,175 rows by 19 columns -- *479,620,325 billion* cells. By the end of the experiment much of my hard drive will be taken up by these.\n\n## Reclaiming 88% Storage Space\n\nAn easy place to cut cells is from redundant or unneeded columns. These are produced by the simulation but the way I have the experiment structured, they aren't needed after it's done running.\n\n```         \n# YAGNI isn't just for code\ndf <- df[, c(\n      # Indepenent Variables\n      'ith_lon', 'ith_lat', 'soils_i', 'SowDate', 'Genotype', 'Date',\n      \n      # Dependent Variables\n      'Maize.AboveGround.Wt', 'Maize.LAI', 'yield_Kgha'\n\n      # Redundant or not needed\n      #'X', 'ith_year_start', 'ith_year_end', 'factorial_file', 'CheckpointID', \n      #'SimulationID', 'Experiment', 'FolderName', 'Zone', 'Clock.Today'\n      )]\n```\n\nThis is an easy way to get rid of over half the cells (down to 47.36%) (and really I should not have saved these in the first place) but we can do better still.\n\nMany of the rows represent times before planting without any data collected. All rows where `Maize.AboveGround.Wt`, `Maize.LAI`, and `Maize.AboveGround.Wt` are 0 can be dropped. Because so much of the year is out of the growing season this is quite helpful and cuts about half of the observations (20.09%).\n\nSplitting these data into two tables with independent variables or dependent variables (with a key) gets the total down to 10,602 + 53,530,975 = 53,541,577. Still a lot but only 11.16% of the starting size!\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Data </th>\n   <th style=\"text-align:right;\"> Size </th>\n   <th style=\"text-align:right;\"> Percent Original </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Original </td>\n   <td style=\"text-align:right;\"> 479620325 </td>\n   <td style=\"text-align:right;\"> 100.00 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Select Cols. </td>\n   <td style=\"text-align:right;\"> 227188575 </td>\n   <td style=\"text-align:right;\"> 47.37 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Filter Rows </td>\n   <td style=\"text-align:right;\"> 96355755 </td>\n   <td style=\"text-align:right;\"> 20.09 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Split Tables </td>\n   <td style=\"text-align:right;\"> 53541577 </td>\n   <td style=\"text-align:right;\"> 11.16 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nI could probably go even further, but now that each experiment takes up only 482 MB instead of 4.64 GB. Furhter optimization can wait for another day.\n\nWhile storage space is important (at this scale), another factor for the performance (and quality of life) is reading in the data. Using the basic `read.csv` function it takes 4 minutes 23 seconds to read in. Using the `vroom` library instead can read in these data in only *4.04 seconds*.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}