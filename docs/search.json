[
  {
    "objectID": "protocols/Rootbot/Using_Sorting_Script/index.html",
    "href": "protocols/Rootbot/Using_Sorting_Script/index.html",
    "title": "Using the Image Sorting Script",
    "section": "",
    "text": "Log into rootbot ssh pi\\@10.206.31.189\nMove to Pictures cd Pictures/\n\n\nLook in the folder with ls – There will either be loose jpgs or they will be packaged into archive folders with the form “YYYYMMDD-Archive”\n\nLog into lambda2 ssh labmember\\@MW22-lambda2\nOn lambda2 move to the inbox cd Documents/rootbot_dev/inst/extdata/Pictures/Inbox\n\nWe expect this inbox to be empty (but it’s okay if there are some jpgs here)\n\n\nFrom the rootbot, copy the files over to lambda2\n\nIf the jpgs are loose run (all one command) rsync -azv ./\\*.jpg labmember\\@10.206.28.81:\\~/Documents/rootbot_dev/inst/extdata/Pictures/Inbox/\n\n\n\nThen make a folder to archive the jpgs in with mkdir$-Archive\nThen copy each day’s files like so mv\\(\\color{red}{202209}`\\*.jpg ./`\\)-Archive/\n\nNote! The red above will need to be customized\nNote! If multiple experiments were run then you’ll need to specify each day that needs to be moved into each archive.\n\nmv 20220922*.jpg ./20220922-Archive/\nmv 20220923*.jpg ./20220923-Archive/\n…\nmv 20220929*.jpg ./20220929-Archive/\nmv 20220930*.jpg ./20220930-Archive/\n\n\n\nIf the jpgs are not loose copy the jpgs from an archive (specify the archive name highlighted below) rsync -azv ./$/\\*.jpg labmember\\@10.206.28.81:\\~/Documents/rootbot_dev/inst/extdata/Pictures/Inbox/\nThe terminal session on rootbot is no longer needed and can be closed.\nConfirm the files have been moved with ls on lambda2\n\n\n\nOn lambda2, move back to “rootbot_dev” with cd \\~/Documents/rootbot_dev\nOn lambda2, start up jupyter without a browser jupyter notebook --no-browser --port=8896\n\n\nOpen a new terminal and connect the jupyter notebook to your machine ssh -N -f -Y -L 8896:localhost:8896 labmember\\@MW22-lambda2\n\n\n\n\n\nOpen a browser and go to http://localhost:8896 (it will change to the url below if everything is okay).\n\nIf you get a login page, you’ll need to copy and paste the token from step 10\n\n\n\nIf you get a different error take a screenshot so we can debug it.\n\nThe screen should now look like this:\n\n\n\nGo to scripts\n\n\n\nOpen the “sortmaster2000”\n\n\n\nRun the script cell by cell until you get to …\n\n\n\n… until you get to this cell which contains instructions on how to get the files to your local computer\n\n\n\nOpen a terminal on lambda2\n\nIf you want to run this command on lambda2\nIt’s okay to open a new ssh session as in step 3\n\n\nIt’s also okay to reuse the terminal running the jupyter notebook. If you want to do this, press “control c”\n\n\n\nFollow the instructions in 17\n\n\nIf you want to run this command locally, on your local machine run rsync -azv --files-from:= labmember\\@MW22-lambda2:\\~/Documents/rootbot_dev/inst/extdata/Pictures/Experiments/\\(\\color{red}{20220915}\\)/send_files.txt ./$\nNote: The red above will need to be customized. The folder will need to be set to the current value and ../dest/ will need to be customized to the location on your computer you want."
  },
  {
    "objectID": "protocols/Operations/Website_Overview/index.html",
    "href": "protocols/Operations/Website_Overview/index.html",
    "title": "Website Overview",
    "section": "",
    "text": "Update: 9/18/2024 If you haven’t setup git please review this presentation and setup git and GitHub.\n\n\nThe easiest way to get the desktop version working is to create a new website or blog and render it. Any missing packages will cause an error. After it builds successfully, you can discard the newly made project directory and switch to this one.\n\nIn RStudio, select File &gt; New Project &gt; New Directory and select either Quarto Website or Quarto Blog.\n\nFollow the remaining prompts placing the project directory in a location where it will be easy to remove.\nThe template site will look like this. Since the focus here is checking dependencies we won’t go into what each of these documents are but Quarto’s documentation is quite good if you would like to learn more.\n\nNote that there are two render buttons. The one on the top pane renders a single document. This is useful to check how a document looks without rebuilding the entire website or for updating a document that is “frozen” and will not be re-rendered when the website is built. The second one under the build tab and will setup the website when run. Render index.qmd now and address any errors (missing libraries) that come up.\nNow render the website. Your default web browser should open to localhost:#### . The page will look like this: \nNow you are ready to build the documentation site! You can close the RStudio project, delete it, and switch to the website repository.\n\n\n\n\nThis is a terse variant of these instructions.\nmkdir rstudio_container && cd rstudio_container\n\n# Add necessary subfolders -----------------------------------------------------\nmkdir -p run var-lib-rstudio-server\nprintf 'provider=sqlite\\ndirectory=/var/lib/rstudio-server\\n' &gt; database.conf\nmkdir home\n\n# Add in preferences -----------------------------------------------------------\nmkdir ide_settings\n# if you're running from within the WSL\n# cp /mnt/c/Users/&lt;USER NAME&gt;/AppData/Roaming/RStudio/rstudio-prefs.json ./ide_settings/\n# if you're running from OSX/Linux\n# cp ~/.config/rstudio/rstudio-prefs.json ./ide_settings/\n\n# Create container with publishing capabilities --------------------------------\necho \"Bootstrap: docker\\nFrom: rocker/verse:4.2\" &gt; RStudio.def\nsudo singularity build RStudio.sif RStudio.def \n\n# Initial run without preferences ----------------------------------------------\nsingularity exec \\\n  --bind run:/run \\\n  --bind var-lib-rstudio-server:/var/lib/rstudio-server \\\n  --bind database.conf:/etc/rstudio/database.conf \\\n  --bind home:/home \\\n  --bind /mnt/c/Users/drk8b9/Documents/LabProtocols:/home/rstudio/LabProtocols \\\n  --bind ide_settings:/home/rstudio/.config/rstudio/ \\\n  RStudio.sif \\\n  rserver \\\n  --www-address=127.0.0.1 \\\n  --www-port=8700 \\\n  --server-user=rstudio\n  \n# Then shutdown server."
  },
  {
    "objectID": "protocols/Operations/Website_Overview/index.html#desktop-version",
    "href": "protocols/Operations/Website_Overview/index.html#desktop-version",
    "title": "Website Overview",
    "section": "",
    "text": "The easiest way to get the desktop version working is to create a new website or blog and render it. Any missing packages will cause an error. After it builds successfully, you can discard the newly made project directory and switch to this one.\n\nIn RStudio, select File &gt; New Project &gt; New Directory and select either Quarto Website or Quarto Blog.\n\nFollow the remaining prompts placing the project directory in a location where it will be easy to remove.\nThe template site will look like this. Since the focus here is checking dependencies we won’t go into what each of these documents are but Quarto’s documentation is quite good if you would like to learn more.\n\nNote that there are two render buttons. The one on the top pane renders a single document. This is useful to check how a document looks without rebuilding the entire website or for updating a document that is “frozen” and will not be re-rendered when the website is built. The second one under the build tab and will setup the website when run. Render index.qmd now and address any errors (missing libraries) that come up.\nNow render the website. Your default web browser should open to localhost:#### . The page will look like this: \nNow you are ready to build the documentation site! You can close the RStudio project, delete it, and switch to the website repository."
  },
  {
    "objectID": "protocols/Operations/Website_Overview/index.html#containerized-version",
    "href": "protocols/Operations/Website_Overview/index.html#containerized-version",
    "title": "Website Overview",
    "section": "",
    "text": "This is a terse variant of these instructions.\nmkdir rstudio_container && cd rstudio_container\n\n# Add necessary subfolders -----------------------------------------------------\nmkdir -p run var-lib-rstudio-server\nprintf 'provider=sqlite\\ndirectory=/var/lib/rstudio-server\\n' &gt; database.conf\nmkdir home\n\n# Add in preferences -----------------------------------------------------------\nmkdir ide_settings\n# if you're running from within the WSL\n# cp /mnt/c/Users/&lt;USER NAME&gt;/AppData/Roaming/RStudio/rstudio-prefs.json ./ide_settings/\n# if you're running from OSX/Linux\n# cp ~/.config/rstudio/rstudio-prefs.json ./ide_settings/\n\n# Create container with publishing capabilities --------------------------------\necho \"Bootstrap: docker\\nFrom: rocker/verse:4.2\" &gt; RStudio.def\nsudo singularity build RStudio.sif RStudio.def \n\n# Initial run without preferences ----------------------------------------------\nsingularity exec \\\n  --bind run:/run \\\n  --bind var-lib-rstudio-server:/var/lib/rstudio-server \\\n  --bind database.conf:/etc/rstudio/database.conf \\\n  --bind home:/home \\\n  --bind /mnt/c/Users/drk8b9/Documents/LabProtocols:/home/rstudio/LabProtocols \\\n  --bind ide_settings:/home/rstudio/.config/rstudio/ \\\n  RStudio.sif \\\n  rserver \\\n  --www-address=127.0.0.1 \\\n  --www-port=8700 \\\n  --server-user=rstudio\n  \n# Then shutdown server."
  },
  {
    "objectID": "protocols/Operations/Website_Overview/index.html#project-root",
    "href": "protocols/Operations/Website_Overview/index.html#project-root",
    "title": "Website Overview",
    "section": "Project Root",
    "text": "Project Root\nLet’s look at this project from a high level. In this directory there’s an Rproj (that you’ve opened), documents for the landing page (index.qmd) and about (about.qmd) pages, directories for protocols and info on people, documents that control the website style and layour (_quarto.yml, styles.css), and a folder that contains the generated html pages (_site).\nLabProtocols\n├── LabProtocols.Rproj\n├── _quarto.yml\n├── _site\n├── about.qmd\n├── index.qmd\n├── people\n├── protocols\n└── styles.css\nThe index.qmd file contains links to protocols. Whenever new pages are added they should be linked here to be visible. The about.qmd file has information on the lab and it’s members. Lab member pictures are pulled from /people/ so any new pictures should be added there along with any personal pages (e.g. if we want to link student resumes). Additional folders could be added to hold assests that may be useful. For instance, we could add a /papers/ folder and link from index.qmd to the lab’s papers. This would be helpful for on boarding."
  },
  {
    "objectID": "protocols/Operations/Website_Overview/index.html#protocols",
    "href": "protocols/Operations/Website_Overview/index.html#protocols",
    "title": "Website Overview",
    "section": "Protocols",
    "text": "Protocols\nThe protocols folder is where most of the work happens. Inside it there are subfolders to roughly group protocols. Right now it contains Drones, Logistics, and Operations. Other useful groups might include Rootbot, Wet Lab, and Field Work. Each protocol page is a folder within one of these groups. For consistency it should be named in Capitalized_Snakecase and contain a Quarto document called index.qmd. There may be images in this folder as well. See the documentation on preparing protocols.\nprotocols\n├── Drones\n│   └── Pix4Dmapper_Stitch\n│       ├── Picture1.png\n│       ├── ...\n│       ├── Picture8.png\n│       └── index.qmd\n├── Logistics\n└── Operations"
  },
  {
    "objectID": "protocols/Operations/Website_Overview/index.html#making-new-protocols",
    "href": "protocols/Operations/Website_Overview/index.html#making-new-protocols",
    "title": "Website Overview",
    "section": "Making new protocols",
    "text": "Making new protocols\nThe easiest way to make a new protocol is to copy the folder for an existing protocol and modify that. Give the folder an informative, Capitalized_Snakecase name and delete any photos or other artifacts that you don’t need. Then open up the index.qmd file and begin editing it.\nAt the top of the file you’ll see some YAML which provides metadata for the page.\n---\ntitle: \"Building an RStudio Singularity Containers\"\nauthor: \"Daniel Kick\"\ndate: \"5/31/2023\"\ndate-modified: \"5/31/2023\"\nexecute:\n  freeze: true\n---\nNote the last two lines here. At present this document is frozen so it will not be re-rendered when the website is built. This means that to update this page you’ll need to click the “Render” button on it before rendering the website. Any pages with code that rely on non-standard libraries should be frozen (so that other people don’t need those installed to rebuild the site) but it is okay to remove these lines while working on a protocol."
  },
  {
    "objectID": "protocols/Operations/Website_Overview/index.html#making-protocols-visible",
    "href": "protocols/Operations/Website_Overview/index.html#making-protocols-visible",
    "title": "Website Overview",
    "section": "Making protocols visible",
    "text": "Making protocols visible\nOnce a protocol is added it needs to be linked on the index.qmd in the project root so it is accessible. Open that document and add a link to the html version of the file. For example, this page is linked on that page as [Website Overview](/protocols/Operations/Website_Overview/index.html). Once this is done, rebuild the website by clicking ther “Render Website” button under the “Build” tab."
  },
  {
    "objectID": "protocols/Operations/Website_Overview/index.html#deploying-updates",
    "href": "protocols/Operations/Website_Overview/index.html#deploying-updates",
    "title": "Website Overview",
    "section": "Deploying updates",
    "text": "Deploying updates\n&lt;font color =“red”&gt; *This section is intentionally left blank. Likely we will deploy through GitHub pages* &lt;/font&gt;"
  },
  {
    "objectID": "protocols/Operations/Authoring_Protocols/index.html",
    "href": "protocols/Operations/Authoring_Protocols/index.html",
    "title": "Authoring Protocols",
    "section": "",
    "text": "Authoring and editing protocols is meant to be easy an accessible. There’s no wrong way to do it so long as you send documentation of the what you want added to a lab member with access to the website. That being said, the form of this documentation falls in different “levels” and the higher the level the faster you’re work will be accessible to everyone 😃. If you send a document it will go through each of these levels before being posted."
  },
  {
    "objectID": "protocols/Operations/Authoring_Protocols/index.html#level-1-word-document",
    "href": "protocols/Operations/Authoring_Protocols/index.html#level-1-word-document",
    "title": "Authoring Protocols",
    "section": "Level 1: Word Document",
    "text": "Level 1: Word Document\n\nHere’s an example protocol. It has clearly written steps, useful images, and an informative title. This is a great start!"
  },
  {
    "objectID": "protocols/Operations/Authoring_Protocols/index.html#level-2-word-document-images-on-the-side",
    "href": "protocols/Operations/Authoring_Protocols/index.html#level-2-word-document-images-on-the-side",
    "title": "Authoring Protocols",
    "section": "Level 2: Word Document, Images on the side",
    "text": "Level 2: Word Document, Images on the side\nBefore a protocol can become a webpage the pictures need to be removed. Unlike a word document which contains the pictures within it, the Quarto documents for this site contain links to the images. For instance, the picture above is ![](Picture1.png) under the hood.\nThe steps to go from Level 1 to Level 2 are:\n\nSave each of the images that were in the document\n\nA handy trick for this is to copy the picture you want from word to powerpoint. Then you can right click and select “Save as Picture” to get the file.\nIf you want annotations (like the red boxes above) you can also do that in powerpoint. Select the image and annotations, right click and select “Group”. Then you can save the whole group as a picture.\n\nIn the place of a picture type ![](PictureName.png) so it’s clear where each picture belongs.\n\nPicture names can be descriptive (“StitchProtocolPage1.png”) or numbered (“Picture1.png”)."
  },
  {
    "objectID": "protocols/Operations/Authoring_Protocols/index.html#level-3-quarto-document-images-on-the-side-in-a-zipped-folder",
    "href": "protocols/Operations/Authoring_Protocols/index.html#level-3-quarto-document-images-on-the-side-in-a-zipped-folder",
    "title": "Authoring Protocols",
    "section": "Level 3: Quarto Document, Images on the side, in a Zipped Folder",
    "text": "Level 3: Quarto Document, Images on the side, in a Zipped Folder\nWhen your protocol is at this level, it’s basically ready for the the website. The big change here is that the protocol is stored in as a “Quarto Markdown File”. Once you’ve completed these steps submit a request to get your 💻🧙 badge.\nBy the end you’ll have a folder with a Quarto file (index.qmd) and several images. The protocol shown above (Stitching Images with Pix4Dmapper), started as a folder called “Pix4Dmapper_Stitch” that looks like this:\n\n\nOpen RStudio and create a new Quarto Document. \nYou’ll see a pop up like this. You can title it now or handle that later.\n\n\n\nRStudio has two visualizing options. Visual and Source. Both are useful but I’d recommend using Visual most of the time.\nVisual:\n\n\nSource:\n\n\nThe ![](Picture1.png) won’t be treated properly if it’s pasted in while RStudio is in “Visual” mode. You can switch into “Source” to correct this or click on the small image icon by “Format” to insert an image.\nThere’s the stitching protocol after I converted it to a Quarto document. One thing to notice is that font color takes some effort. The text that was red in the word document is surrounded by &lt;font color=\"red\"&gt; and &lt;/font&gt;. When this page is converted into html this will be processed and show up nicely (but not until then).\nVisual:\n\n\nSource:\n\n\nSave your file as index.qmd in the same folder as the images for the file.\nFinally, zip this folder and send it to be added to the site!"
  },
  {
    "objectID": "protocols/Logistics/HPC_Generic_Workflow/index.html",
    "href": "protocols/Logistics/HPC_Generic_Workflow/index.html",
    "title": "Generic HPC Workflow",
    "section": "",
    "text": "flowchart LR\n\n%% Parts\nsubgraph PC\n    subgraph PC_Containers\n        def --&gt; sif\n        kern[kernel.json]\n    end\n    subgraph PC_Proj\n        PC_data[/Data/]\n        PC_code[Code]\n    end\n    subgraph PC_Sess\n        PC_shell[Bash session]\n        PC_web[Web browser]\n    end\nend\n\n\nsubgraph HPC\n    HPC_shell[Bash over SSH]\n    ood[Open \\nOnDemand]\n    subgraph HPC_User[\"`home/first.last`\"]\n        HPC_sif\n        HPC_kern[kernel.json]\n    end\n    subgraph HPC_Proj[\"`/project/project_name`\"]\n        HPC_data[/Data/]\n        HPC_code[Code]      \n    end\nend\n\n%% Connections\nPC_data -- dtn --&gt; HPC_data\nPC_code -- dtn --&gt; HPC_code\n\nsif -- dtn --&gt; HPC_sif\nkern -- dtn --&gt; HPC_kern\n\nPC_shell -- login --&gt; HPC_shell\n\nHPC_kern --&gt; ood\nHPC_code --&gt; ood\nHPC_data --&gt; ood \nPC_web --&gt; ood"
  },
  {
    "objectID": "protocols/Logistics/HPC_Generic_Workflow/index.html#tldr",
    "href": "protocols/Logistics/HPC_Generic_Workflow/index.html#tldr",
    "title": "Generic HPC Workflow",
    "section": "TLDR;",
    "text": "TLDR;\n\nhttps://atlas-ood.hpc.msstate.edu/"
  },
  {
    "objectID": "protocols/Logistics/HPC_Generic_Workflow/index.html#ceres-vs-atlas",
    "href": "protocols/Logistics/HPC_Generic_Workflow/index.html#ceres-vs-atlas",
    "title": "Generic HPC Workflow",
    "section": "Ceres vs Atlas",
    "text": "Ceres vs Atlas\nMost of our work is done on Atlas so you’ll need to use this url https://atlas-ood.hpc.msstate.edu/. SciNet may direct you to Ceres instead https://ceres-ood.scinet.usda.gov/. If you login to Ceres, you’ll see the same project directories but any data will be missing.\nTo login you’ll need an authenticator code in addition to your login info. Your user name will be the same for both HPCs but the password should differ.\nIn this example I want a gpu compitable container with jupyter allowing deep neural network development on Atlas.\nBare bones .def file\nBootstrap: docker\nFrom: nvcr.io/nvidia/pytorch:23.04-py3"
  },
  {
    "objectID": "protocols/Logistics/HPC_Generic_Workflow/index.html#testing-container",
    "href": "protocols/Logistics/HPC_Generic_Workflow/index.html#testing-container",
    "title": "Generic HPC Workflow",
    "section": "Testing container:",
    "text": "Testing container:\n\nBuild sandbox container: singularity build --sandbox jnb jupyter.def\nTest for pytorch & jupyter locally: singularity shell jnb python -c “import torch; print( torch.cuda.is_available() )” jupyter-notebook # then check on browser exit\nTest for pytorch on lambda:\nAdd in jupyter:"
  },
  {
    "objectID": "protocols/Logistics/HPC_Generic_Workflow/index.html#finalizing-container",
    "href": "protocols/Logistics/HPC_Generic_Workflow/index.html#finalizing-container",
    "title": "Generic HPC Workflow",
    "section": "Finalizing container",
    "text": "Finalizing container\n\nFinalize\nAdd"
  },
  {
    "objectID": "protocols/Logistics/HPC_Generic_Workflow/index.html#the-cycle",
    "href": "protocols/Logistics/HPC_Generic_Workflow/index.html#the-cycle",
    "title": "Generic HPC Workflow",
    "section": "“The cycle”",
    "text": "“The cycle”\n\nIdentify new needs (libraries, tools)\nEdit .def\nVersion control\nbuild container\nSend to HPC\nDevelopment\n\nLocal or on Open OnDemand\n\nRun GPU code\n\nLocal\nExport notebook to txt and run as script."
  },
  {
    "objectID": "protocols/Logistics/Container_Singularity_Rstudio/index.html",
    "href": "protocols/Logistics/Container_Singularity_Rstudio/index.html",
    "title": "Building an RStudio Singularity Container",
    "section": "",
    "text": "mkdir rstudio_container\ncd rstudio_container\n# \nmkdir -p run var-lib-rstudio-server\nprintf 'provider=sqlite\\ndirectory=/var/lib/rstudio-server\\n' &gt; database.conf\nmkdir home\n\n\n\nRStudio stores your user preferences in rstudio-prefs.json. This file is in AppData/Roaming/RStudio on windows and ~/.config/rstudio on OSX/Linux.\nmkdir ide_settings\nCopy this file to ide_settings. It should look similar to this:\n{\n    \"save_workspace\": \"never\",\n    \"always_save_history\": false,\n    \"reuse_sessions_for_project_links\": true,\n    \"posix_terminal_shell\": \"bash\",\n    \"initial_working_directory\": \"~\",\n    \"panes\": {\n        \"quadrants\": [\n            \"Source\",\n            \"TabSet1\",\n            \"Console\",\n            \"TabSet2\"\n        ],\n        \"tabSet1\": [\n            \"Environment\",\n            \"History\",\n            \"Connections\",\n            \"Build\",\n            \"VCS\",\n            \"Tutorial\",\n            \"Presentation\"\n        ],\n        \"tabSet2\": [\n            \"Files\",\n            \"Plots\",\n            \"Packages\",\n            \"Help\",\n            \"Viewer\",\n            \"Presentations\"\n        ],\n        \"hiddenTabSet\": [],\n        \"console_left_on_top\": false,\n        \"console_right_on_top\": true,\n        \"additional_source_columns\": 0\n    },\n    \"editor_theme\": \"Clouds Midnight\"\n}\n\n\n\nln -s /mnt/c/Users/drk8b9/Documents/LabProtocols/\n(links can be removed with unlink [link name])\n\n\n\nsudo singularity pull docker://rocker/rstudio:4.2 Once this is complete there should be a container file present: rstudio_4.2.sif.\n(for customization construct a .def flie)\nrefer to the Rocker Project for more details on the available containers.\n\n\n\nTo make use of the we need to run the container which will setup the path we need to bind our preferences to.\nsingularity exec \\\n  --bind run:/run \\\n  --bind var-lib-rstudio-server:/var/lib/rstudio-server \\\n  --bind database.conf:/etc/rstudio/database.conf \\\n  --bind home:/home \\\n  rstudio_4.2.sif \\\n  /usr/lib/rstudio-server/bin/rserver \\\n  --www-address=127.0.0.1 \\\n  --www-port=8700 \\\n  --server-user=rstudio\n  \n# note the arguments under --bind can be passed in on one line with ',' separating them. They are included separately to increase readability.\nPress ctrl+c to exit the container. Now this container can be run with --bind ide_settings:/home/rstudio/.config/rstudio/ \\ to use preferred settings."
  },
  {
    "objectID": "protocols/Logistics/Container_Singularity_Rstudio/index.html#rstudio-recommended-example",
    "href": "protocols/Logistics/Container_Singularity_Rstudio/index.html#rstudio-recommended-example",
    "title": "Building an RStudio Singularity Container",
    "section": "",
    "text": "mkdir rstudio_container\ncd rstudio_container\n# \nmkdir -p run var-lib-rstudio-server\nprintf 'provider=sqlite\\ndirectory=/var/lib/rstudio-server\\n' &gt; database.conf\nmkdir home\n\n\n\nRStudio stores your user preferences in rstudio-prefs.json. This file is in AppData/Roaming/RStudio on windows and ~/.config/rstudio on OSX/Linux.\nmkdir ide_settings\nCopy this file to ide_settings. It should look similar to this:\n{\n    \"save_workspace\": \"never\",\n    \"always_save_history\": false,\n    \"reuse_sessions_for_project_links\": true,\n    \"posix_terminal_shell\": \"bash\",\n    \"initial_working_directory\": \"~\",\n    \"panes\": {\n        \"quadrants\": [\n            \"Source\",\n            \"TabSet1\",\n            \"Console\",\n            \"TabSet2\"\n        ],\n        \"tabSet1\": [\n            \"Environment\",\n            \"History\",\n            \"Connections\",\n            \"Build\",\n            \"VCS\",\n            \"Tutorial\",\n            \"Presentation\"\n        ],\n        \"tabSet2\": [\n            \"Files\",\n            \"Plots\",\n            \"Packages\",\n            \"Help\",\n            \"Viewer\",\n            \"Presentations\"\n        ],\n        \"hiddenTabSet\": [],\n        \"console_left_on_top\": false,\n        \"console_right_on_top\": true,\n        \"additional_source_columns\": 0\n    },\n    \"editor_theme\": \"Clouds Midnight\"\n}\n\n\n\nln -s /mnt/c/Users/drk8b9/Documents/LabProtocols/\n(links can be removed with unlink [link name])\n\n\n\nsudo singularity pull docker://rocker/rstudio:4.2 Once this is complete there should be a container file present: rstudio_4.2.sif.\n(for customization construct a .def flie)\nrefer to the Rocker Project for more details on the available containers.\n\n\n\nTo make use of the we need to run the container which will setup the path we need to bind our preferences to.\nsingularity exec \\\n  --bind run:/run \\\n  --bind var-lib-rstudio-server:/var/lib/rstudio-server \\\n  --bind database.conf:/etc/rstudio/database.conf \\\n  --bind home:/home \\\n  rstudio_4.2.sif \\\n  /usr/lib/rstudio-server/bin/rserver \\\n  --www-address=127.0.0.1 \\\n  --www-port=8700 \\\n  --server-user=rstudio\n  \n# note the arguments under --bind can be passed in on one line with ',' separating them. They are included separately to increase readability.\nPress ctrl+c to exit the container. Now this container can be run with --bind ide_settings:/home/rstudio/.config/rstudio/ \\ to use preferred settings."
  },
  {
    "objectID": "protocols/Logistics/Container_Singularity_Rstudio/index.html#using-the-container",
    "href": "protocols/Logistics/Container_Singularity_Rstudio/index.html#using-the-container",
    "title": "Building an RStudio Singularity Container",
    "section": "Using the container",
    "text": "Using the container\nsingularity exec \\\n  --bind run:/run \\\n  --bind var-lib-rstudio-server:/var/lib/rstudio-server \\\n  --bind database.conf:/etc/rstudio/database.conf \\\n  --bind home:/home \\\n  --bind LabProtocols:/home/rstudio/LabProtocols \\\n  --bind ide_settings:/home/rstudio/.config/rstudio/ \\\n  rstudio_4.2.sif \\\n  /usr/lib/rstudio-server/bin/rserver \\\n  --www-address=127.0.0.1 \\\n  --www-port=8700 \\\n  --server-user=rstudio\n  \n# note, you can also bind a folder or file by it's full path. For me on WSL this would be\n# --bind /mnt/c/Users/drk8b9/Documents/LabProtocols:/home/rstudio/LabProtocols \\\nLogin with defaults: “rstudio” and “rstudio”."
  },
  {
    "objectID": "protocols/Logistics/Container_Singularity_From_Conda/index.html",
    "href": "protocols/Logistics/Container_Singularity_From_Conda/index.html",
    "title": "Converting your Conda Enviroment to a Singularity Container",
    "section": "",
    "text": "For many tasks, building a container is overkill. Using a container will allow the same code to run on an HPC, but for purely local analysis a virtual environment (e.g. using conda, mamba, renv, or packrat) will do just fine. This guide supposes you have an analysis developed in a virtual environment (here I assume using conda) that needs to be containerized to run on a different machine.\nTo start out, we need to export our environment’s packages to a .yml file (gpu.yml). For this example I’m using the gpu environment and export the requirements file below.\nNext, we create a .def file (gpu.def) that contains conda and will download the specified requirements.\nBuild the container like so:\nAnd then we can test that the default python is conda …\n… and that it still can access the host gpus."
  },
  {
    "objectID": "protocols/Logistics/Container_Singularity_From_Conda/index.html#references",
    "href": "protocols/Logistics/Container_Singularity_From_Conda/index.html#references",
    "title": "Converting your Conda Enviroment to a Singularity Container",
    "section": "References",
    "text": "References\nThe def file used here was modified from this guide (which uses apptainer)."
  },
  {
    "objectID": "protocols/DryLab/Rovers/RoverBatteryCharge/index.html",
    "href": "protocols/DryLab/Rovers/RoverBatteryCharge/index.html",
    "title": "Rover Battery Charging SOP",
    "section": "",
    "text": "Rover & Battery\nCharging Batteries\n· The batteries are lithium polymer batteries (LiPo), which can be very dangerous if not properly charged, stored, or mishandled. May result in large and dangerous fires/explosions. If a fire occurs, it is a chemical fire which cannot be easily put out or contained.\nThe biggest warning signs for a potential fire:\no A hissing noise\no A very swollen battery\no Punctures of any sort\no Popping noises\n· When charging/storing/using batteries, make sure there is a “Cold” fire extinguisher (regular fire extinguishers will not work) nearby along with the green metal ammo containers with sand in the bottom and buckets of sand.\no The two buckets have lids that are blue\no If any of the above listed issues occur, 1) quickly place the battery in one of the ammo containers or buckets, 2) dump the (sand) from the other container on top of the battery, and 3) put the lid on top to control the flames, but DO NOT CLOSE THE LID ALL THE WAY (this could turn the container into a bomb).\no PULL FIRE ALARM and/or CALL FIREDEPT IMEDIATLY. If safe to remove ammo can/bucket with the battery in it to outside the build do so.\no Evacuate the building but remain in the building area and look for first responders so you can tell them about the hazard.\no Report the incident to Jacob and Harper so they can file the EHS report.\no File any paperwork needed for incident\n§ EHS link: https://ehs.missouri.edu/work/accident-reporting\n· More to know about LiPo batteries\no https://rogershobbycenter.com/lipoguide\no https://vdr.one/everything-you-need-to-know-about-lipo-batteries/\no https://www.youtube.com/watch?v=ogb0DTqsZEs\no https://www.youtube.com/watch?v=eKLHD7_zzCE&t=0s\no What will happen if the LiPo explodes/catches on fire: https://msadowski.github.io/lipo-safety/#:~:text=If%20your%20battery%20starts%20making,unplug%20it%20from%20the%20charger.\nCharging\n·Each battery has 4 cells that are 20 aH (amp hour).\no https://learn.adafruit.com/all-about-batteries/power-capacity-and-power-capability\n· The max charge for each battery is 16.8 Volts, which is 4.2 Volts per cell.\n· The Venom charger can theoretically charge 4 batteries at once, but we typically only charge two batteries at a time.\n·Each of the four cells in the battery needs to be charged and discharged in a balanced manner (e.g., one cell at 2 Volts and another at 4 Volts is dangerous).\n· Each battery has two cords coming from it. The cord with the think red and black wires and the yellow XT90 end is the main power. The cord with five small wires of different colors is for cell voltage level monitoring. NEVER use or charge the battery without the monitoring cord connected to the charger or on of the small monitor units.\n\nPlace the battery/batteries to be charged in the green metal ammo containers. They should stay in the containers at all times while being charged. Be careful not to get sand in or under the plastic covers on the batteries as this could result in rubbing and damage.\n\n\n\nIf at any time during charging the batteries become hot to the touch, or start to hiss, swell, or make popping noises. As long as it is safe to do so,\n\n\n\nUnplug the battery charger from the wall outlet!\nLeave the battery in the Ammo container and dump sand from the white bucket over the battery.\nFlip the Ammo container lid over the battery but DO NOT close the lid tightly as this could create a bomb.\nPULL FIRE ALARM and/or CALL FIREDEPT IMEDIATLY. If safe to unplug XT90 battery from jumper and remove ammo can/bucket with the battery in it to outside the build do so. If not safe then leave it.\nEvacuate the building but remain in the building area and look for first responders so you can tell them about the hazard.\n\nf.Report the incident to Jacob and Harper so they can file the EHS report.\n\nEnsure that the sperate XT90 jumper line (not the line on the batter itself) is properly plugged into the Venom charger and that the “battery balance” adaptor is also plugged into the charger on the same charging channel as the XT90 jumper. VERY IMPORTANT: make sure the XT90 jumper cables are plugged in correctly (Red to Red, Black to Black). NEVER plug the yellow XT90 end into the battery without having the Red and Black jumper ends plugged into the charger!  If the jumper is plugged into the battery and the Red and Black ends touch each other it could cause a fire! \n\n\n\n4.Turn on the power for the charger; make sure the charger is plugged into an outlet.\n\nUse the “CHANNEL” button to select which of the 4 channels you want to charge the battery on.\nYou want to use the “LiPo BALANCE” program. DO NOT USE the “LiPo CHARGE” program as this will not necessarily charge each cell in a balanced manor. Depending on what was previously selected you may have to use the Stop, Decrease, Increase, and Enter buttons to find the correct program.\n\n\n\n\nWe want to charge with 7.0A, 14.8V, and (4S). If these parameters are not correct you can change them by pressing enter (short press, not long press and hold).\n\n\n\nPlug the battery/Batteries (Both the XT90 and the multicolored monitoring cable into the charger using the jumper lines from 2 above.\n\n\n\nTo charge the batteries, long press the start button. The charger will check that the battery is correctly detected and display both the number of cells. If the number of cells for “R:” and “S:” do not match then something is wrong and you need to recheck the battery connections and settings.\nIf both “R:” and “S:” read “4SER” then you can press short press the ENTER button to begin charging. (if you forget to press enter this second time, the batteries will not charge even if they are plugged in).\nOnce charging begins, DO NOT LEAVE THE ROOM (even for 30 seconds) UNLESS YOU TELL SOMEONE SO THEY CAN WATCH OVER THE BATTERIES\nYou can check the status of the batteries on the displayed screen. Pushing “INC” or “DEC” will take you to a second screen where you can see the status of each of the four cells in the battery.\nAfter a long period of time, the charger may time out and you will need to restart it (press start button twice) if the battery is not all the way charged.\n\n· The cells should always be within 0.1-0.2 of each other; if they are not, let someone know.\n· The charger will get hot so keep it in cool area."
  },
  {
    "objectID": "protocols/DryLab/Rovers/RoverBatteryCharge/index.html#instructions",
    "href": "protocols/DryLab/Rovers/RoverBatteryCharge/index.html#instructions",
    "title": "Rover Battery Charging SOP",
    "section": "",
    "text": "Rover & Battery\nCharging Batteries\n· The batteries are lithium polymer batteries (LiPo), which can be very dangerous if not properly charged, stored, or mishandled. May result in large and dangerous fires/explosions. If a fire occurs, it is a chemical fire which cannot be easily put out or contained.\nThe biggest warning signs for a potential fire:\no A hissing noise\no A very swollen battery\no Punctures of any sort\no Popping noises\n· When charging/storing/using batteries, make sure there is a “Cold” fire extinguisher (regular fire extinguishers will not work) nearby along with the green metal ammo containers with sand in the bottom and buckets of sand.\no The two buckets have lids that are blue\no If any of the above listed issues occur, 1) quickly place the battery in one of the ammo containers or buckets, 2) dump the (sand) from the other container on top of the battery, and 3) put the lid on top to control the flames, but DO NOT CLOSE THE LID ALL THE WAY (this could turn the container into a bomb).\no PULL FIRE ALARM and/or CALL FIREDEPT IMEDIATLY. If safe to remove ammo can/bucket with the battery in it to outside the build do so.\no Evacuate the building but remain in the building area and look for first responders so you can tell them about the hazard.\no Report the incident to Jacob and Harper so they can file the EHS report.\no File any paperwork needed for incident\n§ EHS link: https://ehs.missouri.edu/work/accident-reporting\n· More to know about LiPo batteries\no https://rogershobbycenter.com/lipoguide\no https://vdr.one/everything-you-need-to-know-about-lipo-batteries/\no https://www.youtube.com/watch?v=ogb0DTqsZEs\no https://www.youtube.com/watch?v=eKLHD7_zzCE&t=0s\no What will happen if the LiPo explodes/catches on fire: https://msadowski.github.io/lipo-safety/#:~:text=If%20your%20battery%20starts%20making,unplug%20it%20from%20the%20charger.\nCharging\n·Each battery has 4 cells that are 20 aH (amp hour).\no https://learn.adafruit.com/all-about-batteries/power-capacity-and-power-capability\n· The max charge for each battery is 16.8 Volts, which is 4.2 Volts per cell.\n· The Venom charger can theoretically charge 4 batteries at once, but we typically only charge two batteries at a time.\n·Each of the four cells in the battery needs to be charged and discharged in a balanced manner (e.g., one cell at 2 Volts and another at 4 Volts is dangerous).\n· Each battery has two cords coming from it. The cord with the think red and black wires and the yellow XT90 end is the main power. The cord with five small wires of different colors is for cell voltage level monitoring. NEVER use or charge the battery without the monitoring cord connected to the charger or on of the small monitor units.\n\nPlace the battery/batteries to be charged in the green metal ammo containers. They should stay in the containers at all times while being charged. Be careful not to get sand in or under the plastic covers on the batteries as this could result in rubbing and damage.\n\n\n\nIf at any time during charging the batteries become hot to the touch, or start to hiss, swell, or make popping noises. As long as it is safe to do so,\n\n\n\nUnplug the battery charger from the wall outlet!\nLeave the battery in the Ammo container and dump sand from the white bucket over the battery.\nFlip the Ammo container lid over the battery but DO NOT close the lid tightly as this could create a bomb.\nPULL FIRE ALARM and/or CALL FIREDEPT IMEDIATLY. If safe to unplug XT90 battery from jumper and remove ammo can/bucket with the battery in it to outside the build do so. If not safe then leave it.\nEvacuate the building but remain in the building area and look for first responders so you can tell them about the hazard.\n\nf.Report the incident to Jacob and Harper so they can file the EHS report.\n\nEnsure that the sperate XT90 jumper line (not the line on the batter itself) is properly plugged into the Venom charger and that the “battery balance” adaptor is also plugged into the charger on the same charging channel as the XT90 jumper. VERY IMPORTANT: make sure the XT90 jumper cables are plugged in correctly (Red to Red, Black to Black). NEVER plug the yellow XT90 end into the battery without having the Red and Black jumper ends plugged into the charger!  If the jumper is plugged into the battery and the Red and Black ends touch each other it could cause a fire! \n\n\n\n4.Turn on the power for the charger; make sure the charger is plugged into an outlet.\n\nUse the “CHANNEL” button to select which of the 4 channels you want to charge the battery on.\nYou want to use the “LiPo BALANCE” program. DO NOT USE the “LiPo CHARGE” program as this will not necessarily charge each cell in a balanced manor. Depending on what was previously selected you may have to use the Stop, Decrease, Increase, and Enter buttons to find the correct program.\n\n\n\n\nWe want to charge with 7.0A, 14.8V, and (4S). If these parameters are not correct you can change them by pressing enter (short press, not long press and hold).\n\n\n\nPlug the battery/Batteries (Both the XT90 and the multicolored monitoring cable into the charger using the jumper lines from 2 above.\n\n\n\nTo charge the batteries, long press the start button. The charger will check that the battery is correctly detected and display both the number of cells. If the number of cells for “R:” and “S:” do not match then something is wrong and you need to recheck the battery connections and settings.\nIf both “R:” and “S:” read “4SER” then you can press short press the ENTER button to begin charging. (if you forget to press enter this second time, the batteries will not charge even if they are plugged in).\nOnce charging begins, DO NOT LEAVE THE ROOM (even for 30 seconds) UNLESS YOU TELL SOMEONE SO THEY CAN WATCH OVER THE BATTERIES\nYou can check the status of the batteries on the displayed screen. Pushing “INC” or “DEC” will take you to a second screen where you can see the status of each of the four cells in the battery.\nAfter a long period of time, the charger may time out and you will need to restart it (press start button twice) if the battery is not all the way charged.\n\n· The cells should always be within 0.1-0.2 of each other; if they are not, let someone know.\n· The charger will get hot so keep it in cool area."
  },
  {
    "objectID": "protocols/Drones/Pix4Dmapper_Stitch_RE-mx/index.html",
    "href": "protocols/Drones/Pix4Dmapper_Stitch_RE-mx/index.html",
    "title": "Stitching with Pix4Dmapper",
    "section": "",
    "text": "How to Use Pix4Dmapper to Stitch Drone Images (Micasense Camera – 10 Band (RE-mx))\n\nPix4Dmapper is installed on the lambda2 in the dry lab (NOT the Linux lambda)\nCheck what flights need to be stitched on the To Be Stitched List_2022: Teams &gt; UAV Missions &gt; Files\nLaunch Pix4Dmapper on desktop\nSelect New Project with Camera Rigs\n\nName file FlightDate(YYMMDD)_camera(RE-mx)_FieldName_Pix4D\nCreate In: This PC &gt; Desktop &gt; Pix4D &gt; 2022 Fields &gt; “Your Field Name” &gt; 1 Folder called FlightDate(YYMMDD)_drone(RE-mx)_FieldName_Pix4D\n\nSelect Images, Click the Add Images… button, file explorer will open.\n\nNavigate to This PC &gt; wldata (Under Network locations) &gt; Field_Data_2022 &gt; UAV_images_by_field_2022 &gt; Select a Field &gt; Select a folder by Date (YYMMDD) and drone type (RE-mx only)\nYou’ll see 10 folders, you will add all the images from all the folders\nCtrl + A to select all of the JPGs in the folder, Click Open (images will be selected, green check will appear) then Click Next &gt;\n\nDefine the Camera Rig\n\nEnsure the Rig Model selected is RedEdge-M, parameters are saved, Click Next &gt;\n\nKeep default Image Properties, Click Next &gt;\nKeep default settings of Select Output Coordinate System, Click Next &gt;\nProcessing Options Template will open, under Standard select Ag Multispectral (Do not check the box next to Start Processing), Click Finish\nAfter the map loads, Import the GCPs\n\nSelect the Project tab\nSelect GCP/MTP Manager…, the GCP Manager Window will open\nIn the GCP Coordinate System click the Edit… button, the Select GCP Coordinate System window will open\nCheck the Advanced Coordinate Options box\n\n\n\nClick the From List… button under the Known Coordinate System [m] search bar, Coordinate System window will open\nSelect the following from the dropdown lists 1. Datum: WGS 1984  2. Coordinate System: WGS 84 (Top of list, look for the globe\n\nIn the GCP/MTP Table click the Import GCPs… button, the Import Ground Control Points window will open\nCoordinates order: Longitude, Latitude, Altitude\nClick Browse…, navigate to This PC &gt; Desktop &gt; Pix4D &gt; 2022 Fields &gt; GCPs &gt; Select your field’s .txt file, Click Open, Click OK, Click OK. The GCPs will appear as blue crosses on the map\n\n\n\nSave Project!\n\nInitial Processing\n\nClick the Processing tab on the bottom left side of the window, check the box next to 1. Initial Processing (ensure other boxes are unchecked)\nAccept Default options, Start Processing (~30 min)\n\nMarking GCPs\n\nAfter initial processing switch to rayCloud view on the left side of the Pix4D Window, Click on the blue GCP marker and Images will open up on the right (I like to uncheck Cameras and Rays for a cleaner map)\nYou can press hold down to move around the images and zoom in and out to find the GCP (they may not be in every image, that’s ok)\nClick on the center of the GCP to mark it, repeat on 8-15 images. Do this for each GCP.\nIn the Selection panel above the Image panel, Click the Apply button occasionally to update and mark the GCPs\nSelect the Process tab at the top of the window\nSelect Reoptimize (this will take about 10 minutes), a Warning will come up, Click OK\n\n\n\nGenerate a new Quality Report by Clicking the Process tab and Selecting Generate Quality Report (this will take about 15 minutes). Ensure there are green checks next to the 5 parameters in the Quality Check (if not, troubleshoot w/ help from the Pix4D website)\n\n\n\nSave Project!\n\nPoint Cloud and Mesh and DSM, Orthomosaic and Index\n\nCheck the box next to 2. Point Cloud and Mesh and 3. DSM, Orthomsaic and Index in the processing window (be sure to uncheck the previous step so Pix4D doesn’t rerun and take even longer),\nClick Processing Options for step 3. DSM, Orthomosaic and Index\nGo to the Index Calculator tab to calibrate the 10 bands\n\n\n\nSelect the drop down box next to the Correction Type: and select Camera and Sun Irradiance\nClick Calibrate, the calibration image will open a blue box will appear on. Move the box corners to each corner of the square on the calibration panel (if the auto selected image isn’t good quality/shadows/etc. you can Browse at the top of the window and select a better image)\nEnter these reflectance factors into each camera 1. Blue – 0.54011 2. Green – 0.54109 3. Red – 0.53888 4. NIR – 0.53488 5. RedEdge – 0.53795 6. Blue 444 – 0.53983 7. Green 531 – 0.54076 8. Red 650 – 0.53960 9. RedEdge 705 – 0.53825 10. RedEdge 740 – 0.53700\n\n\n\n\nAccept all other default settings and Start processing steps 2. and 3. (~1.5 hrs)\nSave Project, update To Be Stitched List! 😊"
  },
  {
    "objectID": "protocols/Drones/Pix4Dmapper_Stitch_RE-mx/index.html#instructions",
    "href": "protocols/Drones/Pix4Dmapper_Stitch_RE-mx/index.html#instructions",
    "title": "Stitching with Pix4Dmapper",
    "section": "",
    "text": "How to Use Pix4Dmapper to Stitch Drone Images (Micasense Camera – 10 Band (RE-mx))\n\nPix4Dmapper is installed on the lambda2 in the dry lab (NOT the Linux lambda)\nCheck what flights need to be stitched on the To Be Stitched List_2022: Teams &gt; UAV Missions &gt; Files\nLaunch Pix4Dmapper on desktop\nSelect New Project with Camera Rigs\n\nName file FlightDate(YYMMDD)_camera(RE-mx)_FieldName_Pix4D\nCreate In: This PC &gt; Desktop &gt; Pix4D &gt; 2022 Fields &gt; “Your Field Name” &gt; 1 Folder called FlightDate(YYMMDD)_drone(RE-mx)_FieldName_Pix4D\n\nSelect Images, Click the Add Images… button, file explorer will open.\n\nNavigate to This PC &gt; wldata (Under Network locations) &gt; Field_Data_2022 &gt; UAV_images_by_field_2022 &gt; Select a Field &gt; Select a folder by Date (YYMMDD) and drone type (RE-mx only)\nYou’ll see 10 folders, you will add all the images from all the folders\nCtrl + A to select all of the JPGs in the folder, Click Open (images will be selected, green check will appear) then Click Next &gt;\n\nDefine the Camera Rig\n\nEnsure the Rig Model selected is RedEdge-M, parameters are saved, Click Next &gt;\n\nKeep default Image Properties, Click Next &gt;\nKeep default settings of Select Output Coordinate System, Click Next &gt;\nProcessing Options Template will open, under Standard select Ag Multispectral (Do not check the box next to Start Processing), Click Finish\nAfter the map loads, Import the GCPs\n\nSelect the Project tab\nSelect GCP/MTP Manager…, the GCP Manager Window will open\nIn the GCP Coordinate System click the Edit… button, the Select GCP Coordinate System window will open\nCheck the Advanced Coordinate Options box\n\n\n\nClick the From List… button under the Known Coordinate System [m] search bar, Coordinate System window will open\nSelect the following from the dropdown lists 1. Datum: WGS 1984  2. Coordinate System: WGS 84 (Top of list, look for the globe\n\nIn the GCP/MTP Table click the Import GCPs… button, the Import Ground Control Points window will open\nCoordinates order: Longitude, Latitude, Altitude\nClick Browse…, navigate to This PC &gt; Desktop &gt; Pix4D &gt; 2022 Fields &gt; GCPs &gt; Select your field’s .txt file, Click Open, Click OK, Click OK. The GCPs will appear as blue crosses on the map\n\n\n\nSave Project!\n\nInitial Processing\n\nClick the Processing tab on the bottom left side of the window, check the box next to 1. Initial Processing (ensure other boxes are unchecked)\nAccept Default options, Start Processing (~30 min)\n\nMarking GCPs\n\nAfter initial processing switch to rayCloud view on the left side of the Pix4D Window, Click on the blue GCP marker and Images will open up on the right (I like to uncheck Cameras and Rays for a cleaner map)\nYou can press hold down to move around the images and zoom in and out to find the GCP (they may not be in every image, that’s ok)\nClick on the center of the GCP to mark it, repeat on 8-15 images. Do this for each GCP.\nIn the Selection panel above the Image panel, Click the Apply button occasionally to update and mark the GCPs\nSelect the Process tab at the top of the window\nSelect Reoptimize (this will take about 10 minutes), a Warning will come up, Click OK\n\n\n\nGenerate a new Quality Report by Clicking the Process tab and Selecting Generate Quality Report (this will take about 15 minutes). Ensure there are green checks next to the 5 parameters in the Quality Check (if not, troubleshoot w/ help from the Pix4D website)\n\n\n\nSave Project!\n\nPoint Cloud and Mesh and DSM, Orthomosaic and Index\n\nCheck the box next to 2. Point Cloud and Mesh and 3. DSM, Orthomsaic and Index in the processing window (be sure to uncheck the previous step so Pix4D doesn’t rerun and take even longer),\nClick Processing Options for step 3. DSM, Orthomosaic and Index\nGo to the Index Calculator tab to calibrate the 10 bands\n\n\n\nSelect the drop down box next to the Correction Type: and select Camera and Sun Irradiance\nClick Calibrate, the calibration image will open a blue box will appear on. Move the box corners to each corner of the square on the calibration panel (if the auto selected image isn’t good quality/shadows/etc. you can Browse at the top of the window and select a better image)\nEnter these reflectance factors into each camera 1. Blue – 0.54011 2. Green – 0.54109 3. Red – 0.53888 4. NIR – 0.53488 5. RedEdge – 0.53795 6. Blue 444 – 0.53983 7. Green 531 – 0.54076 8. Red 650 – 0.53960 9. RedEdge 705 – 0.53825 10. RedEdge 740 – 0.53700\n\n\n\n\nAccept all other default settings and Start processing steps 2. and 3. (~1.5 hrs)\nSave Project, update To Be Stitched List! 😊"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Writings from the Lab",
    "section": "",
    "text": "Daniel Kick"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/250211_Spring_Preparation/index.html",
    "href": "posts/DeepLearningCommunityofPractice/250211_Spring_Preparation/index.html",
    "title": "Spring Session: Preparation and Schedule",
    "section": "",
    "text": "Hello and welcome to our deep learning spring session! We are iterating on the material we discussed over the during the fall with an eye towards application, logistical details, and when you might select a neural network rather than another model. If someone has directed you to this page and you’d like to be added to the mailing list, please get in touch with us."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/250211_Spring_Preparation/index.html#welcome",
    "href": "posts/DeepLearningCommunityofPractice/250211_Spring_Preparation/index.html#welcome",
    "title": "Spring Session: Preparation and Schedule",
    "section": "",
    "text": "Hello and welcome to our deep learning spring session! We are iterating on the material we discussed over the during the fall with an eye towards application, logistical details, and when you might select a neural network rather than another model. If someone has directed you to this page and you’d like to be added to the mailing list, please get in touch with us."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/250211_Spring_Preparation/index.html#what-we-have-planned",
    "href": "posts/DeepLearningCommunityofPractice/250211_Spring_Preparation/index.html#what-we-have-planned",
    "title": "Spring Session: Preparation and Schedule",
    "section": "What We Have Planned",
    "text": "What We Have Planned\nBelow is our “working draft” for the spring. Based on the feedback survey we’re focusing more on application and less on theory. This means that it will be important to have a good grasp on python’s fundamentals, so you may wish to review the recording of Daniel’s introduction lecture from the fall. Here is a tentative schedule\n\n\n\n\n\nMonth\nDay\nSession\nDeep.Dive\n\n\n\n\nFeb.\n11\nDiscussing the schedule & environmental setup\n\n\n\nFeb.\n18\nData preparation, Model API, Discuss tuning and experiment tracking\n\n\n\nFeb.\n25\nIntro. models for language and regression tasks\n\n\n\nMar.\n4\nNo Session (Maize Meeting)\n\n\n\nMar.\n11\nDiscussion: Learning Projects Planning\n\n\n\nMar.\n18\n\n\n\n\nMar.\n25\nImplementing a transformer (1/2)\nBut what is a GTP 0h27, Visualizing Attention 0h26 , Generatively Pretrained Transformer 1h56 , How might LLMs store facts 0h22\n\n\nMar.\n30\n\n\n\n\nApr.\n8\nImplementing a transformer (2/2)\n\n\n\nApr.\n15\n\n\n\n\nApr.\n22\nPresentation: Learning Project Results\n\n\n\nApr.\n29"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/250211_Spring_Preparation/index.html#logistical-notes",
    "href": "posts/DeepLearningCommunityofPractice/250211_Spring_Preparation/index.html#logistical-notes",
    "title": "Spring Session: Preparation and Schedule",
    "section": "Logistical Notes",
    "text": "Logistical Notes\nTo focus on application and allow for easier re-use of materials there is now a repository on github for our group: dlcp2025. This repository will hold classes, small datasets, environment files and the like and will be updated as we go.\nTo ensure that the code we write together runs on everyone’s computer we’re recommending the following set of tools. You’re welcome to use a different set, but we may not be able to help if you run into issues.\n\nVisual Studio Code - A common IDE that works well with the Windows subsytem for linux and remote machines)\nMicrosoft’s Live Share plugin - Provides a way to collaboratively edit a code (think “google docs for code” or Teletype).\nA linux / *nix shell - If on windows, use the WSL. If on OSX use the built in terminal.\nuv - A fast python package manager. This will take care of enviroment creation and management.\n\nIf you have a conda/mamba enviroment that you already are using for deep learning, you’re welcome to use that as well. However you will likely need to make additions to it as we add tools beyond torch for experiment tracking and hyperparameter tuning."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/250211_Spring_Preparation/index.html#setup",
    "href": "posts/DeepLearningCommunityofPractice/250211_Spring_Preparation/index.html#setup",
    "title": "Spring Session: Preparation and Schedule",
    "section": "Setup:",
    "text": "Setup:\nAssuming you have the recommended tools, start by cloning the shared repository to your local machine. You can do this from the *nix shell or through GitHub desktop.\nOnce downloaded you should see at minimum .toml file and a .lock file. These are the files that uv will use to build the virtual enviroment for us.\n$cd ./path/to/GitHub/directory\n$git clone https://github.com/DanielKick-USDA/dlcp2025.git\n$cd dlcp2025/\n$ ls\npyproject.toml  uv.lock\nIf you saved the repository outside the WSL filesystem (e.g. in Documents) you’ll need to make a note of where it is. Windows directories are accessible like so: /mnt/c/Users/YOUR.USER.NAME/GitHub/dlcp2025/. You might consider creating a symbolic link in your linux home that points to this folder (see Tip: Make your life easier with Symbolic Links in WSL)\nNow run uv sync. This will set up the environment. It may take a few minutes (4m 37s on my machine). You’ll see something like this:\n$ uv sync\nUsing CPython 3.11.11\nCreating virtual environment at: .venv\nResolved 151 packages in 15ms\n░░░░░░░░░░░░░░░░░░░░ [0/146] Installing wheels...                                                                       warning: Failed to hardlink files; falling back to full copy. This may lead to degraded performance.\n         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\n██████████████████░░ [136/146] scikit-learn==1.6.1\nWe can list hidden files to confirm we now have a .venv (with our virtual environment)\n$ ls -a\n.  ..  .git  .venv  pyproject.toml  uv.lock\nBecause we’re installing a good number of deep learning and machine learning tooling, this .venv directory will be sizable.\n$ du -h -d 1\n116K    ./.git\n5.3G    ./.venv\n5.3G    ."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/250211_Spring_Preparation/index.html#using-a-.venv-with-vscode",
    "href": "posts/DeepLearningCommunityofPractice/250211_Spring_Preparation/index.html#using-a-.venv-with-vscode",
    "title": "Spring Session: Preparation and Schedule",
    "section": "Using a .venv with vscode",
    "text": "Using a .venv with vscode\nTo use our uv virtual env we need to first connect to the WSL. Windows treats this like we’re connecting to a remote machine. Click on the connection tile…\n\nThen select “Connect to WSL”.\n\nSelect File -&gt; Open Folder and navigate to your repository (this should be easy if you created a symbolic link as mentioned above. After selecting the the dlcp2025 folder you should see at least the .venv folder and uv’s lock and toml files.\n\nHere we’re looking at a new notebook. On the left you’ll note a “Select Kernel” button. When you click it the drop down in the middle will open. VSCode will likely recognize and suggest the virtual environment and suggest it. Select “.venv”. You should now be set.\nOne quirk to be aware of is that on WSL the first cell you run in a notebook may take many seconds to complete. This is normal and should not be the case for subsequent cells."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/250211_Spring_Preparation/index.html#meeting-notes",
    "href": "posts/DeepLearningCommunityofPractice/250211_Spring_Preparation/index.html#meeting-notes",
    "title": "Spring Session: Preparation and Schedule",
    "section": "Meeting Notes",
    "text": "Meeting Notes\n\nThe slides from today can be found here."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/241105_Bigram/index.html",
    "href": "posts/DeepLearningCommunityofPractice/241105_Bigram/index.html",
    "title": "Deep Learning Discussion: The Bigram Language Model",
    "section": "",
    "text": "Next session we will continue working with the Bigram model. Please finish the video if you are still working on it."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/241105_Bigram/index.html#for-next-session",
    "href": "posts/DeepLearningCommunityofPractice/241105_Bigram/index.html#for-next-session",
    "title": "Deep Learning Discussion: The Bigram Language Model",
    "section": "",
    "text": "Next session we will continue working with the Bigram model. Please finish the video if you are still working on it."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/241105_Bigram/index.html#materials",
    "href": "posts/DeepLearningCommunityofPractice/241105_Bigram/index.html#materials",
    "title": "Deep Learning Discussion: The Bigram Language Model",
    "section": "Materials",
    "text": "Materials\n\nHere is the notebook I wrote during our meeting today (Demo Notebook Part 1)"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/241015_DL_Intro/index.html",
    "href": "posts/DeepLearningCommunityofPractice/241015_DL_Intro/index.html",
    "title": "Summer Session: Kick Off Meeting",
    "section": "",
    "text": "We’re starting off from zero and implementing the core engine that powers deep learning: back propagation.\nFor our next meeting on the 22nd we have an open discussion of backpropagation scheduled. The core video for this session is this one Backpropagation 2h25 in which you’ll build a computational graph and see how values flow forward through this graph to calculate a prediction and backward to calculate gradients. I recommend at minimum watching the video and ideally implementing the code in it as well.\nNote that the in the video description there are links to the notebooks that Andrej Karpathy builds during the video. I find this especially helpful for identifying errors in my code.\nPlease note what you find confusing – someone else probably finds it confusing too!"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/241015_DL_Intro/index.html#for-next-meeting",
    "href": "posts/DeepLearningCommunityofPractice/241015_DL_Intro/index.html#for-next-meeting",
    "title": "Summer Session: Kick Off Meeting",
    "section": "",
    "text": "We’re starting off from zero and implementing the core engine that powers deep learning: back propagation.\nFor our next meeting on the 22nd we have an open discussion of backpropagation scheduled. The core video for this session is this one Backpropagation 2h25 in which you’ll build a computational graph and see how values flow forward through this graph to calculate a prediction and backward to calculate gradients. I recommend at minimum watching the video and ideally implementing the code in it as well.\nNote that the in the video description there are links to the notebooks that Andrej Karpathy builds during the video. I find this especially helpful for identifying errors in my code.\nPlease note what you find confusing – someone else probably finds it confusing too!"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/241015_DL_Intro/index.html#materials",
    "href": "posts/DeepLearningCommunityofPractice/241015_DL_Intro/index.html#materials",
    "title": "Summer Session: Kick Off Meeting",
    "section": "Materials",
    "text": "Materials\n\nHere is a tiny neural network in Excel ( File). Download it and try changing the weights and biases of the layers! In practice we’ll change all the weights and biases at once, but this is nice for building intuition.\nHere are the Meeting Slides."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240924_Python_Plotting/index.html",
    "href": "posts/DeepLearningCommunityofPractice/240924_Python_Plotting/index.html",
    "title": "Python 1: Data Visualization in Python",
    "section": "",
    "text": "Questions to Ponder:\n\nHow do you envision yourself using python and deep learing?\n\nIn the lab?\nFor personal use?\nBoth?\n\nWhat application excites you most?\n\nHow small could you make this project?\nWhen could you work on this?\n\nWhat goals will you be better prepared to achieve by learning these topics?\n\nBefore introducing deep learning we’ll ensure a basic comfort with python.\n\nTo do this we’ll start at a high level (data visualization) then introduce low level concepts.\n\n\nThe presentation slides can be found here."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240924_Python_Plotting/index.html#recap-of-todays-meeting",
    "href": "posts/DeepLearningCommunityofPractice/240924_Python_Plotting/index.html#recap-of-todays-meeting",
    "title": "Python 1: Data Visualization in Python",
    "section": "",
    "text": "Questions to Ponder:\n\nHow do you envision yourself using python and deep learing?\n\nIn the lab?\nFor personal use?\nBoth?\n\nWhat application excites you most?\n\nHow small could you make this project?\nWhen could you work on this?\n\nWhat goals will you be better prepared to achieve by learning these topics?\n\nBefore introducing deep learning we’ll ensure a basic comfort with python.\n\nTo do this we’ll start at a high level (data visualization) then introduce low level concepts.\n\n\nThe presentation slides can be found here."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240924_Python_Plotting/index.html#materials",
    "href": "posts/DeepLearningCommunityofPractice/240924_Python_Plotting/index.html#materials",
    "title": "Python 1: Data Visualization in Python",
    "section": "Materials",
    "text": "Materials\nToday’s slides use data from Allison Horst’s palmerpenguins data package (see also Gorman KB, Williams TD, Fraser WR (2014) ).\nThe files for today are provided zipped together here.\nWhen unzipped you should have\n\nA dataset (penguins.csv)\nA worksheet (penguin_plotly_worksheet.ipynb)\nA set of solutions (penguin_plotly.ipynb)\n\nIf you can’t open the zipped files, download these files, changing the extension of the worksheet and solutions from .txt to .ipynb.\n\npenguins.csv\npenguin_plotly_worksheet.txt\npenguin_plotly.txt"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240924_Python_Plotting/index.html#for-next-meeting",
    "href": "posts/DeepLearningCommunityofPractice/240924_Python_Plotting/index.html#for-next-meeting",
    "title": "Python 1: Data Visualization in Python",
    "section": "For Next Meeting",
    "text": "For Next Meeting\nIf you have not completed the visualization exercises, do. To solidify your understanding, use data you have from a project and plot it with python!\nWhile not required, it’s recommended that you have a learning partner. Talk with folks in your lab or email someone in a different lab if you want someone to learn with!"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240814_intermezzo/index.html",
    "href": "posts/DeepLearningCommunityofPractice/240814_intermezzo/index.html",
    "title": "Summer Session: Meeting 6 Preparation Notes",
    "section": "",
    "text": "Below are a few notes regarding the lecture for the next two sessions (Building a GPT).\nFirst, if you’re looking for a good break point for next week, 38m and 1h1m are good stopping points. By the first point you’ll have seen the Bigram model we worked on weeks ago remade in Pytorch and by the second you’ll have gotten a good preview of some the ideas that will be key in the second half of the video.\nSecond, there are a few potential points for confusing where Python is acting in a way that’s not necessarily intuitive. Karpathy uses a Lambda expressions to define a few functions. Lambdas are handy because they are very terse but this can make them unclear. Here I’ve written out one of these function definitions three ways from most terse to least to show what’s happening here.\n# chars is a list of the unique characters \n# ['\\n', ' ', '!', ...  'y', 'z']\n\n# We've seen a dictionary comprehension previously\n# {'\\n': 0, ' ': 1, '!': 2, ... 'y': 63, 'z': 64}\nstoi  = {s:i for i,s in enumerate(chars)}\n\n# Defining a function by naming a lambda\nencode = lambda s: [stoi[c] for c in s]\n\n# Alternate version\ndef encode1(s):\n    return [stoi[c] for c in s]\n\n# Alternate verson without the list comprehension\ndef encode2(s):\n    out = []\n    for c in s:\n        out.append(stoi[c])\n        \n# Confirm these functions are equivalent\nfor x in ['chars']:\n    print(f'{encode(x)}\\n{encode1(x)}\\n{encode2(x)}')\n\n# [41, 46, 39, 56, 57]\n# [41, 46, 39, 56, 57]\n# [41, 46, 39, 56, 57]\nLater, in the Bigram model class definition there’s a point in the method that generates new tokens (characters) that uses self(idx). What’s happening is that the default method of the class is being called. This default method is what you want to happen when you pass data to the class, so here we want to pass in some sequence and get the next entry in the sequence. We talked about pytorch nn.Module a little last week, but the key idea to know here is that the forward() method makes predictions (it does the forward pass). Thus self(idx) would be the same as writing self.forward(idx) but the former assumes more knowledge of pytorch-isms.\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # ...\n\n    def forward(self, idx, targets=None):\n        # ... \n    \n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            logits, loss = self(idx) # &lt;--- !\n            # ...\nFinally, there are a few points where you may want to stop and compare tensors. For me it’s easier to get a sense of what’s going on by plotting part of the tensors instead of printing them. To do this you can take a 2d slice of the tensor (below we have a 3d tensors so we’re looking at the 0th batch), convert them to numpy arrays and then plot them with plt.imshow(). When comparing two I like to either subtract them (so they’re all 0s if they’re the same) or concatenate them with a spacer and plot them together. Here is example code to do this:\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.imshow(np.concatenate([\n  x[0].numpy(),       # Slice of a tensor\n  np.zeros((T, 1))-1, # Spacer matrix of -1s\n  xbow[0].numpy()     # Slice fo a tensor\n], axis = 1))\nNow we can quickly see that the left two slices are different and the right two are equivalent."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240702_Meeting4/index.html",
    "href": "posts/DeepLearningCommunityofPractice/240702_Meeting4/index.html",
    "title": "Summer Session: Meeting 4",
    "section": "",
    "text": "This supplementary presentation from this week can be found here.\n\nConnections between transforming non-normal data before fitting a linear model to batch normalization\nA motivating puzzle for learning more about the low level operations. I showed a simple MLP to output 0 given 1 or output 1 given 0. This model inconsistently learns. After learning more about backpropagation you’ll be able to easily spot why.\nFinally we discussed why Batch Norm works and the evidence in that it’s not by reducing the change in parameters from cycle to cycle (internal covariance shift “ICS”).\nIn brief in Santurkar, et al. 2018 (“How Does Batch Normalization Help Optimization?”) the authors demonstrate:\n\nA standard model vs batch norm model has no apparent qualitative difference in ICS\nA batch norm model with ICS induced (positive control) neither trains more slowly nor is less performant than a standard model.\nDirect quantification of ICS shows it is often equal to or greater in models with batch norm\nThe range of errors is smaller in models with batch norm in conjunction with other measures this suggests that the “smoothness” of the loss landscape is altered which results in an easier optimization."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240702_Meeting4/index.html#recap",
    "href": "posts/DeepLearningCommunityofPractice/240702_Meeting4/index.html#recap",
    "title": "Summer Session: Meeting 4",
    "section": "",
    "text": "This supplementary presentation from this week can be found here.\n\nConnections between transforming non-normal data before fitting a linear model to batch normalization\nA motivating puzzle for learning more about the low level operations. I showed a simple MLP to output 0 given 1 or output 1 given 0. This model inconsistently learns. After learning more about backpropagation you’ll be able to easily spot why.\nFinally we discussed why Batch Norm works and the evidence in that it’s not by reducing the change in parameters from cycle to cycle (internal covariance shift “ICS”).\nIn brief in Santurkar, et al. 2018 (“How Does Batch Normalization Help Optimization?”) the authors demonstrate:\n\nA standard model vs batch norm model has no apparent qualitative difference in ICS\nA batch norm model with ICS induced (positive control) neither trains more slowly nor is less performant than a standard model.\nDirect quantification of ICS shows it is often equal to or greater in models with batch norm\nThe range of errors is smaller in models with batch norm in conjunction with other measures this suggests that the “smoothness” of the loss landscape is altered which results in an easier optimization."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240702_Meeting4/index.html#preparation-for-next-session",
    "href": "posts/DeepLearningCommunityofPractice/240702_Meeting4/index.html#preparation-for-next-session",
    "title": "Summer Session: Meeting 4",
    "section": "Preparation for Next Session",
    "text": "Preparation for Next Session\nBy request we’re going to pause so that everyone can catch up to the current lecture. We’ll not meet in two weeks but will have Python office hours on Tuesdays so long as at least one person requests them.\nFor next session (tentatively slated for July 30) there are two options:\n\nIf you’re interested predominantly in high level application work through this video on Wavenet.\nIf you’re also interested in the low level work through this video on revisiting backpropagation and then Wavenet.\n\nPlease note that the material on backpropagation is dense. To help visualize the network I have made a diagram of all the tensors in the network here. I recommend printing a copy of it and annotating it with the backward pass. Personally I find the graphical representation to complement to the code nicely.\n\n\nCheers,\nDaniel"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240604_Meeting2/index.html",
    "href": "posts/DeepLearningCommunityofPractice/240604_Meeting2/index.html",
    "title": "Summer Session: Meeting 2",
    "section": "",
    "text": "A bigram model predicts the next letter given a letter\n\nThis can be though of as modeling the transition probabilities\n\nDifferent approaches to learn parameters\n\nNormalized counts\nOptimization"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240604_Meeting2/index.html#key-ideas-from-the-last-session",
    "href": "posts/DeepLearningCommunityofPractice/240604_Meeting2/index.html#key-ideas-from-the-last-session",
    "title": "Summer Session: Meeting 2",
    "section": "",
    "text": "A bigram model predicts the next letter given a letter\n\nThis can be though of as modeling the transition probabilities\n\nDifferent approaches to learn parameters\n\nNormalized counts\nOptimization"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240604_Meeting2/index.html#connections-to-biology",
    "href": "posts/DeepLearningCommunityofPractice/240604_Meeting2/index.html#connections-to-biology",
    "title": "Summer Session: Meeting 2",
    "section": "Connections to Biology:",
    "text": "Connections to Biology:\nWe can think of the bigram probability matrix as a graph where the edges between nodes are the likelihood of moving from one state (here letter) to another.\n\n\n\n\n\n\n\nG\n\n\n\nstart\n\nstart\n\n\n\na\n\na\n\n\n\nstart-&gt;a\n\n\n.5\n\n\n\nb\n\nb\n\n\n\nstart-&gt;b\n\n\n.5\n\n\n\na-&gt;a\n\n\n.1\n\n\n\na-&gt;b\n\n\n.7\n\n\n\nend\n\nend\n\n\n\na-&gt;end\n\n\n.2\n\n\n\nb-&gt;a\n\n\n.3\n\n\n\nb-&gt;b\n\n\n.2\n\n\n\nb-&gt;end\n\n\n.5\n\n\n\n\n\n\n\n\nThrough optimization we can estimate these probabilities of moving between states.\nWe have plenty of these sort of state graphs in biology. We might think about the SIR model in epidemiology.\n\n\n\n\n\n\n\nG\n\n\n\nSusceptible\n\nSusceptible\n\n\n\nInfected\n\nInfected\n\n\n\nSusceptible-&gt;Infected\n\n\n\n\n\nRecovered\n\nRecovered\n\n\n\nInfected-&gt;Recovered\n\n\n\n\n\nRecovered-&gt;Susceptible\n\n\n\n\n\nRecovered-&gt;Recovered\n\n\n\n\n\n\n\n\n\n\nOr we might think about a region of DNA switching between introns and exons.\n\n\n\n\n\n\n\nG\n\n\n\nExon\n\nExon\n\n\n\nExon-&gt;Exon\n\n\n\n\n\nIntron\n\nIntron\n\n\n\nExon-&gt;Intron\n\n\n\n\n\nIntron-&gt;Exon\n\n\n\n\n\nIntron-&gt;Intron\n\n\n\n\n\n\n\n\n\n\nWhile ideas may “rhyme” with bigram models it’s not a perfect match. This sort of model would be more commonly represented as a hidden markov models (e.g. hmmer). That being said, we could build a bigram model using nucleotides or amino acids in the same way that we’re using characters."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240604_Meeting2/index.html#next-session-multilevel-perceptrons-logistics",
    "href": "posts/DeepLearningCommunityofPractice/240604_Meeting2/index.html#next-session-multilevel-perceptrons-logistics",
    "title": "Summer Session: Meeting 2",
    "section": "Next Session: Multilevel Perceptrons + Logistics",
    "text": "Next Session: Multilevel Perceptrons + Logistics\nThe lecture for next session is great. It implements the generative model using Multilevel perceptrons (aka “fully connected” or “dense” networks). It also introduces topics that are useful from theory to practice such as how to select optimizer learning rates.\nFor next session please:\n\nWork through all of this Lecture (1h15m).\nIdentify a dataset and problem for your learning project (more below).\nFind a learning partner if you have not and want one (you can but don’t need to work on a learning project together)."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240604_Meeting2/index.html#reminder-learning-projects",
    "href": "posts/DeepLearningCommunityofPractice/240604_Meeting2/index.html#reminder-learning-projects",
    "title": "Summer Session: Meeting 2",
    "section": "Reminder: Learning Projects",
    "text": "Reminder: Learning Projects\nPutting the ideas we’ve seen into practice will aid your understanding and recall. Previously we discussed this workflow as a way to think about your learning project.\n\n\n\n\n\nflowchart LR\n    a(Learning\\nPartners)\n    b(Learning\\nProblem)\n    c(High Level\\nModel)\n    or{or}\n    d(Low Level\\nModel)\n    e(Advanced\\nModel)\n    f(Advanced\\nInterpretation)\n    \n    a --&gt; b --&gt; c --&gt; or\n    or--&gt; d\n    or--&gt; e\n    or--&gt; f\n\n\n\n\n\n\nI recommend beginning by identifying the dataset or datasets that you’d like to use.\n\nLook for a dataset that is\n\nInteresting or evocative\nSmall (for fast iteration and so we can start building models without a gpu)\nFamiliar (to make it easy to start)\nSimilar ‘shape’ to data you’d like to use in the future\n\n\nIf you have research data that you would like to use consider using a subset to begin with. If you have genomic data, perhaps you use 3 genotypes and 1000 SNPs to start off with. Of if you want to work with hyperspectral images maybe you begin with a single channel and low resolution. After you are comfortable with this reduced data you can take what you’ve learned and apply it to the larger dataset.\nAfter you’ve chosen a dataset you’ll need to select a task to complete. These tend to fall into the categories of classification, regression, and distribution modeling. You should also consider if you want your model to be generative. For many scientific applications we don’t but that shouldn’t limit your imagination!\nFor example let’s say I have a small dataset of mRNA abundances for a few (&lt;200) cells of several cell types. I could try to predict cell type from mRNA counts (classification). Instead I might try to predict some mRNA abundance from other mRNAs’ abundances (regression). Or I could try to detect outliers or measure how dissimilar different cell types are (distribution modeling). With this last approach I could try to produce new observations of mRNA abundances given a cell type1 (generative).\n\nAlternative: Implement a different model using torch\nSince neural networks implement linear operations followed by a non-linear transformation you could try…\n\nImplementing a generalized linear model\nBuilding about a really big linear model\nFitting a linear model with a loss function other than mean squared error\n…"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240604_Meeting2/index.html#footnotes",
    "href": "posts/DeepLearningCommunityofPractice/240604_Meeting2/index.html#footnotes",
    "title": "Summer Session: Meeting 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSampling in a cell type’s region of latent space↩︎"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240507_Meeting0/index.html",
    "href": "posts/DeepLearningCommunityofPractice/240507_Meeting0/index.html",
    "title": "Summer Session: Kick Off Meeting",
    "section": "",
    "text": "We’re starting off from zero and implementing the core engine that powers deep learning: back propagation.\nPlease watch at least the first 1h22m of Andrej Karpathy’s lecture here and try to follow along in your Jupyter Notebook.\nPlease note what you find confusing – someone else probably finds it confusing too!\nReminder: For this you’ll need to install numpy, matplotlib, graphviz, and pytorch.\n\nIf you’re using conda you may need to install graphviz using pip. You can do this from your notebook by typing the command !pip install graphviz. The leading ! will cause this to be treated as a shell command rather than a python command.\nnumpy conda pip\nmatplotlib conda pip\ngraphviz conda pip\npytorch Install Instructions"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240507_Meeting0/index.html#for-next-meeting",
    "href": "posts/DeepLearningCommunityofPractice/240507_Meeting0/index.html#for-next-meeting",
    "title": "Summer Session: Kick Off Meeting",
    "section": "",
    "text": "We’re starting off from zero and implementing the core engine that powers deep learning: back propagation.\nPlease watch at least the first 1h22m of Andrej Karpathy’s lecture here and try to follow along in your Jupyter Notebook.\nPlease note what you find confusing – someone else probably finds it confusing too!\nReminder: For this you’ll need to install numpy, matplotlib, graphviz, and pytorch.\n\nIf you’re using conda you may need to install graphviz using pip. You can do this from your notebook by typing the command !pip install graphviz. The leading ! will cause this to be treated as a shell command rather than a python command.\nnumpy conda pip\nmatplotlib conda pip\ngraphviz conda pip\npytorch Install Instructions"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240507_Meeting0/index.html#materials",
    "href": "posts/DeepLearningCommunityofPractice/240507_Meeting0/index.html#materials",
    "title": "Summer Session: Kick Off Meeting",
    "section": "Materials",
    "text": "Materials\n\nHere is a tiny neural network in Excel ( File). Download it and try changing the weights and biases of the layers! In practice we’ll change all the weights and biases at once, but this is nice for building intuition.\nHere are the Meeting Slides."
  },
  {
    "objectID": "posts/DanielKick/index.html",
    "href": "posts/DanielKick/index.html",
    "title": "Writings",
    "section": "",
    "text": "Sharing n-dimensional data between Python and R\n\n\n\npython\n\n\nr\n\n\nnumpy\n\n\narrays\n\n\ntips\n\n\nparquet\n\n\n\n\n\n\n\nDaniel Kick\n\n\nOct 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAllow named arguments in R’s CLI\n\n\n\nr\n\n\ncli\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nOct 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSQL Strikes Back\n\n\n\nsql\n\n\npostgres\n\n\nparquet\n\n\nintermediate\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRunning GWAS en masse.\n\n\n\ncode\n\n\nbash\n\n\nHPC\n\n\nSLURM\n\n\nintermediate\n\n\nGWAS\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJul 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip: Make your life easier with Symbolic Links in WSL\n\n\n\nbash\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a GWAS Container\n\n\n\ncode\n\n\nr\n\n\nbash\n\n\nintermediate\n\n\ncontainers\n\n\nGWAS\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuick Tip: Write notebooks, run scripts\n\n\n\nbeginner\n\n\ncode\n\n\nbash\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip: Use R to create an email nudge\n\n\n\ncode\n\n\nr\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nApr 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGo Read About R’s Function Call Semantics\n\n\n\ncode\n\n\nintermediate\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMar 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorse is better case study 1\n\n\n\nbeginner\n\n\ncode\n\n\n\n\n\n\n\nDaniel Kick\n\n\nDec 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorse is better and not doing things “right”\n\n\n\ntacit knowledge\n\n\nbeginner\n\n\nintermediate\n\n\ncode\n\n\ndeep learning\n\n\n\n\n\n\n\nDaniel Kick\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse a spreadsheet to manage your CV & Resume\n\n\n\nprofessional development\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nOct 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation as a Super Power\n\n\n\ncode\n\n\nintermediate\n\n\nensembling\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolving the Wrong Problem\n\n\n\ncode\n\n\ndebugging\n\n\ntacit knowledge\n\n\nintermediate\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMaking a “Visible” Neural Network\n\n\n\ncode\n\n\nadvanced\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSave only what you need\n\n\n\ncode\n\n\ndebugging\n\n\ntacit knowledge\n\n\nbeginner\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCapturing Tacit Knowledge Through Blogging\n\n\n\ntacit knowledge\n\n\nbeginner\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Do-nothing Scripting in Bash\n\n\n\ncode\n\n\nbash\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Comment is a Comment.\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrivia: R can have Comments in Tables\n\n\n\ncode\n\n\nr\n\n\nintermediate\n\n\ntips\n\n\ntrivia\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Open a new Interactive Session in Running Session\n\n\n\ncode\n\n\nhpc\n\n\nintermediate\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nOct 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrivia: In Python Missing Isn’t Equal to Itself\n\n\n\ncode\n\n\npython\n\n\nbeginner\n\n\ntips\n\n\ntrivia\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot and the invisible hex code.\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\nggplot\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Cache Intermediate Results with pickle\n\n\n\ncode\n\n\npython\n\n\nintermediate\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMar 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: For those coming from R: Silent In Place Replacement\n\n\n\ncode\n\n\npython\n\n\nr\n\n\nintermediate\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nFeb 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApproximate String Matching\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Reusing Custom Functions\n\n\n\ncode\n\n\npython\n\n\nintermediate\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJul 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: More Readable Data with pretty-print\n\n\n\ncode\n\n\npython\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Find the Graph you want in using a Graph Gallery\n\n\n\ncode\n\n\npython\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTips: Jupyter Plugins\n\n\n\ncode\n\n\npython\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Simulations to Check Your Statistical Intuition\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nsimulation\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMar 23, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot Reordering a Discrete Axis\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nggplot\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMar 11, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot: Parsing Expressions\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nggplot\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMar 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot tips: markdown for italics in plots and adding breaks to color scaling\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\nggplot\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMar 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorkaround for Plotting Dendrograms\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ndendrograms\n\n\n\n\n\n\n\nDaniel Kick\n\n\nFeb 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThought Experiment: Comparing Replicate Experiments’ Conclusions\n\n\n\ncode\n\n\nintermediate\n\n\nr\n\n\nresampling\n\n\n\n\n\n\n\nDaniel Kick\n\n\nFeb 3, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot frienndly corrlation plots with ggcorrplot\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nggplot\n\n\ncorrelations\n\n\n\n\n\n\n\nDaniel Kick\n\n\nFeb 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot font customization\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nggplot\n\n\n\n\n\n\n\nDaniel Kick\n\n\nDec 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling Local Libraries Post Update\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nDec 18, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiagrams as code\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nmermaid\n\n\ngraphviz\n\n\n\n\n\n\n\nDaniel Kick\n\n\nDec 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoman numeral convenince function\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nNov 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResampling doesn’t need to be hard\n\n\n\ncode\n\n\nintermediate\n\n\nr\n\n\nresampling\n\n\n\n\n\n\n\nDaniel Kick\n\n\nNov 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaching your enviroment and why you might not want to.\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Type Matters In gganimate\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nAug 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocument your functions with `roxygen2``\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJul 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReference current dataframe with .\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJul 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInformal profiling with tictoc\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nunlist is handy and you should use it\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot element_text() colors\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nggplot\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 5, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualize the Influence of Outliers\n\n\n\ncode\n\n\nintermediate\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUpdating Results in Word from Table\n\n\n\ncode\n\n\nintermediate\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 26, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGreek Letters in ggplot\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nggplot\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShading with geom_rect\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\nggplot\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t Ignore Your .gitignore\n\n\n\ncode\n\n\nbeginner\n\n\ngit\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCache Intermediate Results\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidyverse ’_at’ variants\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 11, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSyntax Tours: learnxinyminutes\n\n\n\ncode\n\n\nbeginner\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 11, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot: Beware of Factors!\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 6, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUpgrading R versions on Windows\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApply a theme to all your ggplots\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nApr 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClearing all but certain objects\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nApr 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParallel Processing for purrr with furrr\n\n\n\ncode\n\n\nintermediate\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nApr 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoading R functions from source\n\n\n\ncode\n\n\nbeginner\n\n\nr\n\n\ntips\n\n\n\n\n\n\n\nDaniel Kick\n\n\nApr 1, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/DanielKick/241115_r_cli_named_args/index.html",
    "href": "posts/DanielKick/241115_r_cli_named_args/index.html",
    "title": "Allow named arguments in R’s CLI",
    "section": "",
    "text": "When run on the command line, a R script can be set to accept arguments by adding to the script\nIf we call $ Rscript demo.R a b c this line will create a text vector of all the arguments following script (c('a', 'b', 'c')).\nUnlike long options in Linux tools these are unnamed and must be in a fixed order. That’s suboptimal for projects using a mix of tools that do expect named arguments (Some linux and python (argparse)) instead of purely by order. Happily, adding this functionality is simple and requires no libraries.\nWe’ll start by assuming that only named arguments will be passed and that all key/value pairs will be passed together1.\nWe’ll quickly confirm that there are an even number of arguments and then can separate the odd entries into the names (e.g. --inp_file) and the evens into the values (e.g. ./input.txt). The former are set as names for the latter so we now can refer to argments by name rather than index.\nIf we want to get even fancier, we can write a quick function to allow for default values. Since the arguments are named we’ll pass them through as.character() to match the default values (alternately we could name the defaults). All arguments are provided as strings so we can coerce them to the desired data type where needed."
  },
  {
    "objectID": "posts/DanielKick/241115_r_cli_named_args/index.html#footnotes",
    "href": "posts/DanielKick/241115_r_cli_named_args/index.html#footnotes",
    "title": "Allow named arguments in R’s CLI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that the following could be extended to apply only to long options so that an odd number of long and short options would not be an issue.↩︎"
  },
  {
    "objectID": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html",
    "href": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html",
    "title": "Running GWAS en masse.",
    "section": "",
    "text": "As part of a project I’m about to run a lot of analyses. Below are some initial tricks I’ve figured out with (hopefully) more posts like this to come as I find more effective patterns.\nThe goal here was to run a GWAS for some number of phenotypes using one or more genotype file and save all the output into a reasonably named subdirectory. Here’s the directory structure:\n.\n├── gapit.sif     # Discussed previously\n├── gwas.R        #\n├── 01_mkqueue.sh    # We'll focus on these two\n├── 02_next.sh       #    \n├── run_gwas.sbatch     # Loads apptainer and runs 02_next.sh\n│\n├── genotypes\n│   └── 5_Genotype_Data_All_Years_30000.hmp.txt\n│\n├── phenotypes\n│   ├── phno_Ear_Height_cm.txt\n│   ├── ... \n│   └── phno_Yield_Mg_ha.txt\n│\n└── output"
  },
  {
    "objectID": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#context",
    "href": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#context",
    "title": "Running GWAS en masse.",
    "section": "",
    "text": "As part of a project I’m about to run a lot of analyses. Below are some initial tricks I’ve figured out with (hopefully) more posts like this to come as I find more effective patterns.\nThe goal here was to run a GWAS for some number of phenotypes using one or more genotype file and save all the output into a reasonably named subdirectory. Here’s the directory structure:\n.\n├── gapit.sif     # Discussed previously\n├── gwas.R        #\n├── 01_mkqueue.sh    # We'll focus on these two\n├── 02_next.sh       #    \n├── run_gwas.sbatch     # Loads apptainer and runs 02_next.sh\n│\n├── genotypes\n│   └── 5_Genotype_Data_All_Years_30000.hmp.txt\n│\n├── phenotypes\n│   ├── phno_Ear_Height_cm.txt\n│   ├── ... \n│   └── phno_Yield_Mg_ha.txt\n│\n└── output"
  },
  {
    "objectID": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#design",
    "href": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#design",
    "title": "Running GWAS en masse.",
    "section": "Design",
    "text": "Design\nWe could modify gwas.R for each genotype/phenotype pair that needs to be run, create sbatch files for gwas_1.R to gwas_n.R and run them that way. This approach would be relatively fast1 to implement, but depends on 1) not making errors in editing these files and 2) isn’t a transferable solution to new datasets.\nLet’s start by breaking this down into manageable components:\n\nIdentify all the combinations of parameters that the script should be run with.\nPut those combinations in a queue\nUntil the queue is empty…\n\nPop an entry from from the queue\nEdit the R script to contain the new parameters\nRun the script in using the singularity container.\n\n\nThis approach is nice in that since we’re modifying a shared queue we can refer to the same files over and over again. Instead of ending up with n versions of gwas.R and n sbatch files we’ll have only one sbatch file that we’ll run again and again until the queue is empty."
  },
  {
    "objectID": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#building-a-queue-01_mkqueue.sh",
    "href": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#building-a-queue-01_mkqueue.sh",
    "title": "Running GWAS en masse.",
    "section": "Building a queue: 01_mkqueue.sh",
    "text": "Building a queue: 01_mkqueue.sh\nIn this example we have many phenotpe files but only one genotype file. Despite this, instead of hardcoding the genotype we’ll pretend we might have more at some future date (maybe we want to try different snp densities). We’ll build a simple queue by globbing all the hapmaps in genotypes/ and all the phnotype files in phenotypes/ and appending them to queue.txt.\nfor i in ./genotypes/*.hmp.txt; \ndo \n    for j in ./phenotypes/phno*.txt; \n    do  \n        echo $i $j &gt;&gt; ./queue.txt; \n    done; \ndone"
  },
  {
    "objectID": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#modifying-the-r-script-on-the-fly",
    "href": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#modifying-the-r-script-on-the-fly",
    "title": "Running GWAS en masse.",
    "section": "Modifying the R script on the fly",
    "text": "Modifying the R script on the fly\nEach line in queue.txt contains parameters we would like to run gwas.R with. There are different ways to accomplish this. For instance we could change gwas.R to read the top line from the queue file and parse it as parameters. This approach is one that I intend to use in the future but is not what I’ve done here. Here I’ve put the paths that the parameters will replace near the top of the file and changed nothing else.\nphno_file=\"./hmp_phno.txt\"\ngeno_file=\"./geno.hmp.txt\"\nFor each analysis I’ll duplicate this template file, move it to the output directory and then swap out these paths using sed. This works nicely and leaves a record of the code that was run but for more complex scripts might risk unintentionally editing lines you weren’t expecting to."
  },
  {
    "objectID": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#running-a-single-gwas-02_next.sh",
    "href": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#running-a-single-gwas-02_next.sh",
    "title": "Running GWAS en masse.",
    "section": "Running a single GWAS: 02_next.sh",
    "text": "Running a single GWAS: 02_next.sh\nNow we’re to the real core of the system. We need to manage the queue and modify and run the R script. We begin by reading the first line of the queue.txt file with head and using awk to grab the fields2 containing the genotype and phenotype paths.\ngeno_path=$(head -n 1 queue.txt |awk '{print $1}')\nphno_path=$(head -n 1 queue.txt |awk '{print $2}')\nNext we want to overwrite queue.txt without the first line. First we check how many lines are in the file with wc and then use tail to all but the top line. This updated queue we save to a temp file and then use mv to overwrite the queue file. We have to write an intermediate file because the pipe &gt; will activate before tail resulting queue being overwritten with an empty file.\n# find the lenght of the queue so it can be shortened\ntmp=($(wc --lines ./queue.txt))\nnlines=${tmp[0]}\n\n# This the pipe runs before tail so we have to use a temp file and then rename it.\ntail -n $((nlines -1)) queue.txt &gt; queue.tmp && mv queue.tmp queue.txt\nNow we can parse the paths into a directory name we can make use of. We’ll do this by using sed to replace text in this string to remove the directory information ( s|./.*/|| ) and file extension (e.g. s|\\.hmp\\.txt|| ). These file names we’ll concatenate into the name of the output directory ($save_dir).\n# create a new save location\n# remove the ./dir/ and .txt \nphno_name=$(echo $phno_path |sed 's|\\./.*/||' |sed 's|\\.txt||')\ngeno_name=$(echo $geno_path |sed 's|\\./.*/||' |sed 's|\\.hmp\\.txt||')\n\nsave_dir='./output/'$phno_name'__'$geno_name\nThe R script writes its output into the working directory so we’ll make a directory for this set of parameters, cd into it, and copy the template script in.\nmkdir $save_dir\n\n# change pwd \n# copy gwas.R to the save dir\ncd  './'$save_dir\ncp ../../gwas.R ./gwas.R\nUsing sed again, this time with the inplace option -i, we swap out the placeholder paths with a new path relative to the working directory.\n# modify the paths with a leading '.' to be '../'\n# use sed to replace the run settings\nsed -i 's|./hmp_phno.txt|../.'$phno_path'|' ./gwas.R\nsed -i 's|./geno.hmp.txt|../.'$geno_path'|' ./gwas.R\nWith those changes made we can execute this script using the singularity container gapit.sif in the root of the project.\nsingularity exec ../../gapit.sif Rscript ./gwas.R &gt; run.out\nHere’s the whole script all together.\n#!/bin/bash\n\ngeno_path=$(head -n 1 queue.txt |awk '{print $1}')\nphno_path=$(head -n 1 queue.txt |awk '{print $2}')\n\n# find the lenght of the queue so it can be shortened\ntmp=($(wc --lines ./queue.txt))\nnlines=${tmp[0]}\n\n# This the pipe runs before tail so we have to use a temp file and then rename it.\ntail -n $((nlines -1)) queue.txt &gt; queue.tmp && mv queue.tmp queue.txt\n\n# create a new save location\n# remove the ./dir/ and .txt \nphno_name=$(echo $phno_path |sed 's|\\./.*/||' |sed 's|\\.txt||')\ngeno_name=$(echo $geno_path |sed 's|\\./.*/||' |sed 's|\\.hmp\\.txt||')\n\nsave_dir='./output/'$phno_name'__'$geno_name\nmkdir $save_dir\n\n# change pwd \n# copy gwas.R to the save dir\ncd  './'$save_dir\ncp ../../gwas.R ./gwas.R\n\n# modify the paths with a leading '.' to be '../'\n# use sed to replace the run settings\nsed -i 's|./hmp_phno.txt|../.'$phno_path'|' ./gwas.R\nsed -i 's|./geno.hmp.txt|../.'$geno_path'|' ./gwas.R\n\nsingularity exec ../../gapit.sif Rscript ./gwas.R &gt; run.out"
  },
  {
    "objectID": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#putting-it-all-together",
    "href": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#putting-it-all-together",
    "title": "Running GWAS en masse.",
    "section": "Putting it all together",
    "text": "Putting it all together\nThe last thing we need to do is write a simple sbatch file to load apptainer (or singlularity) and run 02_next.sh. I’ve saved this as run_gwas.sbatch.\n# ... \n# Your node settings here\n# ...\n\nmodule load apptainer\nbash 02_next.sh\nNow we can put it all together. First run 01_mkqueue.sh to build the queue and then run sbatch run_gwas.sbatch over and over. Recycling the code from above to find the length of the queue we can set up a simple for loop to run sbatch the needed number of times.\n\nbash 01_mkqueue.sh\nn_runs=$(wc --lines ./queue.txt |awk '{print $1}')\nfor i in $(seq 1 $n_runs); \ndo  \n    echo $i\n    sbatch run_gwas.sbatch \ndone\nAnd that’s it! Now we let this churn away filling output/ with the nicely organized results for all our phenotypes."
  },
  {
    "objectID": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#footnotes",
    "href": "posts/DanielKick/240702_r_GAPIT_container_en_masse/240624_r_GAPIT_container/index.html#footnotes",
    "title": "Running GWAS en masse.",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ni.e. faster than what I’m about to describe↩︎\nThere’s a space between them so `awk ’{print $1}` will return the text before the space.↩︎"
  },
  {
    "objectID": "posts/DanielKick/240624_r_GAPIT_container/index.html",
    "href": "posts/DanielKick/240624_r_GAPIT_container/index.html",
    "title": "Building a GWAS Container",
    "section": "",
    "text": "Today we’re going to look at building a container to run a GWAS1 on a computing cluster. Container construction can be rigorous and necessitate a fair bit of understanding but we can get a lot of the benefits without, say, doing everything we can to make the container small enough to run on the idea of a computer.\nWe’ll start by looking at a simple script to run GWAS with GAPIT. We need only install tidyverse and GAPIT so our container will be simple.\nInstead of Docker we’re going to use singularity (aka apptainer) to avoid needing root access – which we won’t have on an HPC. We begin by creating a definition file with installation instructions for our analysis’ dependencies. Happily, we can build off the Docker ecosystem which lets us avoid a lot of steps by simply using an image from the rocker project with R installed. Additional dependencies are installed as if we were working on the linux command line2.\nNext we build the container. If you have root access the first command will work. Otherwise try the second. The machine that builds the container will need internet access. I’d recommend using a local machine to build this (if you’re on windows, take a look into the Windows Subsystem for Linux ). Depending on the HPC you’re using there might be a node set aside on the HPC for this (using the login nodes for this sort of thing is not recommended – it can interfere with other people accessing it).\nAfter you’ve moved your files and the container to the HPC you can open a shell in the container. This shell should still have access to the host’s files (but here’s a link to how to bind baths in case it doesn’t). From there you can start R in interactive mode or launch the R script."
  },
  {
    "objectID": "posts/DanielKick/240624_r_GAPIT_container/index.html#footnotes",
    "href": "posts/DanielKick/240624_r_GAPIT_container/index.html#footnotes",
    "title": "Building a GWAS Container",
    "section": "Footnotes",
    "text": "Footnotes\n\n\na Genome Wide Association Study identifies regions in a genome that are associated with a trait↩︎\nWhile beyond the scope of this post, you can also create sandbox images which exist as a directory instead of .sif file. Running a such an image in writable mode allows for interactive tinkering.↩︎"
  },
  {
    "objectID": "posts/DanielKick/240516_r_qr_email/index.html",
    "href": "posts/DanielKick/240516_r_qr_email/index.html",
    "title": "Tip: Use R to create an email nudge",
    "section": "",
    "text": "Suppose you’re giving a presentation and you want to make it easy for people to contact you afterwards. Maybe you have your email in the acknowledgements or maybe you make business cards with a qr code to your website.\nThese are good steps but we can go further. Let’s make qr code that nudges people to send an email. I’ve used this to good effect for getting emails of people who would like to be notified when a software project goes live.\nHere’s the plan: 1. Give people your email. 2. Make it easy for them to send you an email. 3. Encourage them to do it now.\nTo accomplish this we’re going to create a mailto link and encode it as a qr code. mailtos are opened in your default mail application so this gets the address where it’ll be used with zero typing. We’ll add some suggested text to the email. This gives the user a starting point and gives us a default subject line to search for later.\nHere’s what this looks like in R. After setting the email and default subject and body text the spaces are replaced with %20 (20 is the ASCII hexdecimal code for space). We concatenate these strings together and then use the marvelous qrcode library to make a graphic that’s ready for a poster, presentation, or card.\n\nlibrary(qrcode)\nlibrary(tidyverse)\n\nWarning: package 'lubridate' was built under R version 4.4.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nemail    = 'your.name@usda.gov'\ninp_subj = 'Re: Envriomental Deep Learning Presentation'\ninp_body = 'Hello from the conference...'\n\n\ninp_subj = str_replace_all(inp_subj, ' ', '%20')\ninp_body = str_replace_all(inp_body, ' ', '%20')\n\ninp_text = paste0(c(\n  '&lt;a href=\"mailto:',email,'?&subject=', inp_subj, '&body=', inp_body, '&gt; &lt;/a&gt;'\n), collapse = '')\n\nplot(qrcode::qr_code(inp_text))"
  },
  {
    "objectID": "posts/DanielKick/231229_scheduling_worse_is_better/index.html",
    "href": "posts/DanielKick/231229_scheduling_worse_is_better/index.html",
    "title": "Worse is better case study 1",
    "section": "",
    "text": "Slurm1 is a tool commonly available on computing clusters for scheduling job. For a while I’ve wanted a local instalition of it so I can queue computational experiments to run while I’m sleeping or gone. Trouble is, it’s never been a high enough priority to devote much time to getting it set up (or convince a sysadmin to help me set it up). Every few months I’ll work through a tutorial until I find I’ve exhausted the time set aside for the task without a working installation. After the most recent cycle of this I resolved to build an imperfect solution instead."
  },
  {
    "objectID": "posts/DanielKick/231229_scheduling_worse_is_better/index.html#thinking-throught-the-system-requirements",
    "href": "posts/DanielKick/231229_scheduling_worse_is_better/index.html#thinking-throught-the-system-requirements",
    "title": "Worse is better case study 1",
    "section": "Thinking throught the system requirements",
    "text": "Thinking throught the system requirements\nLet’s start with what I want my solution to do:\n\nRun jupyter notebooks\nMaintain a queue of jobs to run\nMake sure that resources (here the GPU’s memory) are free for the next job and run jobs as other jobs finish2\nControl the system (add jobs to be run and such)\n\nJupyter notebooks can be run from the command line in a virtual environment (e.g.conda run -n gpu_env jupyter execute notebook0.ipynb).\nA queue could be as simple as a list of notebooks to be run (maintained in a list or a text file) so this requirement is easy to satisfy. Without the In the simplest conception, the jobs could listed (separated by a ;) and run from the command line and each would run in turn. This would not allow for modifying the queue however.\nFreeing up resources is and starting new jobs is more challenging. Ideally when the notebook finishes running it should release the resources but what if there’s an error or bug the process release them? In a notebook we could include os._exit(00) in the last cell to kill the process but if the notebook runs correctly this shouldn’t be an issue. As a fail safe I could keep an eye on what new programs are using the GPU and if they don’t release memory stop them with kill -9 $PID. Not a pretty solution (and if I start another process that uses the GPU, it could get killed) but it will work.\nLastly is a way to control the system. There needs to be a way to modify it’s state even if it’s running in the background. A simple way to do this would be to specify commands files in a specific location and naming convention."
  },
  {
    "objectID": "posts/DanielKick/231229_scheduling_worse_is_better/index.html#implementation",
    "href": "posts/DanielKick/231229_scheduling_worse_is_better/index.html#implementation",
    "title": "Worse is better case study 1",
    "section": "Implementation",
    "text": "Implementation\nI’m using python for this tool since I find it’s more readable than bash. The notebooks I want to schedule are in a single directory so I’ll add a file, SchedulerControlCenter.py, there and a folder for the control files I’ll use to modify the scheduler’s state.\n.\n├── notebook0.ipynb\n├── notebook1.ipynb\n├── ...\n├── SchedulerControlCenter.py\n└── SchedulerFiles\n    └── ctrl0.json\n\nControl\nStarting with the control files, I’d like to be able to add, remove, and move jobs in the queue, prevent a process from being closed (if I’m using the GPU while this is running), print information on the system’s state, and provide some notes to the user (in case I forget details later).\nHere’s a dictionary with keys corresponding to each of these cases.\ndata = {\n    'info'                :[],                  # Print this message\n    'nvidia_base_pids_add':['40082'],           # Prevent a specific PID from being autoclosed. \n    'nvidia_base_pids_del':['40082'],           # Allow a specific PID to be autoclosed.\n    'ipynb_names_read'    :[],                  # Print currently queued notebooks.\n    'ipynb_names_add'     :['notebook0.ipynb'], # Add a notebook (to the end) of the queue\n    'ipynb_names_next'    :['notebook0.ipynb'], # Add a notebook to the beginning of the queue (does not need to be in the queue)\n    'ipynb_names_del'     :['notebook0.ipynb'], # Remove a notebook from the queue\n}\nTo make things easy I’ll create a function to write a dictionary as json and autoname the file (so I don’t overwrite unprocessed commands).\nimport json\n\ndef write_ctrl_json(data: dict):\n    ctrl_files = [e for e in os.listdir('./SchedulerFiles/') if (re.match('ctrl.json', e) or re.match('ctrl\\d+.json', e))]\n    # find the next number to write to. In case 'ctrl.json' and 'ctrl0.json' exist, I'll write to 1\n    max_num = [e.replace('ctrl', '').replace('.json', '') for e in  ctrl_files]\n    max_num = [int(e) for e in max_num if e != '']\n    if max_num == []: max_num = 0\n    else: max_num = max(max_num)\n\n    with open(f'./SchedulerFiles/ctrl{max_num+1}.json', 'w', encoding='utf-8') as f:\n        json.dump(data, f, ensure_ascii=False, indent=4)\n\n\nThe Scheduler\n\nSetup\nIn practice I’ll begin by writing these control files then start the scheduler. I’ll need to parse the instructions, run the first job, check for new instructions, then run the next job and then repeat this until all jobs are complete. In between jobs, I’ll compare the processes running on the GPU and if any are not supposed to be there, stop them.\nI’ll keep track of the original state (nvidia_base_pids) of the GPU using a list of the PIDs that were running on it and current state as a dictionary (nvidia_state) which makes it easy to keep additional information for future use (e.g. how much memory is being used). The queue itself (ipynb_names) can be a simple list of the files to be run. These will need to be populated either from the GPU’s current state (with _init_nvidia()) or from the control files (with _parse_ctrl_jsons()). After that, the scheduler can begin working through the queued notebooks, controlled by the main() method.\nimport os, subprocess, re, json\n\nclass Scheduler():\n  def __init__(self):\n        self.background_mode = background_mode\n        self.nvidia_base_pids = []\n        self.nvidia_state = {}\n        self._init_nvidia()\n        self.ipynb_names = []\n        self._parse_ctrl_jsons()\n        self.main()\nThe GPU’s initial state needs to be recorded which I’ll do by reading all the processes running on it, and saving these PIDs.\n    def _init_nvidia(self):\n        self._read_nvidia()\n        self.nvidia_base_pids = list(self.nvidia_state.keys())\nFinding these processes takes a little doing. From the command line nvidia-smi produces a nicely formatted text table with this information. I’ve used subprocess to capture this information, and then I’ve parsed the table to get the relevant rows and put the information from each into a dictionary (in the list running_processes). Then each dictionary is saved in the self.nvidia_state under it’s process name.\n    def _read_nvidia(self):    \n        x = subprocess.run(\"nvidia-smi\", shell=True, check=True,  capture_output=True)\n\n        x = str(x).split('\\\\n')\n\n        table_blocks = [i for i in range(len(x)) if re.match('.+===+.+', x[i])]\n        table_breaks = [i for i in range(len(x)) if re.match('.+---+.+', x[i])]\n        process_row  = [i for i in range(len(x)) if re.match('.+ Processes: .+', x[i])]\n        start_line = [i for i in table_blocks if i &gt; process_row[0] ][0]\n        end_line   = [i for i in table_breaks if i &gt; process_row[0] ][0]\n\n        running_processes = [x[i] for i in range(start_line+1, end_line)]\n        running_processes = [dict(zip(\n            ['GPU', 'GI', 'CI', 'PID', 'Type', 'ProcessName', 'GPUMem'],\n            [e for e in line.strip('|').split(' ') if e != ''])) for line in running_processes]\n\n        for e in running_processes:\n            self.nvidia_state[e['PID']] = e\n\n\nReading controls\nNow it needs to read the control files. I’ll identify all the json files in ./SchedulerFiles/ that begin with ‘ctrl’ then run each in turn3. After a file is read the method will check if any of the keys are ‘info’ and return a help message4 if so. Then it will go through each key in order and modify self.nvidia_base_pids or self.ipynb_names accordingly. After a file is processed, it will delete it so that the system doesn’t get trapped in a loop – adding the same notebooks to the queue over and over.\n    def _parse_ctrl_jsons(self):\n        ctrl_files = [e for e in os.listdir('./SchedulerFiles/') if (re.match('ctrl.json', e) or re.match('ctrl\\d+.json', e))]\n        if len(ctrl_files) &gt;= 1:\n            for ctrl_file in ctrl_files:            \n                with open('./SchedulerFiles/'+ctrl_file, 'r') as f:\n                    data = json.load(f)\n\n                keys = tuple(data.keys())\n\n                if 'info' in keys:\n                    print(\"\"\"Text ommited for space\"\"\")\n                for key in keys:\n                    if 'nvidia_base_pids_add' == key:\n                        self.nvidia_base_pids += data[key]\n                    if 'nvidia_base_pids_del' == key:\n                        self.nvidia_base_pids = [e for e in self.nvidia_base_pids if e not in data[key]]\n                    if 'ipynb_names_read' == key:\n                        print(self.ipynb_names)\n                    if 'ipynb_names_add' == key:\n                        self.ipynb_names += data[key]\n                    if 'ipynb_names_next' == key:\n                        # technically this could be used to add files and set them to first\n                        self.ipynb_names = data[key]+[e for e in self.ipynb_names  if e != data[key]]\n                    if 'ipynb_names_del' == key:\n                        self.ipynb_names = [e for e in self.ipynb_names if e != data[key]]\n                \n                # remove the file\n                os.unlink('./SchedulerFiles/'+ctrl_file)            \n\n\nRunning the next job\nNow there’s a way to add jobs to the queue there needs to be a method to run them. This method will check if there’s a file to be run, if it exists, and if so use subprocess to run it in the appropriate conda virtual environment.\n    def _advance_queue(self):\n        if len(self.ipynb_names) == 0:\n            pass\n        else:\n            ipynb_name = self.ipynb_names.pop(0)\n            if os.path.exists(ipynb_name) == False:\n                pass\n            else:\n                process = subprocess.Popen(\n                    f\"conda run -n gpu_env jupyter execute {ipynb_name}\".split(), stdout=subprocess.PIPE\n                    )\n                output, error = process.communicate()\n\n\n\nThe main loop\nNow I can use these methods to process all the items in the queue. As long as there are items to process, it will use _advance_queue() to process the one at the front of the queue. Next it will check if there are any new commands to process. Then it will check if the GPU state matches expectations. If there are any PIDs using the GPU that are not listed in the nvidia_base_pids list these will be stopped. Once the queue is exhausted, the script will stop.\n    def main(self):\n        while len(self.ipynb_names) &gt; 0:\n            print(f'Running {self.ipynb_names[0]}')\n            self._advance_queue()\n            # allow for external controls\n            self._parse_ctrl_jsons()\n            self._read_nvidia()\n            # kill all the processes that were not running at the start. \n            for gpu_pid in [e for e in self.nvidia_state.keys() if e not in self.nvidia_base_pids]:\n                subprocess.run(f'kill -9 {gpu_pid}', shell=True)\n        print('No more queued ipynbs. Exiting.')\n\nAll together (and possible improvements)\nThis system works nicely for how quick it was to write up5. There are plenty of improvements that could be made. Suppose you wanted this to run in the background and idle until you added a new job to the queue. One could imaging changing the main() method to achieve this and extending _parse_ctrl_jsons() to get the system to stop idling and shut down. Or suppose you wanted to queue different file types or run notebooks in different environments – _advance_queue() could be extended to do this. Finally, suppose you don’t want to manually exempt PIDs that aren’t using much of the GPU’s resources. Each PID’s usage is available in the nvidia_state dictionary of dictionaries under GPUMem, so a threshold could be set.\nThese changes and other customization for your use case are left as an exercise for the reader.\nEdit 2023-12-20: I’ve added a background mode and option to begin the main loop directly on initialization.\nimport os, subprocess, re, json,  time\nclass Scheduler():\n    def __init__(self, background_mode = False, run_main = False):\n        self.background_mode = background_mode\n        self.exit = False\n        self.nvidia_base_pids = []\n        self.nvidia_state = {}\n        self._init_nvidia()\n        self.ipynb_names = []\n        if run_main:\n            self.main()\n\n\n    def _init_nvidia(self):\n        self._read_nvidia()\n        self.nvidia_base_pids = list(self.nvidia_state.keys())\n\n    def _read_nvidia(self):    \n        x = subprocess.run(\"nvidia-smi\", shell=True, check=True,  capture_output=True)\n\n        x = str(x).split('\\\\n')\n\n        table_blocks = [i for i in range(len(x)) if re.match('.+===+.+', x[i])]\n        table_breaks = [i for i in range(len(x)) if re.match('.+---+.+', x[i])]\n        process_row  = [i for i in range(len(x)) if re.match('.+ Processes: .+', x[i])]\n        start_line = [i for i in table_blocks if i &gt; process_row[0] ][0]\n        end_line   = [i for i in table_breaks if i &gt; process_row[0] ][0]\n\n        running_processes = [x[i] for i in range(start_line+1, end_line)]\n        running_processes = [dict(zip(\n            ['GPU', 'GI', 'CI', 'PID', 'Type', 'ProcessName', 'GPUMem'],\n            [e for e in line.strip('|').split(' ') if e != ''])) for line in running_processes]\n\n        for e in running_processes:\n            self.nvidia_state[e['PID']] = e\n    \n    def _parse_ctrl_jsons(self):\n        ctrl_files = [e for e in os.listdir('./SchedulerFiles/') if (re.match('ctrl.json', e) or re.match('ctrl\\d+.json', e))]\n        if len(ctrl_files) &gt;= 1:\n            for ctrl_file in ctrl_files:            \n                with open('./SchedulerFiles/'+ctrl_file, 'r') as f:\n                    data = json.load(f)\n\n                keys = tuple(data.keys())\n\n                if 'info' in keys:\n                    print(\"\"\"\nThis scheduling tool uses json files to modify its state while running. \nIt will look for json files beginning with 'ctrl' and containing 0 or more digits in \n./SchedulerFiles/ and then run each. This json should be interpretable as a python dictionary.\nFiles are interpreted in the order of the keys but conflicting orders are not recommended. \nExample file:\n{\n    'info'                :[],                            -&gt; Print this message\n    'nvidia_base_pids_add':['40082'],                     -&gt; Prevent a specific PID from being autoclosed. (e.g. if you're running a gpu session interactively)\n    'nvidia_base_pids_del':['40082'],                     -&gt; Allow a specific PID to be autoclosed.\n    'ipynb_names_read'    :[],                            -&gt; Print currently queued notebooks.\n    'ipynb_names_add'     :['SchedulerTestScript.ipynb'], -&gt; Add a notebook (to the end) of the queue\n    'ipynb_names_next'    :['SchedulerTestScript.ipynb'], -&gt; Add a notebook to the beginning of the queue (does not need to be in the queue)\n    'ipynb_names_del'     :['SchedulerTestScript.ipynb'], -&gt; Remove a notebook from the queue\n    'background_mode'     :['True'],                      -&gt; Set to idle if there are no notebooks in the queue\n    'exit'                :[],                            -&gt; Remove a notebook from the queue\n                          \n}\"\"\")\n                for key in keys:\n                    if 'nvidia_base_pids_add' == key:\n                        self.nvidia_base_pids += data[key]\n                    if 'nvidia_base_pids_del' == key:\n                        self.nvidia_base_pids = [e for e in self.nvidia_base_pids if e not in data[key]]\n                    if 'ipynb_names_read' == key:\n                        print(self.ipynb_names)\n                    if 'ipynb_names_add' == key:\n                        self.ipynb_names += data[key]\n                    if 'ipynb_names_next' == key:\n                        # technically this could be used to add files and set them to first\n                        self.ipynb_names = data[key]+[e for e in self.ipynb_names  if e != data[key]]\n                    if 'ipynb_names_del' == key:\n                        self.ipynb_names = [e for e in self.ipynb_names if e != data[key]]\n                    if 'background_mode' == key:\n                        dat = data[key][0]\n                        if type(dat) == str:\n                            if dat.lower() == 'true':\n                                dat = True\n                            elif dat.lower() == 'false':\n                                dat = False\n                            else:\n                                print(f'{dat} not interpretable as True or False')\n                        if type(dat) == bool:\n                            self.background_mode = dat\n                    if 'exit' == key:\n                        self.exit = True\n\n                # remove the file\n                os.unlink('./SchedulerFiles/'+ctrl_file)\n\n    def _advance_queue(self):\n        if len(self.ipynb_names) == 0:\n            pass\n        else:\n            ipynb_name = self.ipynb_names.pop(0)\n            if os.path.exists(ipynb_name) == False:\n                pass\n            else:\n                process = subprocess.Popen(\n                    f\"conda run -n fastai jupyter execute {ipynb_name}\".split(), stdout=subprocess.PIPE\n                    )\n                output, error = process.communicate()\n\n      def main(self):\n        while ((len(self.ipynb_names) &gt; 0) or (self.background_mode)):\n            if ((len(self.ipynb_names) == 0) and (self.background_mode)):\n                # if idling in background mode wait to check for new commands. \n                time.sleep(10)\n                # While idling any new gpu PIDs should be ignored.\n                self._init_nvidia()\n\n            if self.exit: break        \n            self._parse_ctrl_jsons()\n\n            if self.exit: break\n            if (len(self.ipynb_names) &gt; 0):\n                print(f'Running {self.ipynb_names[0]}')\n                self._advance_queue()\n\n                # allow for external controls\n                self._parse_ctrl_jsons()\n                if self.exit: break        \n\n                self._read_nvidia()\n                # kill all the processes that were not running at the start. \n                for gpu_pid in [e for e in self.nvidia_state.keys() if e not in self.nvidia_base_pids]:\n                    subprocess.run(f'kill -9 {gpu_pid}', shell=True)\n            print(f'Running {time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())}')\n        print(    f'Exiting {time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())}')\n            \n\n# Example usage: Start in background mode. While in background mode new gpu processes shouldn't be killed.\n# shlr = Scheduler(background_mode = True, run_main=True)"
  },
  {
    "objectID": "posts/DanielKick/231229_scheduling_worse_is_better/index.html#footnotes",
    "href": "posts/DanielKick/231229_scheduling_worse_is_better/index.html#footnotes",
    "title": "Worse is better case study 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIndeed, it is named after that slurm.↩︎\nThis is less useful than running jobs as soon as resources are available. However, it does mean that implementing the system is a lot easier. The goal is to get most of the benefit quickly, then return and replace or extend this system once my needs outgrow it.↩︎\nIf there are over 10 control files then they might not run in the order the user expects (e.g. ctrl10 would be run between ctrl1 and ctrl2) but I don’t anticipate issuing that many commands at once.↩︎\nFor formatting this message is omitted here. It is:\nThis scheduling tool uses json files to modify its state while running. \nIt will look for json files beginning with 'ctrl' and containing 0 or more digits in \n./SchedulerFiles/ and then run each. This json should be interpretable as a python dictionary.\nFiles are interpreted in the order of the keys but conflicting orders are not recommended. \nExample file:\n{\n    'info'                :[],                            -&gt; Print this message\n    'nvidia_base_pids_add':['40082'],                     -&gt; Prevent a specific PID from being autoclosed. (e.g. if you're running a gpu session interactively)\n    'nvidia_base_pids_del':['40082'],                     -&gt; Allow a specific PID to be autoclosed.\n    'ipynb_names_read'    :[],                            -&gt; Print currently queued notebooks.\n    'ipynb_names_add'     :['SchedulerTestScript.ipynb'], -&gt; Add a notebook (to the end) of the queue\n    'ipynb_names_next'    :['SchedulerTestScript.ipynb'], -&gt; Add a notebook to the beginning of the queue (does not need to be in the queue)\n    'ipynb_names_del'     :['SchedulerTestScript.ipynb'], -&gt; Remove a notebook from the queue\n}\n↩︎\nWriting this explanation took longer than writing the code.↩︎"
  },
  {
    "objectID": "posts/DanielKick/231003_profdev_r_for_cv/index.html",
    "href": "posts/DanielKick/231003_profdev_r_for_cv/index.html",
    "title": "Use a spreadsheet to manage your CV & Resume",
    "section": "",
    "text": "For the past year I’ve been using a spreadsheet and RStudio to manage my resume and curriculum vitae. This post is a pitch for why you might want to do this and an overview of the system. There will be a follow up post on how to get started if you decide to use his approach."
  },
  {
    "objectID": "posts/DanielKick/231003_profdev_r_for_cv/index.html#why-use-a-spreadsheet",
    "href": "posts/DanielKick/231003_profdev_r_for_cv/index.html#why-use-a-spreadsheet",
    "title": "Use a spreadsheet to manage your CV & Resume",
    "section": "Why use a spreadsheet?",
    "text": "Why use a spreadsheet?\nThe key reason why you should use a spreadsheet to manage your resume is that it enables easy filtering and sorting. In essence your spreadsheet becomes a personal work history database from which you can quickly retrieve entries relevant to the application at hand.\nIf instead all your experiences went into an all encompassing text document, tailoring a document to a position would require you to go through each section and cut out most of the entries. To be clear – there are far worse strategies out there. Having a single reference document keeps your information together and means that much of your formatting work is done ahead of time.\nThis is where RStudio comes in. Rmarkdown gives you a way to draw entries from your spreadsheet, filter them, and then turn those entries into beautifully formatted text. Not to mention that you can have R update text for you1."
  },
  {
    "objectID": "posts/DanielKick/231003_profdev_r_for_cv/index.html#overview",
    "href": "posts/DanielKick/231003_profdev_r_for_cv/index.html#overview",
    "title": "Use a spreadsheet to manage your CV & Resume",
    "section": "Overview",
    "text": "Overview\nLet’s see how this works. All the code and data for my resume and cv is on GitHub. Every time I update the spreadsheet or tweak the documents’ aesthetics I add a commit.\n\nUpdating the documents is a 4 step process that takes just a few moments. Earlier this month when a paper of mine was accepted. To update my documents I…\n\nedited the positions.csv document and added a row with the paper’s bibliographic information and url.\nopened my Rmarkdown files Curriculum-Vitae.Rmd and Resume.Rmd and knit them to pdfs.\ncopied the output pdfs to a second repository to share with others\ncommited and pushed these new documents to GitHub (and then did the same for the cv repository)\n\n\nYou might be wondering – what’s with the last two steps? Why move the documents and then push to GitHub instead of keeping them where they are? And why put these on GitHub in the first place?\nPutting these on GitHub (or online for that matter) makes it easy to go from your resume to your online presence (LinkedIn, Orcid, personal website, etc.) from links in your resume. This means that if you embed a link to your resume in your resume then every paper copy you hand out and every business card links to the most up to date version.\n\nUsing a second repository is just to have a cleaner presentation. Every file that isn’t your resume is a distraction – and if your recipient wants to see your GitHub they’ll be just one click away."
  },
  {
    "objectID": "posts/DanielKick/231003_profdev_r_for_cv/index.html#a-more-detailed-look",
    "href": "posts/DanielKick/231003_profdev_r_for_cv/index.html#a-more-detailed-look",
    "title": "Use a spreadsheet to manage your CV & Resume",
    "section": "A More Detailed Look:",
    "text": "A More Detailed Look:\nLet’s take a look at positions.csv.\nThe first column, section, is the categories in your document. Several categories might be presented together (e.g. national_presentations and regional_presentations) but they aren’t assumed to be. The next column, in_resume, is a simple filter. Everything goes into the curriculum vitae, not so with the resume. Next we have institution. This one is a little odd because it includes university or organization names but also lists of authors (e.g. row 44). This is because all the items in this column are formatted the same way. After showing the title of an entry, we want to show this information. Dates are included using the start and end columns and location information (here urls) in loc. Finally, are several description_ columns. Extra information you want can be added to these.\n\nWhen I updated this spreadsheet I already had an entry from when I submitted the paper and put it $bioR\\chiiv$.\n\nOnly three cells had to change, moving it from the in review section to the academic articles section, tweaking the title, and updating the url.\n\nIn RStudio, the CV is ready to go. All I had to do was click the Knit button and wait for the pdf.\nThe resume took ever so slightly more work. For space and aesthetic reasons I use a non-default formatting for my publications. This is a manual step but is not hard at all. All I did was:\n\nRead in the position data and run line 433 position_data %&gt;% print_section('academic_articles'). This produces markdown formatted text. Each line of text is treated as a separate “item” and will be formatted according to some rules.\nCopy the markdown formatted text for the new publication.\nPaste it into the document and tweak the formatting:\n\nUse a smaller font size for everything between &lt;font size=\"1\"&gt; and &lt;/font&gt; .\nBold my name using ***\nDisplay authors and link together as the second line and don’t apply the third line’s formatting rules to anything (N/A)\n\n\n\nWith those edits made I click the “Knit” button again et voilà! Resume updated and ready to go."
  },
  {
    "objectID": "posts/DanielKick/231003_profdev_r_for_cv/index.html#bonus-updating-values-in-the-text",
    "href": "posts/DanielKick/231003_profdev_r_for_cv/index.html#bonus-updating-values-in-the-text",
    "title": "Use a spreadsheet to manage your CV & Resume",
    "section": "Bonus: Updating values in the text",
    "text": "Bonus: Updating values in the text\nOne of the coolest tricks you can do if your resume is in R is to update text dynamically. You don’t have to search through and count how many students you’ve mentored, or papers you published2.\nHere’s a simple example. I want to include how long I’ve been using R, but I don’t want to have to update that by hand. You could calculate this in R like so:\n\ntoday     &lt;- as.Date(format(Sys.time(), \"%Y-%m-%d\")) # Get today's date\nstart_R   &lt;- as.Date(\"2017-01-29\")                   # Set starting date\ndays_diff &lt;- difftime(today, start_R)                # Calc. days elapsed\nyears     &lt;- as.numeric(days_diff) / 365             # Convert to years\nyears     &lt;- round(years)                            # Round\nyears\n\nRMarkdown let’s you embed this calculation in the text. In my documents I have something like this “R Programming (8 years )” which will show up in the pdf as “R Programming (# years)”."
  },
  {
    "objectID": "posts/DanielKick/231003_profdev_r_for_cv/index.html#footnotes",
    "href": "posts/DanielKick/231003_profdev_r_for_cv/index.html#footnotes",
    "title": "Use a spreadsheet to manage your CV & Resume",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee the bonus at the very end.↩︎\nLook at line 430 in Resume.Rmd↩︎"
  },
  {
    "objectID": "posts/DanielKick/230915_solving_the_wrong_problem/index.html",
    "href": "posts/DanielKick/230915_solving_the_wrong_problem/index.html",
    "title": "Solving the Wrong Problem",
    "section": "",
    "text": "There’s a perspective that “it is better to know nothing than to know what ain’t so.”1 In my experience this is certainly the case with debugging because “knowing” will lead you down a rabbit trail of trying to solve the wrong problem.\nThe approach that works well for me is to “trust but verify” your knowledge. If your initial attempts to fix the bug in your code don’t work, take some time to check your assumptions – specifically your assumptions about where the bug is. This slows down your work initially because you’re often testing things that are behaving as you expect expectations, but this saves you from spending a lot of time trying to fix the wrong problems."
  },
  {
    "objectID": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#what-you-know-that-aint-so",
    "href": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#what-you-know-that-aint-so",
    "title": "Solving the Wrong Problem",
    "section": "",
    "text": "There’s a perspective that “it is better to know nothing than to know what ain’t so.”1 In my experience this is certainly the case with debugging because “knowing” will lead you down a rabbit trail of trying to solve the wrong problem.\nThe approach that works well for me is to “trust but verify” your knowledge. If your initial attempts to fix the bug in your code don’t work, take some time to check your assumptions – specifically your assumptions about where the bug is. This slows down your work initially because you’re often testing things that are behaving as you expect expectations, but this saves you from spending a lot of time trying to fix the wrong problems."
  },
  {
    "objectID": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#a-recent-example",
    "href": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#a-recent-example",
    "title": "Solving the Wrong Problem",
    "section": "A recent example:",
    "text": "A recent example:\nI’m writing a model that uses relationships between genes to predict a trait. The problem is that the model isn’t something I can write by hand (there are 6,067 inputs) and is way to slow – I’ve estimated it would take about 1.7 days to complete a single epoch (training cycle).\n\n\n\nHere are all the model’s processes for just two input genes.\n\n\nIn the diagram above, data from each gene (the nodes at the top) is fed into different functions (nodes 0-23) representing associations between different biological processes until they reach the output node (24) which predicts the trait.\nSome nodes share the same input (here node 14 and 10 both need node 11 as input). Under the hood I have the model storing the output of these nodes so it doesn’t have to re-calculate outputs un-necessarily (here the model would look-up the output of node 11 instead of recalculating it). This seems to work nicely but is a little unusual – in over two years this is the first time I’ve manually done this sort of thing.\nBecause of that, when I move my model from a tiny demo data set to the real thing and saw it was slow as molasses I “knew” my model slow because it was storing and retrieving intermediate results.\nOne assumption underlying this is that the model library is effectively designed and optimized such that it’s easier to get worse performance by doing unconventional things than better performance. This isn’t a bad assumption most of the time but we’ll see how it got me thinking about the wrong problem. My thought process went something like this:\n“Okay, so I’m doing something a little unconventional by looking up module outputs. Maybe if I can rewrite the model without this, some ☆Pytorch magic☆ will happen improving training speed.”\n“Hmm, the most straightforward way to write a model would be to chain the inputs and outputs like so”\ninput_1 = x_list[0]\nmodule_2 = x_list[1]\nintermediate_1 = module_1(input_1)\nintermediate_2 = module_2(input_2)\noutput  = module_3(nn.Concatenate([intermediate_1, intermediate_2], axis = 1))\n“But it would be unfeasible to do this because I’d have to write a line for each input and process 8,868 in total… or would it?”\nThis should have seemed like a totally unreasonable thing to do and been where I stopped to think if there was another way to get a speed increase (or tested this by writing a tiny neural net by hand with and without caching results and looked for a tiny difference in speed). However, years ago I met a class deadline by using python2 to write python2 code so this seemed perfectly feasible.\nSo the plan became :\n\nGenerate a boat load strings containing python code\nUse Python’s exec() and eval() functions to run each string\nSit back and think about what a clever idea it was having my code write my code.\n\nSeveral hours later I’ve learned a fair bit about how exec() and eval() handle scope and that their behavior between python2 and python3 has changed and still have no working model. So I decide to print the code I wanted executed to the console paste it (all 8,868 lines of it) into my model definition, and run it.\nThis solution was inelegant but quick to implement and exactly what needed to happen because the model didn’t perform any better. If anything it was slower, so there definitely wasn’t any ☆Pytorch magic☆ happening. This was a big enough surprise that it got me to question if the model was the problem after all instead of running down other rabbit trails."
  },
  {
    "objectID": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#so-wheres-the-problem",
    "href": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#so-wheres-the-problem",
    "title": "Solving the Wrong Problem",
    "section": "So where’s the problem?",
    "text": "So where’s the problem?\nBuilding a model may be the most evocative part of the data science workflow, but the steps that precede it are often as or more important. Choices around how to handle missing or potentially unrepresentative data are important as are how data is stored and moved around. In this case, I wasn’t thinking about these critical choices.\nFor each individual, there are data for genes throughout its genome (x_list, a list where each entry is a gene’s SNPs), and it’s trait of interest (y). Here’s the (simplified) code for this data set:\nclass ListDataset(Dataset):\n    def __init__(self, y, x_list):\n        self.y = y \n        self.x_list = x_list\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        # Get the data for index `idx`\n        y_idx =self.y[idx]\n        x_idx =[x[idx, ] for x in self.x_list]\n        \n        # Move to GPU so model can access it\n        y_idx.to('cuda')\n        x_idx = [x.to('cuda') for x in x_idx]\n        return y_idx, x_idx\nDo you spot what’s happening? When __getitem__ loads an observation, it has to move data from each gene to the GPU. This process isn’t instantaneous and is happening for each of the 6,067 genes every time an observation is loaded.\nTraining a network with a mere 100 observations (batch size of 10) takes 101.89s/it but if all the data is moved to the GPU before its 15% faster at 86.34s/it.\nThat’s nice, but since there are over 80,000 observations, it’s not enough to make training this model feasible. There’s another place we can look for improvements, and that’s the batch size. Increasing the batch size will mean that more observations are being moved to the GPU at a time so it has to happen fewer times. In this example getting all training observations in a single batch makes training 86% faster at 13.44s/it."
  },
  {
    "objectID": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#take-home",
    "href": "posts/DanielKick/230915_solving_the_wrong_problem/index.html#take-home",
    "title": "Solving the Wrong Problem",
    "section": "Take home:",
    "text": "Take home:\nTesting your assumptions (especially while debugging) is like insurance. When you’re on the right track from the start, it’ll cost you a little time that you otherwise wouldn’t have spent but it’ll keep you from spending a lot of time trying to solve the wrong problem.\npost script:\nEven solving the right problem the result may not be what you want. Extrapolating from a more realistic subset of the data results in an estimated 5.6 hours per epoch. Better than 1.7 days, but not a home run ."
  },
  {
    "objectID": "posts/DanielKick/230912_vignettes_for_tacit_knowledge/index.html",
    "href": "posts/DanielKick/230912_vignettes_for_tacit_knowledge/index.html",
    "title": "Capturing Tacit Knowledge Through Blogging",
    "section": "",
    "text": "One of my goals with mentoring is to help others avoid places I got stuck (or get through them more easily) when I was learning. Part of this is writing guides, documentation, SOPs, etc. that I wish I had, but there’s some knowledge that doesn’t fit into a nice long form documents.\nTacit knowledge (e.g. how to approach different problems) or helpful little tricks aren’t well suited to that sort of format. Posts in this series are designed to collect thoughts in this vein to hopefully help out a fellow computational traveler starting their journey.\nTo that end these posts will be a mix of tricks, advice, retrospectives, and maybe a few guides that don’t fit in the protocols/ folder."
  },
  {
    "objectID": "posts/DanielKick/230912_vignettes_for_tacit_knowledge/index.html#advice-à-la-carte",
    "href": "posts/DanielKick/230912_vignettes_for_tacit_knowledge/index.html#advice-à-la-carte",
    "title": "Capturing Tacit Knowledge Through Blogging",
    "section": "",
    "text": "One of my goals with mentoring is to help others avoid places I got stuck (or get through them more easily) when I was learning. Part of this is writing guides, documentation, SOPs, etc. that I wish I had, but there’s some knowledge that doesn’t fit into a nice long form documents.\nTacit knowledge (e.g. how to approach different problems) or helpful little tricks aren’t well suited to that sort of format. Posts in this series are designed to collect thoughts in this vein to hopefully help out a fellow computational traveler starting their journey.\nTo that end these posts will be a mix of tricks, advice, retrospectives, and maybe a few guides that don’t fit in the protocols/ folder."
  },
  {
    "objectID": "posts/DanielKick/230524_linux_do_nothing_scripting/index.html",
    "href": "posts/DanielKick/230524_linux_do_nothing_scripting/index.html",
    "title": "Tips: Do-nothing Scripting in Bash",
    "section": "",
    "text": "Do-nothing scripting is a nice way to blend documenting a protocol with running it. You can use this template as a place to start:\n#!/usr/bin/bash\n#-----------------------------------------------------------------------------#\nSTEP='Step 0:'\necho \"$STEP\"\necho \"Run? (y/n)\"; read -n 1 k &lt;&1\nif [[ $k = n ]] ; then\nprintf \"\\nSkipping $STEP\\n\"; fi\nelse\nprintf \"\\nDoing $STEP\\n\"\n# Code for step here:\nNote, having the condition be on n instead of yes allows for the code (which will vary in length) to be at the end. This makes the control flow easy to see."
  },
  {
    "objectID": "posts/DanielKick/220928_python_trivia_missing/index.html",
    "href": "posts/DanielKick/220928_python_trivia_missing/index.html",
    "title": "Trivia: In Python Missing Isn’t Equal to Itself",
    "section": "",
    "text": "Python quirk I just learned and think is worth sharing. A missing valued doesn’t equal itself.\nHere’s the context: I’m making a list of values from a column that could not be converted to a date. Missing values can’t be converted so they end up in the list (e.g. [nan, '7/5/21 for pass 2']. So how do we discard this empty value? We use a list comprehension to see if the value is equal to itself ( [val for val in my_list if val == val] ) and will get a nan free list."
  },
  {
    "objectID": "posts/DanielKick/220323_python_caching_pickle/index.html",
    "href": "posts/DanielKick/220323_python_caching_pickle/index.html",
    "title": "Tips: Cache Intermediate Results with pickle",
    "section": "",
    "text": "Here’s a useful pattern I’ve been getting a lot of mileage out of lately. If you’re running an analysis that has a time consuming step you can save the result as a python readable “pickle” file. Addendum: In some cases pickling a python objects can sometimes succeed in storing and retrieving data where a library’s built in functions for saving/loading data fails.\nimport pickle as pkl\n\npath = \"./data_intermediates/processed_data.pkl\"\nif os.path.exists(path):\n    processed_data = pkl.load(open(path, 'rb'))\nelse:\n    # Make `processed_data` here\n    pkl.dump(processed_data, open(path, 'wb'))\nThis also lets you batch a process so that you can do more with your resources. For example here’s a list comprehension that will (for each day from 0-287) rearrange the weather data to be in “long” format. This is concise but requires processing the whole list at once which takes a lot of resources.\nsal_long_list = [_get_weather_long(results_list = res,\n                                   current_day = ith_day) for ith_day in np.linspace(start = 0, stop = 287, num = 288)]\nIf we incorporate it into the pattern above we can hold fewer items in memory at a time and then merge them (e.g. with list.extend() ) after the fact.\nfor ii in range(3):\n    file_path = '../data/result_intermediates/sal_df_W_long_part_day'+['0-95', \n                                                                       '96-191', \n                                                                       '192-287'][ii]+'.pkl'\n    if os.path.exists(file_path):\n        sal_long_list = pkl.load(open(file_path, 'rb'))\n\n    else:\n        # The original list comprehension is here, \n        # just made messier by selecting a subset of the indices.\n        sal_long_list = [_get_weather_long(                                \n            results_list = res,\n            current_day = current_day) for current_day in [\n            [int(e) for e in np.linspace(start = 0, stop = 95, num = 96)],   # Batch 1\n            [int(e) for e in np.linspace(start = 96, stop = 191, num = 96)], # Batch 2\n            [int(e) for e in np.linspace(start = 192, stop = 287, num = 96)] # Batch 3\n        ][ii]\n        ]\n        pkl.dump(sal_long_list, open(file_path, 'wb'))"
  },
  {
    "objectID": "posts/DanielKick/210713_python_custom_functions/index.html",
    "href": "posts/DanielKick/210713_python_custom_functions/index.html",
    "title": "Tips: Reusing Custom Functions",
    "section": "",
    "text": "Amendment: For packaging functions also see nbdev.\nI wanted to reuse a custom function across a few scripts without having copies of the same code in each script. The solution I found is to set up a module to hold these functions. This seems straightforward once you know how it’s done.\n\nSet up a directory containing your functions and a blank file called __init__.py.\n\nAdd the directory containing your module directory to the system path (here MaizeModel instead of MaizeModel\\\\library). If you’re on OSX or linux you’ll probably use single forward slashes instead of double backslashes.\n\nFinally import and call your functions.\n\nCaveats:\n\nIt seems that the system path isn’t permanently altered by sys.path.append, so one would need that at the start of the script or modify it some other way.\nIf your custom functions are in the in the same directory as your script, I think you can skip all of this and just import them.\nIf your functions are in a sub-directory of the same directory as your script, I think you can get away without adding the directory to the path."
  },
  {
    "objectID": "posts/DanielKick/210615_python_graph_gallery/index.html",
    "href": "posts/DanielKick/210615_python_graph_gallery/index.html",
    "title": "Tips: Find the Graph you want in using a Graph Gallery",
    "section": "",
    "text": "This or similar sites can be helpful for looking up the right code/library for a plot. You can also find library specific ones. (matplotlib, plotly)\nOne of R’s main plotting libraries, ggplot2, describes plots by layering one component on top of another (e.g. starting with x and y variables, adding points, adding error bars, adding aesthetic adjustments). If that sort of approach appeals to you there is a python version of this library called plotnine (github, example use)."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/240321_r_go_read_about_function_call/index.html",
    "href": "posts/DanielKick/210000_phd_tips/240321_r_go_read_about_function_call/index.html",
    "title": "Go Read About R’s Function Call Semantics",
    "section": "",
    "text": "This is the sort of thing you don’t realize until it would be really useful to access the name of a variable or run text as if it were code. I think the most accessible example of R’s wizardry is in plotting- you pass variables (time, mv) to plot or ggplot instead of strings (“time”, “mv”) and magically you get axis labels. R gets access to the value of a variable and its name and no one notices because it just works."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/220531_tips_ggplot_invisible_hexcode/index.html",
    "href": "posts/DanielKick/210000_phd_tips/220531_tips_ggplot_invisible_hexcode/index.html",
    "title": "ggplot and the invisible hex code.",
    "section": "",
    "text": "If you use an eight digit hex code for specifying a color value, the first two control transparency. Thus, you can set fill = “#00000000” in ggplot to get a boxplot with no fill. Here’s a (somewhat contrived) use case: points on top occlude the cross bar but box on top hides the observations.\n\nlibrary(ggplot2)\ndata.frame(y = 0, x = 1:9) |&gt;\n  ggplot(aes(x = x, y = y))+\n  geom_point(color = 'firebrick')+\n  geom_boxplot(fill = '#00000000')"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/210323_simulation_for_intuition/index.html",
    "href": "posts/DanielKick/210000_phd_tips/210323_simulation_for_intuition/index.html",
    "title": "Using Simulations to Check Your Statistical Intuition",
    "section": "",
    "text": "R’s distribution simulation functions (e.g. dbinom, runif) make it quick and easy to double check one’s intuitions. For example, I’d been thinking that under H0 the distribution of correlations from normal samples should drop off sharply as you go away from 0 such that a shift in correlation from 0 -&gt; 0.1 is much more likely than 0.8 -&gt; 0.9.\nSo I used purrr::map() to run a quick simulation. Here we simulate the null distribution based on 100,000 observations and compute the chance of a value being above 0.7. If it was uniform we would expect ~15% (.03/2) of the distribution to be here but end up with ~1.2% with the drop off.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nset.seed(89745) \ncor_check &lt;- map(1:100000, function(i){ \n  cor(rnorm(10), rnorm(10), method = \"pearson\") \n}) \ncor_check &lt;- data.frame(cor = do.call(rbind, cor_check))\n\nmean(cor_check$cor &gt;= 0.7)*100 \n\n[1] 1.227\n\n#1.227 Percent \n\n\nggplot(cor_check, aes(x = cor))+\n  geom_histogram(binwidth = 0.05)"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/210309_ggplot_expressions/index.html",
    "href": "posts/DanielKick/210000_phd_tips/210309_ggplot_expressions/index.html",
    "title": "ggplot: Parsing Expressions",
    "section": "",
    "text": "A useful trick is to pass expressions into ggplot. Here I’ve used the following as arguments in labs().\nc(\"r11\", \"r1\", \"Ihtk.0\", \"Ihtk.Slope\", \"Ia.0\", \"Ia.Slope\", \"vrest\")\nc(expression(M\\~Omega), expression(M\\~Omega), \"nA\", expression(frac(nA,\nmV)), \"nA\", expression(frac(nA, mV)), \"mV\" )\n\n\n\nimage (30).png\n\n\nYou can also do something like this theme(plot.title = element_text(face=\"italic\")) to add italics to the figure title."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/210221_dendrograms/index.html",
    "href": "posts/DanielKick/210000_phd_tips/210221_dendrograms/index.html",
    "title": "Workaround for Plotting Dendrograms",
    "section": "",
    "text": "I think I have a solution to get decent enough dendrograms without fussing with base graphics.\nThe overview of my workaround is to cluster with pvclust, extract $hclust , plot it as a dendrogram, coerce into a ggplot. This makes it easy enough to replicate the functionality of the colored_bars() function by making additional plots. The function below makes a few plots in addition to the dendrogram. If you end up working with base graphics anyway, dendextend is still worth a look.\nHere’s an example:\n# needed \nlibrary(pvclust)\nlibrary(tidyverse)\nlibrary(dendextend) # for color_labels\nlibrary(ggnewscale) # to accommodate two fill scales  https://github.com/eliocamp/ggnewscale\n\n# recommended\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(patchwork)\nlibrary(scales) # for overiding scientific notation on dendrogram y axis \n\n# Make a demo dataset\ntux &lt;- select(palmerpenguins::penguins, \n              species,\n              bill_length_mm, bill_depth_mm, \n              flipper_length_mm, body_mass_g) \n\ntux &lt;- tux[complete.cases(tux), ]\n\nset.seed(54646)\ntux &lt;- tux[sample(1:nrow(tux), 30), ] # for faster demo clustering\n\n# Example use\no &lt;- \n  mk_hclust_plts(\n    df = mutate(tux, uid = paste(seq(1, nrow(tux)), species, sep = \"-\")),\n    cluster_by = c(\"bill_length_mm\", \"bill_depth_mm\", \n                   \"flipper_length_mm\", \"body_mass_g\"),\n    uid_col = \"uid\",\n    n_clusters = 3,\n    true_groups = \"species\",\n    true_colors = RColorBrewer::brewer.pal(3, \"Set2\"),\n    cluster_colors = RColorBrewer::brewer.pal(3, \"Set1\") \n  )\n\n\n# Patchwork to arrange the output plots\n(o$dendrogram_both+\n    scale_y_continuous(limits = c(-.0001, 0.00022), labels = scales::comma)\n)/ \n  (o$group_compare_tile+\n     # lims(y =  c(2, -2))+ # y axis can be flipped like so\n     theme(legend.position = \"\")\n  ) / \n  (o$heatmap_raw + theme(legend.position = \"right\")) / \n  (o$heatmap_z + theme(legend.position = \"right\")) + \n  patchwork::plot_layout(heights = c(5, .3, 1.25, 1.25))\n\n\n# example 2\n\n# o &lt;- \n# mk_hclust_plts(\n#   df = mutate(iris, uid = paste(seq(1, nrow(iris)), Species, sep = \"-\")),\n#   cluster_by = c(\"Sepal.Length\", \"Sepal.Width\", \"Petal.Length\", \"Petal.Width\"),\n#   uid_col = \"uid\",\n#   n_clusters = 3,\n#   true_groups = \"Species\",\n#   true_colors = RColorBrewer::brewer.pal(3, \"Set2\"),\n#   cluster_colors = RColorBrewer::brewer.pal(3, \"Set1\") \n# )\nps, it’s worth checking your code on sample datasets (penguins,iris, mpg, etc. ). That’ll help iron out weird behavior sooner rather than later. \n(2021-2-21) Edit: The original function here depended on the factor levels of the clusters and true groups to make the color’s consistent between plots (e.g. factors ordered abcdABCD not aAbBcCdD). This breaks down with some cases (e.g. if you start group names with a number (e.g. 0hours)). The below edit uses ggnewscale to fix this.\nmk_hclust_plts &lt;- function(\n  df = unite(M_winxiqr, uid, Experiment, Cell, sep = \"-\"),\n  cluster_by = c(\"vrest\", \"r11\", \"r1\", \"Ihtk.0\", \"Ihtk.Slope\", \"Ia.0\", \"Ia.Slope\"),\n  uid_col = \"uid\",\n  n_clusters = 3,\n  true_groups = \"Condition\",\n  true_colors = RColorBrewer::brewer.pal(3, \"Set2\"),\n  cluster_colors = RColorBrewer::brewer.pal(3, \"Set1\")\n){\n  df &lt;- as.data.frame(df) \n  \n  if (!exists(\"cluster_colors\")){\n    cluster_colors = rainbow(n_clusters)\n  }\n  ## prep\n  # move uid to rowname\n  row.names(df) &lt;- df[[uid_col]]\n  df_groups &lt;- select(df, all_of(true_groups))\n  df &lt;- df[, cluster_by]\n  \n  ## Cluster\n  cluster &lt;- pvclust(t(df),\n                     method.hclust = \"ward.D2\",\n                     method.dist = \"correlation\",\n                     use.cor = \"pairwise.complete.obs\")\n  \n  \n  ## make dendrogram  ####\n  dend &lt;- cluster$hclust %&gt;% \n    as.dendrogram() \n  \n  # iteratively coloring the labels is a workaround to get the \"true\" groups shown\n  dend_labs &lt;- rownames_to_column(df_groups, var = \"rownames\")[, ]\n  dend_labs &lt;- full_join(data.frame(rownames = labels(dend)), dend_labs)\n  for(i in seq_along(unique(df_groups[[true_groups]]))){\n    true_group &lt;- unique(df_groups[[true_groups]])[i]\n    true_color &lt;- true_colors[i]\n    \n    dend &lt;- dend %&gt;% \n      dendextend::color_labels(\n        col = true_color, \n        labels = dend_labs[dend_labs[[true_groups]] == true_group, \"rownames\"]) \n    \n    \n  }\n  \n  dend &lt;- dend %&gt;% \n    set(\"branches_k_color\", \n        k = n_clusters, \n        value = cluster_colors\n    ) %&gt;% \n    set(\"branches_lwd\", 0.7) %&gt;%\n    set(\"labels_cex\", 0.6) \n\n  dend_cluster_only &lt;- dend %&gt;% \n    set(\"labels_colors\",\n        k = n_clusters,\n        value = cluster_colors) %&gt;%\n    as.ggdend()\n  \n  \n  dend &lt;- dend %&gt;% \n    as.ggdend()\n  \n  \n  \n  plt_dend_cluster_only &lt;- ggplot(dend_cluster_only)+\n    theme(axis.ticks.y = element_line(),\n          axis.text.y = element_text(),\n          axis.line.y = element_line())\n  \n  \n  plt_dend &lt;- ggplot(dend)+\n    theme(axis.ticks.y = element_line(),\n          axis.text.y = element_text(),\n          axis.line.y = element_line())  \n  \n\n  ## Add reality ribbon with or without clustering result ####\n  groups_to_plt &lt;- full_join(\n    as.data.frame(dend$labels),\n    rownames_to_column(var = \"label\", df_groups))\n  \n  plt_grouping &lt;- groups_to_plt %&gt;% \n    ggplot(aes_string(x=\"x\", y=\"0\", fill = true_groups))+\n    geom_tile()+\n    scale_fill_manual(values = true_colors)+\n    theme_void()+\n    labs(x = \"\", y = \"\")+\n    theme(legend.position = \"left\")\n  \n  plt_grouping_contrast &lt;- ggplot()+\n    geom_tile(data = groups_to_plt, aes_string(x=\"x\", y=\"0.5\", fill = true_groups))+\n    scale_fill_manual(values = true_colors)+\n    \n    ggnewscale::new_scale(\"fill\") +\n    geom_tile(data = data.frame(x = seq_along(dend_cluster_only$labels$col),\n                                cluster_groups = as.character(as.numeric(as.factor(dend_cluster_only$labels$col)))\n    ),\n    aes_string(x=\"x\", y= \"-0.5\", fill = \"cluster_groups\"),\n    )+\n    scale_fill_manual(values = cluster_colors)+\n    \n    theme_void()+\n    labs(x = \"\", y = \"\")+\n    theme(legend.position = \"left\")\n  \n  \n  ## Add heatmap  ####\n  data_to_plt &lt;- full_join(\n    as.data.frame(dend$labels),\n    rownames_to_column(var = \"label\", df)) \n  \n  data_to_plt &lt;- \n    data_to_plt %&gt;% \n    gather(\"key\", \"value\", \n           names(data_to_plt)[\n             !(names(data_to_plt) %in% c(\"x\", \"y\", \n                                         \"label\", \"col\", \"cex\", \n                                         true_groups))\n           ])\n  \n  plt_heatmap_raw &lt;- data_to_plt %&gt;% \n    ggplot(aes(x, \n               y = key, \n               fill = value))+\n    geom_tile()+\n    scale_fill_viridis_c()+\n    labs(x = \"\", y = \"\")+\n    theme(panel.background = element_blank(),\n          axis.ticks.x = element_blank(),\n          axis.text.x = element_blank(),\n          legend.position = \"left\")\n  \n  \n  plt_heatmap_z &lt;- data_to_plt %&gt;% \n    group_by(key) %&gt;% \n    mutate(mean = mean(value, na.rm = T),\n           sd = sd(value, na.rm = T)) %&gt;% \n    mutate(value = ((value - mean)/sd)) %&gt;% # Now Z scores\n    ggplot(aes(x, \n               y = key, \n               fill = value))+\n    geom_tile()+\n    scale_fill_viridis_c()+\n    labs(x = \"\", y = \"\")+\n    theme(panel.background = element_blank(),\n          axis.ticks.x = element_blank(),\n          axis.text.x = element_blank(),\n          legend.position = \"left\")\n  \n  \n  ## Return plots, manually tweak layout  ####\n  return(\n    list(\n      pvclust_out = cluster,\n      dendrogram_clusters = plt_dend_cluster_only,\n      dendrogram_both = plt_dend,\n      group_tile = plt_grouping,\n      group_compare_tile = plt_grouping_contrast,\n      heatmap_raw = plt_heatmap_raw,\n      heatmap_z = plt_heatmap_z\n    )\n  )\n}"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/210202_ggplot_ggcorrplot/index.html",
    "href": "posts/DanielKick/210000_phd_tips/210202_ggplot_ggcorrplot/index.html",
    "title": "ggplot frienndly corrlation plots with ggcorrplot",
    "section": "",
    "text": "A co-worker ran into issues with corrplot::corrplot() cutting off the title. A useful alternative is ggcorrplot. It makes okay plots with ggplot2’s logic. Not as clean as the above but it’ll work with patchwork and cowplot. Unfortunately, scale_colour_stepsn doesn’t override the scaling.\nlibrary(ggcorrplot)\n\np.mat &lt;- cor_pmat(cor_df)\nggcorrplot(cor(cor_df, use = \"pairwise.complete.obs\"), \n           p.mat = p.mat,\n           insig = \"blank\",\n           type = \"upper\",\n           outline.col = \"white\",\n           colors = RColorBrewer::brewer.pal(n = 9, name = \"PuOr\")[c(1,5,9)]\n           )+\n  labs(title = \"Brian_AP_Delayed\")\n\n\n\nimage (19).png\n\n\nggcorrplot appears to call internal functions which makes modifying it quickly impractical (one would probably be best forking the package and modifying that). I think I have a workaround that gets the same binning behavior:\nAfter the significance matrix (p.mat) is generated overwrite the correlation matrix with the middle value of each desired bin.\n          bkkca      cav1      cav2\nbkkca 1.0000000 0.3452702 0.5603564\ncav1  0.3452702 1.0000000 0.7880727\ncav2  0.5603564 0.7880727 1.0000000\n&gt; # bin the correlations so there are fewer colors used in the figure\n&gt; cor_bins &lt;- seq(-1, 1, length.out = 9)\n&gt; for (i in 1:(length(cor_bins)-1)){\n+   test[test &gt; cor_bins[i] &amp; test &lt; cor_bins[i+1]] &lt;- ((cor_bins[i] + cor_bins[i+1])/2)\n+ }\n&gt; test\n      bkkca  cav1  cav2\nbkkca 1.000 0.375 0.625\ncav1  0.375 1.000 0.875\ncav2  0.625 0.875 1.000\nHere this makes very slight changes to the plot. (legend dropped to not imply a continuous fill) \n(2021-2-2) Last update, this is harder to read up will use the more extreme value to get closer to corrplot\ntest &lt;- seq(-1, 1, length.out = 5)+.0000001\ntest\n#-0.9999999 -0.4999999  0.0000001  0.5000001  1.0000001\nfor (i in 1:(length(cor_bins)-1)){\n  # test[test &gt; cor_bins[i] &amp; test &lt; cor_bins[i+1]] &lt;- (cor_bins[i] + cor_bins[i+1])\n  test[test &gt; cor_bins[i] &amp; test &lt; cor_bins[i+1]] &lt;-   cor_bins[c(i, (i+1))[which.max(abs(cor_bins[i:(i+1)]))]]\n}\ntest\n#-1.00 -0.50  0.25  0.75  1.00\n\n\n\nimage (21).png"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/201218_r_install_local_post_upgrade/index.html",
    "href": "posts/DanielKick/210000_phd_tips/201218_r_install_local_post_upgrade/index.html",
    "title": "Installing Local Libraries Post Update",
    "section": "",
    "text": "I just ran into the same issue after updating R on OSX — none of the libraries are associated with the new version. I tested and abandoned directly copying the libraries over in favor programmatic reinstallation. The downside to this is that it’s slower. The upside is that if you just copy them, R will ask you to update the packages anyway.\nThis won’t work for libraries that aren’t on CRAN but drastically reduces the number of libraries you’re installing by hand. In my case this took care of all but about 3% of the libraries I had installed for 3.6.\nall_packages &lt;- list.files(\"/Library/Frameworks/R.framework/Versions/3.6/Resources/library/\")\ninstalled_packages &lt;- list.files(\"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/\")\nall_packages &lt;- all_packages[!(all_packages %in% installed_packages)]\n\noptions(install.packages.compile.from.source = \"always\")\n\nfor (package in all_packages){\n  try(install.packages(package))\n}\n\noptions(install.packages.compile.from.source = \"interactive\")"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/201120_tips_r_roman_numerals/index.html",
    "href": "posts/DanielKick/210000_phd_tips/201120_tips_r_roman_numerals/index.html",
    "title": "Roman numeral convenince function",
    "section": "",
    "text": "For all you classics folks out there, R has a convenience function just for you! (As long as your numbers aren’t too big). Credit goes to Georgios Karamanis @geokaramanis for teaching me this.\n&gt; as.numeric(as.roman(\"MCXXIII\"))\n[1] 1123\n&gt; as.roman(1123)\n[1] MCXXIII\n&gt; as.roman(11234)\n[1] &lt;NA&gt;"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200919_tips_r_cache_env/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200919_tips_r_cache_env/index.html",
    "title": "Caching your enviroment and why you might not want to.",
    "section": "",
    "text": "If you’re working on data that takes a long time to process, consider adding a the following to your analysis.\nsave.image(file='myEnvironment.RData')\nload('myEnvironment.RData')\nThis will let you reload your environment. One can also by default save your environment when closing Rstudio but that may make it easy to reference objects that no longer are generated in the document itself thereby speeding software rot."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200730_use_roxygen2/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200730_use_roxygen2/index.html",
    "title": "Document your functions with `roxygen2``",
    "section": "",
    "text": "If you want to make future you more positively disposed to present you, then organize your work with R packages, save your custom functions in the /R/ directory. The function roxygen2::roxygenise() will documentation comments for your functions into help pages. For example running roxygenise() with the below function saved produces the attached help page accessible via ?shrug.\n#' @title Print Shrug\n#' @description This function prints a shrug emoji a specified number of times, provided the input value is a numeric greater than zero.\n#' @param n how many shrugs should be printed\n#' @author Daniel Kick (\\email{daniel.r.kick@@gmail.com})\n#' @export\n#' @examples\n#' shrug(5)\n\nshrug &lt;- function(n = 1, ...){\n  if (is.numeric(n) & n&gt;0){\n    for (i in seq(1, n)){\n        cat(\"¯\\_(ツ)_/¯\n\")\n    }\n  }\n}\n\n\n\nimage (12).png"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200625_tips_r_profiling_with_tictoc/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200625_tips_r_profiling_with_tictoc/index.html",
    "title": "Informal profiling with tictoc",
    "section": "",
    "text": "The tictoc library has convenience functions for timing code. Here’s the basic usage relative to timing with base R.\nlibrary(tictoc)\ntic()\n# code here\ntoc()\n\ntic &lt;- Sys.time()\n# code here\ntoc &lt;- Sys.time()\nprint(toc - tic)\nWhere this library excels is when you want to time multiple parts of your code. Each tic pushes the time onto a stack and each toc pops the most recent time from said stack. That means you don’t have to worry about assigning several timing variables even if you want to time nested code.\ntic()\n# stack is 1 deep\nfor (i in 1:10) {\n     tic()\n     # stack is now 2 deep\n     for (j in 1:10){\n          tic()\n          # stack is now 3 deep\n          toc()\n     }\n     toc()\n}\ntoc()"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200605_tips_ggplot_element_text/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200605_tips_ggplot_element_text/index.html",
    "title": "ggplot element_text() colors",
    "section": "",
    "text": "ggplot’s, element_text() comes with a color argument. Why does that matter? It doesn’t just accept atomics, you can hand it a vector! This effectively gives one access to conditional formatting. This works on ggplot2_3.3.0 but “vectorized input to element_text() is not officially supported” so YMMV with newer versions.\nHere’s an example I think it makes an otherwise unbearable figure a little more so without requiring duplicated labels.\nOne thing to be aware of is the ordering of a character/ factor may differ between a data.frame and the plot. In the visualized example, I had mRNA as type character instead of factor so it got alphabetized when it was plotted rather than by the order appearing in the data.frame."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200526_tips_update_word_doc_by_links/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200526_tips_update_word_doc_by_links/index.html",
    "title": "Updating Results in Word from Table",
    "section": "",
    "text": "Handy trick: Write your test results into a summary csv. Then while writing you can insert a link to the cell value for a result into your .doc. After that, changing post hoc corrections, re-sampling iterations, or data QC just requires you to re-run your code and let word refresh all the links.\nYou can also do this from excel, but you’ll have to update the formula for the cell used instead.\n/"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200519_tips_ggplot_greek_letters/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200519_tips_ggplot_greek_letters/index.html",
    "title": "Greek Letters in ggplot",
    "section": "",
    "text": "If you need to include greek letters or special characters in a plot use annotate() instead of geom_text(). On my machine it preforms a lot faster.\n# 57.62977 secs\ngeom_text(aes(x = 15, y = 25, label = \"phi~22.5\"), parse=TRUE)\n# 1.988642 secs\nannotate(\"text\", x = 15, y = 25, parse = TRUE, label = as.character(expression(paste(phi, \" 22.5\"))))"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200512_tips_r_cache_results/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200512_tips_r_cache_results/index.html",
    "title": "Cache Intermediate Results",
    "section": "",
    "text": "Save R objects as rds. These could be works in progress or items that take a while to generate (e.g. re-sampled results). This gives the utility of saving a work space without the dangers.\nlibrary(here)\nsave(df, file = here(\"data\", \"df.rds\"))\n# df is now saved at ./data/df.rds\n\nload(here(\"data\", \"df.rds\"))\n# df is now loaded from ./data/df.rds as df"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200511_tidyverse_at_variants/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200511_tidyverse_at_variants/index.html",
    "title": "Tidyverse ’_at’ variants",
    "section": "",
    "text": "tldr: tidyverse function variants take slightly different input. Testing out a variant or two can save you a lot of debugging time.\nIn tidyverse watch out for inconsistencies in function versions. There are variants of common functions (e.g. mutate(), mutate_all(), mutate_at()) don’t necessarily behave the same way (or how you would expect).\nHere, I was applying an operation to a grouped df where each Experiment contains several FileNames with multiple observations in each. To keep everything reusable I’m using exp instead of Experiment to select the right col.\n&gt; exp = \"Experiment\"\n&gt; rec = \"FileName\"\n\n&gt; df %&gt;%\n+     dplyr::select(\n+       exp, rec, r11, r12\n+     )\n# A tibble: 564 x 4\n   Experiment FileName           r11   r12\n   &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n 1 190808a    190808a_0020.abf  5.06 0.150\n 2 190808a    190808a_0020.abf  5.13 0.160\n 3 190808a    190808a_0020.abf  5.11 0.122\n 4 190808a    190808a_0020.abf  2.49 0.152\n 5 190808a    190808a_0020.abf  2.49 0.195\n# ... with 559 more rows\nAs soon as we do the same thing with group_by() we don’t get the right column even though select() didn’t have an issue with exp.\n&gt; df %&gt;%\n+     dplyr::select(\n+       exp, rec, r11, r12\n+     ) %&gt;%\n+     group_by(exp, rec)\nError: Column `exp` is unknown\nSo we can try explicitly selecting the columns we want as groupings.\n&gt;     df %&gt;%\n+     dplyr::select(\n+       exp, rec, r11, r12\n+     ) %&gt;%\n+     group_by(vars(exp, rec))\nError: Column `vars(exp, rec)` must be length 564 (the number of rows) or one, not 2\nNo dice there. vars() is designed to work with the _at variants so we can try that. et voilà!\n&gt; df %&gt;%\n+     dplyr::select(\n+       exp, rec, r11, r12\n+     ) %&gt;%\n+     group_by_at(vars(exp, rec))\n\n# A tibble: 564 x 4\n# Groups:   Experiment, FileName [69]\n   Experiment FileName           r11   r12\n   &lt;chr&gt;      &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n 1 190808a    190808a_0020.abf  5.06 0.150\n 2 190808a    190808a_0020.abf  5.13 0.160\n 3 190808a    190808a_0020.abf  5.11 0.122\n 4 190808a    190808a_0020.abf  2.49 0.152\n 5 190808a    190808a_0020.abf  2.49 0.195\n# ... with 559 more rows"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200504_tips_r_faster_install/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200504_tips_r_faster_install/index.html",
    "title": "Upgrading R versions on Windows",
    "section": "",
    "text": "If you’re on windows, installr should allow you to copy over the libraries from previous versions."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200420_tips_r_rm/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200420_tips_r_rm/index.html",
    "title": "Clearing all but certain objects",
    "section": "",
    "text": "If you run into a situation where you’re using a ton of memory (e.g. manipulating transcriptomic data, resampling, or working with electrophysiology traces) use rm() to selectively get rid off objects in the environment. A useful pattern is to write out large objects you’ll need in the future, remove them, and then read them back in when you need them. This is usually not important, but when it is, it is.\nIf you’re working interactively and tempted to use rm(list = ls()) consider restarting your r session (ctrl+shift+F10 on windows). Overreliance on rm(list = ls()) is poor form.\nA side note – unlike listing items where the function matching the unix command acts on the environment and a new command acts on the files system (ls() and list.files()) the functions for removing items don’t follow this logic. rm() acts on objects in your environment whereas unlink() acts on system files.\nSimilarly if you want to retain only specific objects you can take this approach:\n # get rid of everything\nrm(list=ls())\n\n# get rid of everything except specific objects and all loaded functions\nrm(list = \n   ls()[!(ls() %in% c(\n   # objects\n   c(\"data1\", \"data2\", \"bool1\", \"list1\"), \n   # functions\n   lsf.str())\n   ) ])"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200401_tips_load_r_functions/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200401_tips_load_r_functions/index.html",
    "title": "Loading R functions from source",
    "section": "",
    "text": "You can keep your working Rmd easier to navigate and less buggy by 1) packaging code into functions and 2) adding them to a companion Rfile. Load your functions with source() in the same block you load your libraries with a relative path, full path, or ideally with here().\nlibrary(here)\nsource(here(\"R\", \"02MoniterGapJunction.R\")) #here's output is effectively ../R/02MoniterGapJunction.R"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the Lab",
    "section": "",
    "text": "Harper LaFond joined the Washburn Lab in 2022 as the lab technician. She received her B.S. in Fisheries and Wildlife and her M.S. in Plant, Insect & Microbial Sciences specializing in entomology, both at the University of Missouri – Columbia. Harper has enjoyed working an array of field technician jobs with all sorts of living things from bugs and birds of prey to sturgeon and river carp. She then worked at the University of Missouri, Grape and Wine Institute as the viticulture extension specialist before joining the maize group at the USDA (she only works with crops that can be fermented into tasty drinks). Harper enjoys listening to podcasts on her walks to and from work, cooking dinner with her husband and sharing the leftovers with their dog, Chase.\n\n\n\n\n\n\n\n\n\nTyler is a plant scientist with training in crop physiology and molecular genetics. He is interested in plant adaptation to natural and agronomic stress, and leveraging genetic diversity to improve plant resilience for a changing climate.\n\n\n\nWorasit’s research focuses on integrating engineering and data science with plant science to advance plant phenomics research. He leverages remote sensing platforms, including proximal sensing, IoT-based systems, drones, and satellites, with various sensor types to collect multi-scale data for phenotyping in plant breeding programs. His current focus is on developing a program enhancing individual plant detection in maize using drone imagery. More information can be found on his LinkedIn, Google Scholar.\n\n\n\nDaniel’s research centers on using deep learning and computational approaches to make the most out of biological data, e.g. by using deep learning to capture how yield in influenced by the complex relationships between genetic, environmental, and management factors. He also runs technical trainings, a deep learning group, and publishes notes and retrospectives here. More information can be found on his website, danielkick.com.\n\n\n\n\n\n\nShawn’s dissertation research focuses on using comparative genomics and phylogenomics to understand the evolution of mustard crops and their wild relatives (tribe Brassiceae) with respect to a shared ancient polyploidy. He is also affiliated with the Bioinformatics and Analytics Core at MU and works on various genomics projects for faculty investigators across MU.\n\n\n\nEmma is in the Plant, Insect, and Microbial Sciences PhD program at the University of Missouri. She works as a USDA-ARS Graduate Student Research Fellow and seeks to determine how different maize genotypes, landraces, and wild relatives perform photosynthesis under cold conditions. This work will ultimately help in developing elite maize cultivars that thrive in early season cooler settings.\n\n\n\n\n\n\nMorgan Mathison joined the lab in May of 2023 as an undergraduate research assistant. She currently is a senior at the University of Missouri studying Plant Science with an emphasis in Breeding, Biology, and Biotechnology. She is also completing a minor in Biology and a certificate in Floral Artistry and Management. She has a passion for plant ecology, evolution, and conservation. Her current research focuses on using drone imaging in order to evaluate cross environmental conditions in maize.\n\n\n\nHenry is a junior in computer science who has been gaining experience in plant science. He has a special focus on distributed systems with high-throughput phenotyping operations."
  },
  {
    "objectID": "about.html#permanent-members",
    "href": "about.html#permanent-members",
    "title": "About the Lab",
    "section": "",
    "text": "Harper LaFond joined the Washburn Lab in 2022 as the lab technician. She received her B.S. in Fisheries and Wildlife and her M.S. in Plant, Insect & Microbial Sciences specializing in entomology, both at the University of Missouri – Columbia. Harper has enjoyed working an array of field technician jobs with all sorts of living things from bugs and birds of prey to sturgeon and river carp. She then worked at the University of Missouri, Grape and Wine Institute as the viticulture extension specialist before joining the maize group at the USDA (she only works with crops that can be fermented into tasty drinks). Harper enjoys listening to podcasts on her walks to and from work, cooking dinner with her husband and sharing the leftovers with their dog, Chase."
  },
  {
    "objectID": "about.html#post-docs",
    "href": "about.html#post-docs",
    "title": "About the Lab",
    "section": "",
    "text": "Tyler is a plant scientist with training in crop physiology and molecular genetics. He is interested in plant adaptation to natural and agronomic stress, and leveraging genetic diversity to improve plant resilience for a changing climate.\n\n\n\nWorasit’s research focuses on integrating engineering and data science with plant science to advance plant phenomics research. He leverages remote sensing platforms, including proximal sensing, IoT-based systems, drones, and satellites, with various sensor types to collect multi-scale data for phenotyping in plant breeding programs. His current focus is on developing a program enhancing individual plant detection in maize using drone imagery. More information can be found on his LinkedIn, Google Scholar.\n\n\n\nDaniel’s research centers on using deep learning and computational approaches to make the most out of biological data, e.g. by using deep learning to capture how yield in influenced by the complex relationships between genetic, environmental, and management factors. He also runs technical trainings, a deep learning group, and publishes notes and retrospectives here. More information can be found on his website, danielkick.com."
  },
  {
    "objectID": "about.html#graduate-students",
    "href": "about.html#graduate-students",
    "title": "About the Lab",
    "section": "",
    "text": "Shawn’s dissertation research focuses on using comparative genomics and phylogenomics to understand the evolution of mustard crops and their wild relatives (tribe Brassiceae) with respect to a shared ancient polyploidy. He is also affiliated with the Bioinformatics and Analytics Core at MU and works on various genomics projects for faculty investigators across MU.\n\n\n\nEmma is in the Plant, Insect, and Microbial Sciences PhD program at the University of Missouri. She works as a USDA-ARS Graduate Student Research Fellow and seeks to determine how different maize genotypes, landraces, and wild relatives perform photosynthesis under cold conditions. This work will ultimately help in developing elite maize cultivars that thrive in early season cooler settings."
  },
  {
    "objectID": "about.html#undergraduate-researchers",
    "href": "about.html#undergraduate-researchers",
    "title": "About the Lab",
    "section": "",
    "text": "Morgan Mathison joined the lab in May of 2023 as an undergraduate research assistant. She currently is a senior at the University of Missouri studying Plant Science with an emphasis in Breeding, Biology, and Biotechnology. She is also completing a minor in Biology and a certificate in Floral Artistry and Management. She has a passion for plant ecology, evolution, and conservation. Her current research focuses on using drone imaging in order to evaluate cross environmental conditions in maize.\n\n\n\nHenry is a junior in computer science who has been gaining experience in plant science. He has a special focus on distributed systems with high-throughput phenotyping operations."
  },
  {
    "objectID": "about.html#post-docs-1",
    "href": "about.html#post-docs-1",
    "title": "About the Lab",
    "section": "Post Docs",
    "text": "Post Docs\n\nPiyush Pandey (PhD)"
  },
  {
    "objectID": "about.html#undergraduate-researchers-1",
    "href": "about.html#undergraduate-researchers-1",
    "title": "About the Lab",
    "section": "Undergraduate Researchers",
    "text": "Undergraduate Researchers\n\nGrace Sidberry \n\n\nMadi Mitchell \nMadi can be found here.\n\n\nMia Ruppel\n\n\nBrady Blanton"
  },
  {
    "objectID": "about.html#staff",
    "href": "about.html#staff",
    "title": "About the Lab",
    "section": "Staff",
    "text": "Staff\n\nKate Guill"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Washburn Lab Resources",
    "section": "",
    "text": "Rover Use in Field\nRover Battery Charging\n\n\n\nStitching Images with Pix4Dmapper\nStitching Images with Pix4Dmapper: RE-mx Drone\nGridding the Reference Flight (QGIS)\n\n\n\n\n\n\nRestarting the Image Script on the Rootbot’s Pi\n\n\n\n\n\n\nMaking QR Codes for Samples and Plants\nTransferring Data Between Computers\n“Do Nothing” Scripting to Progressively Automate tasks\n\n\n\nSingularity: RStudio in a box\nUsing Port Forwarding to Access Your Jupyter Notebook\nMaking a Singularity Container From Your Conda Environment\nUsing your Singularity container with Open OnDemand\n In progress: \nOverview: Suggested HPC Workflow\nUsing Open OnDemand\n\n\n\nAdding Protocols\n In progress: \nWebsite Overview"
  },
  {
    "objectID": "index.html#field-work",
    "href": "index.html#field-work",
    "title": "Washburn Lab Resources",
    "section": "",
    "text": "Rover Use in Field\nRover Battery Charging\n\n\n\nStitching Images with Pix4Dmapper\nStitching Images with Pix4Dmapper: RE-mx Drone\nGridding the Reference Flight (QGIS)"
  },
  {
    "objectID": "index.html#lab-work",
    "href": "index.html#lab-work",
    "title": "Washburn Lab Resources",
    "section": "",
    "text": "Restarting the Image Script on the Rootbot’s Pi"
  },
  {
    "objectID": "index.html#computational-work",
    "href": "index.html#computational-work",
    "title": "Washburn Lab Resources",
    "section": "",
    "text": "Making QR Codes for Samples and Plants\nTransferring Data Between Computers\n“Do Nothing” Scripting to Progressively Automate tasks\n\n\n\nSingularity: RStudio in a box\nUsing Port Forwarding to Access Your Jupyter Notebook\nMaking a Singularity Container From Your Conda Environment\nUsing your Singularity container with Open OnDemand\n In progress: \nOverview: Suggested HPC Workflow\nUsing Open OnDemand\n\n\n\nAdding Protocols\n In progress: \nWebsite Overview"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200410_tips_furrr_is_multithreaded_purrr/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200410_tips_furrr_is_multithreaded_purrr/index.html",
    "title": "Parallel Processing for purrr with furrr",
    "section": "",
    "text": "If you’re iteratively making plots, resampling, or doing another task that would have your reach for a for loop or lapply() use furrr::future_map() instead. furrr gives parallel processing ready versions of tidyverse’s purrr functions (e.g. map(), walk()). It’s easy to install the dependencies and takes a lot of the headache out of parallel processing."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200421_tips_ggplot_theme_default/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200421_tips_ggplot_theme_default/index.html",
    "title": "Apply a theme to all your ggplots",
    "section": "",
    "text": "If you’re applying the same theme to all your graphs, set it globally instead e.g. theme_set(ggplot2::theme_minimal()). If you have a lot of custom changes to your theme, throw those into a function and set that to the global theme."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200506_tips_ggplot_factors/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200506_tips_ggplot_factors/index.html",
    "title": "ggplot: Beware of Factors!",
    "section": "",
    "text": "Don’t get suckered when converting factors! Numeric data can be assigned to a factor type which will throw a wrench in a plot or analysis (1/3) \nA knee jerk reaction would be to convert it to a numeric with as.numeric(). That doesn’t work either. (2/3) \nHowever if you use as.numeric(as.character()) then it works. That’s because factors are ordinal and named so if you convert the type to character first to ensure R is working with the factor names instead of the ranks. (3/3)"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200511_tips_learnxiny/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200511_tips_learnxiny/index.html",
    "title": "Syntax Tours: learnxinyminutes",
    "section": "",
    "text": "Need a quick base R syntax lookup? Check out https://learnxinyminutes.com/docs/r/ . There are even examples with lm() and glm()."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200515_tips_git_gitignore/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200515_tips_git_gitignore/index.html",
    "title": "Don’t Ignore Your .gitignore",
    "section": "",
    "text": "If you’re using git for version control, don’t forget about the .gitignore file. Anything large and static (like .abfs) or procedurally generated (e.g. plots) you can toss in the .gitignore and you’ll not see it when you commit.\nI have tabular data that lives in ./inst/extdata/ is processed by a script and then saved as a .rds in ./data/. Here’s my .gitignore.\n.Rproj.user\n.Rhistory\n.RData\n.Ruserdata\n# Don't track ABFs -- large and static\n*.abf\n# Don't track files that are generated from the scripts\n/data/*\nFor more check out git-scm"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200519_tips_ggplot_shading_with_geom_rect/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200519_tips_ggplot_shading_with_geom_rect/index.html",
    "title": "Shading with geom_rect",
    "section": "",
    "text": "Use geom_rect() with the min/max set to -Inf and Inf to add a pleasant shading to your facets.\nFor example by passing it a data frame with the faceting variables and a column to use for the color (green if positive, red if negative) we can make facets behave like cells in a heatmap!\n# &gt; tp2\n# # A tibble: 23 x 5\n#    Condition   Trace             Time  Change StimId\n#    &lt;fct&gt;       &lt;chr&gt;             &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt; \n#  1 PS.0.orig   170623b_0007.abf  60         1 a     \n#  2 PS.45.orig  170825a_0008.abf  60        -1 c     \n\nggplot(df)+\n  geom_rect(data = tp2, aes(fill = Change),xmin = -Inf,xmax = Inf, ymin = -Inf,ymax = Inf,alpha = 0.3) +\n  geom_hline(yintercept = 0, color = \"cornflowerblue\")+\n  geom_pointline(aes(x = Time, y = rc, group = Experiment), shape = 1, color = \"black\")+\n  ylim(-2, 3.5)+\n  scale_fill_gradientn(colors = c(\"Red\", \"Grey\", \"Green\"))+\n  theme_base()+\n  theme(legend.position = \"\")\n\n\n\nimage (5).png"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200604_visualize_outlier_influence/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200604_visualize_outlier_influence/index.html",
    "title": "Visualize the Influence of Outliers",
    "section": "",
    "text": "If you’re looking at linear relationships, give this code a spin. It’ll automatically flag outliers based on 1.5*IQR and show you the fit with and without the outliers. Since the output is a ggplot, you and add to it or tweak the aesthetics of the output (see example below).\nHere’s the function.\nscatter_w_wo_outliers &lt;- function(temp = filter(M, Time == \"Baseline\"),\n                                  X = \"bkkca\", \n                                  Y = \"Ihtk.0\"){\n  \n  # flag outliers based on 1.5xIQR from median\n  X_low  &lt;- median(temp[[X]], na.rm = T) - (1.5 * IQR(temp[[X]], na.rm = T))\n  X_high &lt;- median(temp[[X]], na.rm = T) + (1.5 * IQR(temp[[X]], na.rm = T))\n  Y_low  &lt;- median(temp[[Y]], na.rm = T) - (1.5 * IQR(temp[[Y]], na.rm = T))\n  Y_high &lt;- median(temp[[Y]], na.rm = T) + (1.5 * IQR(temp[[Y]], na.rm = T))\n  \n  X_pass &lt;- (temp[[X]] &gt; X_low) * (temp[[X]] &lt; X_high)\n  Y_pass &lt;- (temp[[Y]] &gt; Y_low) * (temp[[Y]] &lt; Y_high)\n\n  temp$flag &lt;- ifelse((X_pass * Y_pass) == 1, T, F)\n  \n  \n  # Duplicate so we have dataset 1, 2 (introduces duplicates)\n  temp &lt;- rbind(temp[temp$flag == T, ], mutate(temp, flag = F))\n  \n  formula1 &lt;- y ~ x\n  \n  plt &lt;- ggplot(temp, aes_string(X, Y, color = \"flag\"))+\n    geom_smooth(data = temp, method = lm, se = F, fullrange = T)+\n    geom_point(data = temp)+\n    geom_point(data = temp, color = \"black\", shape = 1)+\n    geom_point(data = temp[temp$flag, ])+\n    ggpmisc::stat_poly_eq(aes(label =  paste(stat(eq.label), \"*\" with \"*\", \n                                             stat(rr.label), \"*\", \"*\", \n                                             stat(f.value.label), \"*\", and \"*\",\n                                             stat(p.value.label), \"*\".\"\",\n                                             sep = \"\")),\n                          formula = formula1, parse = TRUE, size = 4)+\n    \n    scale_color_manual(values = c(\"darkgray\", \"black\"))+\n    theme_bw()+\n    theme(legend.position = \"\")\n  \n  return(plt)\n}\nHere’s a reproducible example. We’re creating a Simpson’s paradox by giving the “outliers” a negative slope and the real data a positive slope. I’ve added a red line showing the true relationship.\nset.seed(45645684)\ndf &lt;- data.frame(x = rnorm(30, mean = 10, sd = 4),\n                 noise = runif(30, min = -2, max = 2),\n                 y = NA,\n                 is_outlier = rbinom(30, 1, prob = 0.2))\n\n\ndf$y &lt;- ifelse(df$is_outlier, \n               -5*df$x+df$noise,\n               2*df$x+df$noise)\n\nscatter_w_wo_outliers(temp = df,\n                      X = \"x\",\n                      Y = \"y\")+\n  geom_abline(slope = 2, intercept = 0, color = \"firebrick\")\n\n\n\nimage (7).png\n\n\n2020-6-4 Daniel Here’s a related utility function. For a given column it’ll return a logical vector where outliers are FALSE.\nw_in_x_iqr &lt;- function(col_in, multiplier = 1.5){\n  col_in &lt;- as.vector(col_in)\n  \n  X_low  &lt;- median(col_in, na.rm = T) - (multiplier * IQR(col_in, na.rm = T))\n  X_high &lt;- median(col_in, na.rm = T) + (multiplier * IQR(col_in, na.rm = T))\n  X_pass &lt;- (col_in &gt; X_low) * (col_in &lt; X_high)\n  \n  return(as.logical(X_pass))\n}\ne.g.\n&gt; mutate(M, ex = w_in_x_iqr(bkkca)) %&gt;% select(bkkca, ex) %&gt;% arrange(bkkca) %&gt;% tail()\n\n#  A tibble: 6 x 2\n#   bkkca ex   \n#   &lt;dbl&gt; &lt;lgl&gt;\n# 1 3056. TRUE \n# 2 3222. TRUE \n# 3 3255. TRUE   # Within bounds\n# 4 3552. FALSE  # Outside bounds \n# 5 3817. FALSE\n# 6 6740. FALSE"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200612_tips_r_use_unlist/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200612_tips_r_use_unlist/index.html",
    "title": "unlist is handy and you should use it",
    "section": "",
    "text": "Y’all should be using unlist(). unlist is a crazy handy function when you pair it with the purrr library. It’ll take a list and try to give you a vector. Strictly speaking it’s probably best to be using map_dbl() or map_chr() instead but let’s not worry about that at the moment.\nWhy do I love unlist so much? Because unlist(map( )) gives you a flexible, effective way to iterate (that is parallel friendly with minor changes).\nHere’s an example: I have a bunch of traces in a folder and I need to know if there are any experiments that didn’t copy. Additionally, I’d like to know what kind of data is in the file. I could look through them one by one, but that’s no fun. Thankfully, R has functions that perform similarly to shell functions. (More on these some other time.)\nSo we start by defining a data.frame with all file names and their sizes. Note that calling unlist(map()) inside data.frame() lets us do a lot work very quickly. We find all the files, get information about each one, and then selectively return the size.\ntraces_dir &lt;- \"C:/Users/Daniel/Documents/Trace_Holding\"\n\nfiles_df &lt;- data.frame(files = list.files(traces_dir),\n                       bytes = unlist(map(list.files(traces_dir), \n                                          function(abf){ &lt;http://file.info|file.info&gt;(paste0(traces_dir, \"/\",abf))$size })))\n\n#              files    bytes\n# 1 190808a_0000.abf 15006208 &lt;- long gap free recording\n# 2 190808a_0001.abf 15006208\n# 3 190808a_0002.abf 15006208\nThe experiment is embedded in the file name for each. Once again , with a little help from unlist we can split the file names into a list of list (i.e. \"190808a_0000.abf\" becomes [[1]] [[1]] \"190808a\" [[2]] \"0000.abf\" select only the first part and populate a new column.\nfiles_df$Experiment &lt;- unlist(transpose(str_split(files_df$files, pattern = \"_\"))[[1]])\nOkay, now we can make use of this. I’ve defined a data.frame for metadata about the experiments.\nfile_groups\n#    Experiment       Group\n# 1      190924    Baseline\n# ... \n# 19     190918     Delayed\nWe can join these data frames and apply a little tidyverse magic to see what experiments are missing (we could also compare the sets of experiments directly).\nfull_join(file_groups, files_df) %&gt;% \n  filter(&lt;http://is.na|is.na&gt;(files)) %&gt;% \n  group_by(Group, Experiment) %&gt;% \n  tally()\n\n#  Experiments that didn't transfer:\n\n#   Group       Experiment     n\n# 1 Baseline    190924         1\n# 2 Baseline    190924a        1\n# ...\nWe can repeat the same strategy to programattically look at the protocol types (e.g. based on file size or channel number via &lt;http://file.info|file.info&gt; | readABF()). Moral of the story, you should so stop applying yourself and give unlist and purrr functions a try."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200728_tips_r_tidyverse_current_df/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200728_tips_r_tidyverse_current_df/index.html",
    "title": "Reference current dataframe with .",
    "section": "",
    "text": "In tidyverse you can use . to reference the current dataframe. This is really useful for plotting. For example, I’d like to plot channels In7 and In12 in that order but by default In12 will come first. We could save an intermediate dataframe and re-level the factors, but by referencing the dataframe piped into mutate we can skip this step.\ntemp %&gt;% # temp is a down-sampled ephys recording\n  ungroup() %&gt;% \n  gather(key, value, c(\"In7\", \"In12\")) %&gt;% \n  mutate(key = factor(.$key, levels = c(\"In7\", \"In12\"))) %&gt;%      # Note that if you have groupings, you'll need to get rid of them or supply a column of the same length as the group. \n  ggplot(aes(Time, value, color = key, group = interaction(key, Sweep)))+\n  geom_path()\n\n\n\nimage (11).png"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/200822_r_datatype_gganimate/index.html",
    "href": "posts/DanielKick/210000_phd_tips/200822_r_datatype_gganimate/index.html",
    "title": "Data Type Matters In gganimate",
    "section": "",
    "text": "Be mindful of your data types. Sometimes T == 1 == 1.0 (logical, int, double) but assuming these are equivalent can get you into trouble. For example, in these animations, the only difference is the data is coerced to logicals or integers."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/201104_r_resampling_demo/index.html",
    "href": "posts/DanielKick/210000_phd_tips/201104_r_resampling_demo/index.html",
    "title": "Resampling doesn’t need to be hard",
    "section": "",
    "text": "As an example to show how accessible resampling can be, here’s bit of code that resamples an anova and computes an empirical p value.\ntemp is a dataframe containing the data Condition is a column with exactly that temp_col is the name of a dependent variable. It’s a string to make this easy to reuse. if you haven’t used map before it’s basically a for loop that returns a list. When the output get’s passed into unlist it becomes an array.\ntemp_shuffle &lt;- temp\nresample_array &lt;- map(1:1000, function(i){\n     temp_shuffle$Condition &lt;- sample(temp_shuffle$Condition, replace = F)\n     fm &lt;- lm(as.formula(paste0(temp_col, \" ~ Condition\")), data = temp_shuffle)      \n     return(car::Anova(fm)[1,3])\n}) %&gt;% unlist()\nep &lt;- mean(resample_array &gt;= car::Anova(fm)[1,3])\nThe down side is that it takes orders of magnitude more time to run because you’re running the same code hundreds or thousands of times. This is only a problem if you need crazy high precision or have a really complex/hard to fit model. For reference using the code above took about ~2 seconds/dv for 1000 iterations on my machine.\nA handy pattern is to use map to summarize data and then bind it.\nmap_res &lt;- map(names(M)[names(M) != \"Sample\"], function(i){\n  res &lt;- shapiro.test(M[[i]])\n\n  return(\n    list(\n    mrna = i,\n    p = res$p.value\n    )\n  )\n})\n\nshapiro_res &lt;- do.call(rbind, map_res)"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/201209_diagrams_as_code/index.html",
    "href": "posts/DanielKick/210000_phd_tips/201209_diagrams_as_code/index.html",
    "title": "Diagrams as code",
    "section": "",
    "text": "Check out DiagrammeR (or mermaid) if you need clean easy flow diagrams. I find they’re not to hard to make and even easier to forget that you’ve made them.\nDiagrammeR::grViz(\"digraph {\ngraph [layout = dot]\n\n# define the global styles of the nodes. We can override these in box if we wish\nnode [shape = circle, style = filled, fillcolor = LightSteelBlue]\n\ndata1 [label = 'Input', shape = folder, fillcolor = Beige]\n\nglmnet [label = 'Lasso \nRegression', shape = box, fillcolor = Linen]\nmnnet [label = 'Mulitnomial \nNeural \nNetwork', shape = box, fillcolor = Linen]\nnnet [label = 'Neural \nNetwork', shape = box, fillcolor = Linen]\nknn [label = 'k-Nearest \nNeighbor', shape = box, fillcolor = Linen]\nranger [label = 'Random \nForest', shape = box, fillcolor = Linen]\nsvml [label = 'SVM \nLinear', shape = box, fillcolor = Linen]\nsvmr [label = 'SVM \nRadial', shape = box, fillcolor = Linen]\n\ndata2 [label = '5-fold CV \nAccuracy', shape = folder, fillcolor = Beige]\n\n# edge definitions with the node IDs\ndata1 -&gt; {glmnet mnnet nnet knn ranger svml svmr} -&gt; data2\n{alpha lambda} -&gt; glmnet\ndecay -&gt; mnnet\n{size decay} -&gt; nnet\nk -&gt; knn\n{mtry splitrule minNodeSize} -&gt; ranger\ncost -&gt; svml\nsigma -&gt; svmr\n}\")\n\n\n\nimage (15).png"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/201222_ggplot_font_custom/index.html",
    "href": "posts/DanielKick/210000_phd_tips/201222_ggplot_font_custom/index.html",
    "title": "ggplot font customization",
    "section": "",
    "text": "You can set ggplot’s font using the theme function. Particularly if combined with functions from ggthemes or ggsci you can get very pleasing visualizations quickly. Beyond accessing fonts already on your system you can import and fonts with minimal hassle.\ne.g. to get the font Metal Mania ready to use one might run:\nlibrary(showtext)\nfont_add_google(name = \"Metal Mania\", family = \"Metal+Mania\")\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(patchwork) # for adding together plots at the end\nlibrary(extrafont)\n# font_import()) # &lt;--- run this once\nloadfonts(device = \"win\", quiet = TRUE) # &lt;--- run this once per session\n# extrafont::fonts() # see fonts that are available\n\nplt1 &lt;- palmerpenguins::penguins %&gt;%\n  filter(!&lt;http://is.na|is.na&gt;(sex)) %&gt;%\n  mutate(sex = case_when(sex == \"male\" ~ \"m\",\n                         sex == \"female\" ~ \"f\")) %&gt;% \n  ggplot(aes(sex, body_mass_g, fill = species, group = interaction(species, sex)))+\n  geom_boxplot()+\n  ggthemes::scale_fill_colorblind()+\n  ggthemes::theme_clean()+\n  theme(legend.position = \"\")+\n  facet_grid(.~species)+\n  labs(title = \"Default\")\n\nplt2 &lt;- plt1+\n  theme(text = element_text(family = \"Consolas\"))+\n  labs(title = \"Consolas\")\n\nplt3 &lt;- plt1+\n  theme(text = element_text(family = \"Garamond\"))+\n  labs(title = \"Garamond\")\n\nplt1 + plt2 + plt3\n\n\n\nimage (17).png"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/210203_thought_exp_replicated_res/index.html",
    "href": "posts/DanielKick/210000_phd_tips/210203_thought_exp_replicated_res/index.html",
    "title": "Thought Experiment: Comparing Replicate Experiments’ Conclusions",
    "section": "",
    "text": "A collaborator and I just talked through how one might compare two sets of p values. We’re thinking about the following approach.\nSetup: You have two replicates of an experiment (r1, r2). In each experiment you measured three mRNAs (a, b, c) in control and treatment (c, t). You want to know if the same trends in r1 show up in r2 but there is a batch effect that will prevent comparing them directly (e.g. you can’t run a t test on r1 _a_ c and r2 _a_ c )\nProposed Solution:\n\nCompare each mRNA within a replicate and note the sign of change and if the p value reached a pre-determined cutoff (we lose some information doing this since we’re pooling 0.06 and 0.99 together)\n\nrep  |mrna |sign |pval |sig  |\nr1   | a   | -   |0.04 |1    |\nr1   | b   | +   |0.10 |0    |\n...  |     |     |     |     |\nr2   | c   | -   |0.02 |1    |\n\nMultiply the sign by the significance code so that -1 = “significant decrease”, 0 = “no significant change”, +1 = “significant increase”\n\nrep  |mrna |sign |pval |sig  |sxp  |\nr1   | a   | -   |0.04 |1    |+1   |\nr1   | b   | +   |0.10 |0    |0    |\n...  |     |     |     |     |     |\nr2   | c   | -   |0.02 |0    |-1   |\n\nReshape these as two vectors and treat them as categorical data. Then compare the “assignment” between these two lists using a jaccard index as if we were comparing an assignment from clustering against reality.\n\nr1 &lt;- as.character(c(1, 0, 0))\nr2 &lt;- as.character(c(0, 0, -1))\njaccard(r1, r2)\n\nUse resampling to find an empirical p value for this observed jaccard index.\n\nDoes that seem reasonable? Is there another way you would go about it? (Email me what you think!)\nHere’s an implementation for two sets of correlations. Here we bin the correlations into 5 bins use a jaccard index to assess whether the bin assignments are the same for both datasets (Brian’s and mine). To confirm that the measured jaccard index (0.23) isn’t anything to write home about we can generate an empirical p value (ep = 0.13).\n# cor_comp_df\n#\n#    Source Time     x     y        Corr\n#    &lt;fct&gt;  &lt;fct&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n#  1 Person1  Baseline bkkca cav1    0.441\n#  2 Person1  Baseline bkkca cav2    0.476\n#  3 Person1  Baseline bkkca inx1    0.435\n#  4 Person1  Baseline bkkca inx2    0.159\n#  5 Person1  Baseline bkkca inx3   -0.174\n#  6 Person1  Baseline bkkca inx4   -0.167\n\nn_bins &lt;- 5\n\ntemp &lt;- cor_comp_df %&gt;% \n  mutate(Bins = cut(Corr, breaks = seq(-1, 1, length.out = n_bins))) %&gt;% \n  select(-Corr) %&gt;% \n  pivot_wider(names_from = \"Source\", values_from = \"Bins\")\n\n# temp\n#\n#   Time     x     y     Person1    Person2  \n#   &lt;fct&gt;    &lt;chr&gt; &lt;chr&gt; &lt;fct&gt;    &lt;fct&gt;   \n# 1 Baseline bkkca cav1  (0,0.5]  (0,0.5] \n# 2 Baseline bkkca cav2  (0,0.5]  (0,0.5] \n# 3 Baseline bkkca inx1  (0,0.5]  (0.5,1] \n# 4 Baseline bkkca inx2  (0,0.5]  (0.5,1] \n# 5 Baseline bkkca inx3  (-0.5,0] (0.5,1] \n# 6 Baseline bkkca inx4  (-0.5,0] (-0.5,0]\n\n\nlibrary('clusteval')\nobs_jaccard &lt;- cluster_similarity(temp$Person1, temp$Person2, similarity=\"jaccard\")  \n\n# 0.2304234\n\nnull_jaccard &lt;- map(1:10000, function(i){\n  cluster_similarity(sample(temp$Person1, replace = F), \n                     temp$Person2, similarity=\"jaccard\")\n  }) %&gt;% \n  unlist()\n\n\ntemp &lt;- with(density(null_jaccard), data.frame(x, y))\ntemp &lt;- temp %&gt;% mutate(xmax = max(x),\n                obs = obs_jaccard)\n\nggplot(data = temp, aes(x = x, y = y))+\n  geom_line()+\n  geom_vline(xintercept = obs_jaccard)+\n  geom_ribbon(data = temp[temp$x &gt; temp$obs, ], \n              aes(xmin = obs, xmax = xmax, ymin = 0, ymax = y))+\n  labs(subtitle = paste(\"empirical p=\", as.character(mean(null_jaccard &gt;= obs_jaccard))))\n\n\n\nimage (22).png\n\n\nIt’s worth generating an empirical p value for each comparison you’re making. For example here I’m comparing the results of an experiment replicate. Each dependent variable is assigned a group based on if one would conclude it there was a difference (0 or 1) between groups and the sign of that difference (+ or -). Seeing a Jaccard index of 0.61 (out of 1) we might conclude we replicated most of the findings. However, the empirical p value is 1 because most of the comparisons were non-significant in both groups resulting in a high floor for the index."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/210305_tips_ggplot_markdown/index.html",
    "href": "posts/DanielKick/210000_phd_tips/210305_tips_ggplot_markdown/index.html",
    "title": "ggplot tips: markdown for italics in plots and adding breaks to color scaling",
    "section": "",
    "text": "Two tricks today: 1. Scale fill functions can accept breaks and limit arguments so you don’t have to use hacky workarounds like binning the data before plotting (which is what I usually do). 2. library(ggtext) lets you render markdown within plots (e.g. for those pesky mRNAs)\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nset.seed(42)\ndf &lt;- do.call(cbind, map(\n  1:5, \n  function(e){\n    data.frame(gene = rnorm(10))\n    })\n  )\n\ndf &lt;- df |&gt;\n  corrr::correlate() |&gt;\n  corrr::shave() |&gt; \n  pivot_longer(cols = starts_with('gene')) |&gt; \n  rename(term2 = name, Cor = value) |&gt; \n  drop_na()\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\nhead(df)\n\n# A tibble: 6 × 3\n  term   term2      Cor\n  &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;\n1 gene.1 gene   -0.375 \n2 gene.2 gene    0.440 \n3 gene.2 gene.1 -0.153 \n4 gene.3 gene   -0.404 \n5 gene.3 gene.1  0.465 \n6 gene.3 gene.2 -0.0600\n\n\nBefore:\n\ndf |&gt; \n  ggplot(aes(term, term2, fill = Cor))+ \n  geom_tile()+ labs(x = \"mRNA\", y = \"\")+ \n  scale_fill_distiller(palette = \"PuOr\")+ \n  coord_fixed()\n\n\n\n\n\n\n\n\nAfter:\n\nlibrary(ggtext) # https://github.com/wilkelab/ggtext\nlibrary(glue)\n\ndf |&gt; \n  mutate(term = glue((\"&lt;i&gt;{term}&lt;/i&gt;\"))) |&gt;\n  ggplot(aes(term, term2, fill = Cor))+ \n  geom_tile()+ labs(x = \"mRNA\", y = \"\")+ \n  theme(axis.text.x = element_markdown(angle = 45))+ # &lt;-- Note that we have element_markdown not element_text\nscale_fill_stepsn(\n  colors=RColorBrewer::brewer.pal(n = 8, name = \"PuOr\"), \n  na.value = \"transparent\", breaks=round(seq(-1, 1, length.out =8), digits = 2), \n  limits=c(-1,1) )"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/210311_ggplot_discrete_axis_ordering/index.html",
    "href": "posts/DanielKick/210000_phd_tips/210311_ggplot_discrete_axis_ordering/index.html",
    "title": "ggplot Reordering a Discrete Axis",
    "section": "",
    "text": "Reordering a discrete axis in ggplot after generation a lot simpler than one might expect. Rather than converting a character column to a factor (what if the data gets pivoted?), or using one column for the position and one for the labels, you can use xlim or ylim.\n&gt; mrna_cols \\# desired order \\# \\[1\\] \"nav\" \"cav1\" \"cav2\" \"bkkca\"\n&gt; \"shaker\" \"shal\" \"shab\" \"shaw1\" \\# \\[9\\] \"shaw2\" \"inx1\" \"inx2\" \"inx3\"\n\no_mrna$heatmap_z / # ggplot object within a list\no_mrna$heatmap_z+ylim(mrna_cols)\n\n\n\nimage (31).png"
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/210907_tips_r_approximate_string_matching/index.html",
    "href": "posts/DanielKick/210000_phd_tips/210907_tips_r_approximate_string_matching/index.html",
    "title": "Approximate String Matching",
    "section": "",
    "text": "fuzzywuzzy is a tool that isn’t necessary most of the time but when it is it can save a ton of time. It lets you do approximate string matching. I’ve used it for handling typos and differences in white space/punctuation/naming conventions in entry labels and it’s worked nicely. There’s a port for R and a few other languages too."
  },
  {
    "objectID": "posts/DanielKick/210000_phd_tips/230523_r_a_comment_is_a_comment/index.html",
    "href": "posts/DanielKick/210000_phd_tips/230523_r_a_comment_is_a_comment/index.html",
    "title": "A Comment is a Comment.",
    "section": "",
    "text": "R treats “#” as a comment in text files.\nIf you have a file with this in a a header field (e.g. for biologists “# chromosomes”) R will fail to load the file. The solution is to pass in an explicit comment character like so read.table(“table_file.txt”, comment.char=‘’)."
  },
  {
    "objectID": "posts/DanielKick/210609_python_jupyter_plugins/index.html",
    "href": "posts/DanielKick/210609_python_jupyter_plugins/index.html",
    "title": "Tips: Jupyter Plugins",
    "section": "",
    "text": "I came across a handy set of tools for jupyter. There are of extensions for the notebooks that give you access to code snips, autocomplete by default, rendering a notebook as a slide show, and other features. To get it installed within an anaconda virtual environment you may only need to install it with this command:\nconda install -c conda-forge jupyter_contrib_nbextensions\nI not all of the extensions were showing up for me until I also ran these two lines, so it may take a bit of fiddling to get it to run.\njupyter contrib nbextension install --user\njupyter nbextension enable codefolding/main\nHere’s a linkto a page that shows some of these extensions in action."
  },
  {
    "objectID": "posts/DanielKick/210621_python_data_readability/index.html",
    "href": "posts/DanielKick/210621_python_data_readability/index.html",
    "title": "Tips: More Readable Data with pretty-print",
    "section": "",
    "text": "Here’s a tool that some may find useful when working with data that’s not yet in a [DataFrame]. It lets one “pretty-print” an object making any text that would wrap easier to read.\n# [In]\nprint(results_dictionary)\nprint(\"\\n --------------------------------------------- \\n\")\nimport pprint\npprint.PrettyPrinter(indent=4).pprint(results_dictionary)\n# [Out]\n{'active': True, 'additionalInfo:programDbId': '343', 'additionalInfo:programName': 'UC Davis', 'commonCropName': 'Cassava', 'contacts': None, 'culturalPractices': None, 'dataLinks': [], \n# ...\n'trialDbId': '343', 'trialName': 'UC Davis'}\n--------------------------------------------- \n{   'active': True,\n    'additionalInfo:programDbId': '343',    'additionalInfo:programName': 'UC Davis',    'commonCropName': 'Cassava',    'contacts': None,    'culturalPractices': None,    'dataLinks': [],    # ...    'trialDbId': '343',    'trialName': 'UC Davis'}"
  },
  {
    "objectID": "posts/DanielKick/220216_python_silent_replace/index.html",
    "href": "posts/DanielKick/220216_python_silent_replace/index.html",
    "title": "Tips: For those coming from R: Silent In Place Replacement",
    "section": "",
    "text": "Silent, in place assignment updating an object This tripped me up even though it’s consistent with how I’ve seen other objects behave. I needed an attribute to hold data extracted from a collection of files in a directory and created a class for this.\nclass hps_search_experiment:\n    def __init__(self, path=\"\", trial_type=''):\n        self.path = path\n        self.trial_type = trial_type\n        self.hps_best_trial = None\n        \n    def process_hps_files(self):\n        # ...\n        \n        self.hps_best_trial = hps_best_trial\nHowever, running like so fails.\ntest = hps_search_experiment(\n    path = './hps_search_intermediates_G/', \n    trial_type = 'rnr')\n    \ntest = test.process_hps_files()\ntest.hps_best_trial\n\n#&gt; AttributeError: 'NoneType' object has no attribute 'hps_best_trial'\nThis had me baffeld because I was thinking with R’s norms of data &lt;- data %&gt;% funciton() where in place replacement is the exception. Instead I needed to be thinking with python’s base object norms (e.g. a_list.extend(['b', 'c']) ). This fails because I overwrote test with the output of the method, which returns notthing since it’s overwriting attributes within test’s scope.\nThese would also work to update the attribute:\nself.hps_best_trial = hps_best_trial\n\nhps_search_experiment.__setattr__(self, \"hps_best_trial\", hps_best_trial)\n\n# if it's initialized as a list\nself.hps_best_trial.append([hps_best_trial]) \n# if a dict is initialized for data\nself.data = {'a':1}\nself.data.update({'hps_best_trial':hps_best_trial})"
  },
  {
    "objectID": "posts/DanielKick/220523_r_comments_in_tables/index.html",
    "href": "posts/DanielKick/220523_r_comments_in_tables/index.html",
    "title": "Trivia: R can have Comments in Tables",
    "section": "",
    "text": "R allows for comments to exist in tables. If there’s a # in the table you’re reading (e.g. as a part of a column name like chromosome#) then it can cause an unequal number of values between rows (everything on that line following it is ignored). The solution is to specify the comment character explicitly to be used (it can be ’’ to have no comment characters). Here’s an example:\necho \"a, b, c#, d\" &gt; test_table.txt\n&gt; Rscript -e \"read.table('test_table.txt')\"\n#   V1 V2 V3\n# 1 a, b,  c\n&gt; Rscript -e \"read.table('test_table.txt', comment.char = '')\" # with no comment character, all entries will be read\n#   V1 V2  V3 V4\n# 1 a, b, c#,  d"
  },
  {
    "objectID": "posts/DanielKick/221027_hpc_access_running_session/index.html",
    "href": "posts/DanielKick/221027_hpc_access_running_session/index.html",
    "title": "Tips: Open a new Interactive Session in Running Session",
    "section": "",
    "text": "A handy trick with batched processes on an HPC is that you can start an interactive session in a running session. Here’s an example where I needed to check if I was nearing the maximum allowed memory:\nHere I list my active jobs to get the jobid, run bash on that node, and list the processes by memory usage.\n[daniel.kick@Atlas-login-1 BLUP_G]$ squeue -u daniel.kick\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           2491772     atlas   BLUP-W daniel.k  R      39:56      1 Atlas-0025\n[daniel.kick@Atlas-login-1 BLUP_G]$ srun --pty --jobid 2491772 bash\n[daniel.kick@Atlas-0025 BLUP_G]$ htop"
  },
  {
    "objectID": "posts/DanielKick/230607_simulating_ensembles/index.html",
    "href": "posts/DanielKick/230607_simulating_ensembles/index.html",
    "title": "Simulation as a Super Power",
    "section": "",
    "text": "Writing simulations is one of the best ways I’m aware of to build one’s statistical intuition and comfort with data visualization. In addition to being able to try out new statistical tests and know exactly what effects they should find, they’re also great for communicating ideas and persuading others.\nA few months ago I had occasion to do just that.\n\\[...\\]\nAt the time I was advocating in a manuscript that when one needs to make a prediction combining predictions from different models is the way to go. Specifically, my results suggest that using a weighted average to make accurate models more influential. To do this, the predictions from each model are multiplied by the inverse of the model’s root mean squared error (rmse) of the model and summed. Someone helping me improve this manuscript thought that instead I should be weighting by the inverse of the model’s variance. This is a reasonable expectation (variance weighting is beautifully explained here) so I needed to convince my collaborator before the manuscript was ready for the world – Here’s how I did this with in a simulation that was only about 100 lines1 of R code."
  },
  {
    "objectID": "posts/DanielKick/230607_simulating_ensembles/index.html#simulating-observations",
    "href": "posts/DanielKick/230607_simulating_ensembles/index.html#simulating-observations",
    "title": "Simulation as a Super Power",
    "section": "Simulating Observations",
    "text": "Simulating Observations\nLet’s imagine we’re studying dead simple system where the output is equal to the input (\\(y = 0 + 1*x\\)). We can simulate as many samples as we would like from this system over a range of xs.\n\nn = 100 # samples to be generated\nxs &lt;- seq(0, 20, length.out = n) # x values evenly spaced from 0-20 \nys &lt;- 0+1*xs\n\nHere’s the simulated “data”.\n\n\n\n\n\n\n\n\n\nNow we can simulate models that are trying to predict y. To do this we’ll think of a model as being equivalent to the true value of y plus some model specific error. If we assume that the models aren’t prone to systematically over or underestimating, then we can use a normal distribution to generate these errors like so:\n\nmean1 =  0 # error doesn't tend to be over or under\nvar1  =  1 # variance of the error\ny1 &lt;- ys + rnorm(n = n, mean = mean1, sd = sqrt(var1))\n\nWe can simulate a better model by decreasing the variance (errors are consistently closer to the mean of 0). Conversely we can simulate a worse model by making the model tend to over or undershoot by changing the mean or make larger errors more common by increasing the varience. Here’s a model that’s worse than the previous one.\n\nmean2 =  1 # predictions tend to overshoot\nvar2  = 10 # larger errors are more common\ny2 &lt;- ys + rnorm(n = n, mean = mean2, sd = sqrt(var2))\n\nLet’s look at the predictions from model y1 and model y.\n\n\n\n\n\n\n\n\n\nHere we can see that y1’s error (vertical lines) are considerably smaller than that of y2.\nWe can subtract the true value \\(y\\) from the predicted value \\(\\hat y\\) to see this more clearly.\n\n\n\n\n\n\n\n\n\nIn panel B we can see the difference between the two error distributions for the models (save a few irregularities in these distributions from only using 100 samples.\nNow we can try out different averaging schemes to cancel out some of the error and get a better prediction. We can test a simple average like so.\n\ne1 &lt;- 0.5*y1 + 0.5*y2\n\nWe can also try placing more weight on models with less variable predictions (and hopefully smaller errors).\n\nyhat_vars  &lt;- unlist(map(list(y1, y2), function(e){var(e)})) # Calculate variance for each model's predictions\nwght_vars  &lt;- (1/yhat_vars)/sum(1/yhat_vars) # Take the inverse and get percent weight by dividing by the sum \ne2 &lt;- wght_vars[1]*y1  + wght_vars[2]*y2 # Scale each model's prediction and add to get the weighted average.\n\nWe can also try placing more weigh on models that are more accurate2.\n\nyhat_rmses &lt;- unlist(map(list(y1, y2), function(e){sqrt((sum((ys-e)**2)/n))}))\nwght_rmses &lt;- (1/yhat_rmses)/sum(1/yhat_rmses)\ne3 &lt;- wght_rmses[1]*y1 + wght_rmses[2]*y2\n\nNow we can calculate the RMSE for both models and these weighed averages.\n\n\n\n\n\nname\ny_rmse\nmean1\nmean2\nvar1\nvar2\n\n\n\n\ny1\n1.023765\n0\n1\n1\n10\n\n\ny2\n3.218318\n0\n1\n1\n10\n\n\nunif\n1.796390\n0\n1\n1\n10\n\n\nvar\n1.670037\n0\n1\n1\n10\n\n\nrmse\n1.217204\n0\n1\n1\n10\n\n\n\n\n\n\n\ny1 is the best set of predictions, averaging did not benefit predicitons here. This is not too much of a shock since y2 was generated by a model that was prone to systematically overshooting the true value and was more likely to have bigger errors.\nBut how would these results change if the models were more similar? What if the models had more similar error variences? Or if one was prone to overshooting while the other was prone to undershooting?"
  },
  {
    "objectID": "posts/DanielKick/230607_simulating_ensembles/index.html#expanding-the-simulation",
    "href": "posts/DanielKick/230607_simulating_ensembles/index.html#expanding-the-simulation",
    "title": "Simulation as a Super Power",
    "section": "Expanding the Simulation",
    "text": "Expanding the Simulation\nTo answer this we can package all the code above into a function with variables for each models’ error distribution and how many observations to simulate and return the RMSE for each model. This function (run_sim) is a little lengthy so I’ve collapsed it here:\n\n\nCode\nrun_sim &lt;- function(\n    n = 10000,\n    mean1 = 0,\n    mean2 = 1, \n    var1 = 1,\n    var2 = 10\n  ){\n  xs &lt;- seq(0, 20, length.out = n)\n  ys &lt;- 0+1*xs\n\n  # Simulate models\n  y1 &lt;- ys + rnorm(n = n, mean = mean1, sd = sqrt(var1))\n  y2 &lt;- ys + rnorm(n = n, mean = mean2, sd = sqrt(var2))\n  \n  # Equal weights\n  e1 &lt;- 0.5*y1 + 0.5*y2\n  \n  # Variance weights\n  yhat_vars  &lt;- unlist(map(list(y1, y2), function(e){var(e)}))\n  wght_vars  &lt;- (1/yhat_vars)/sum(1/yhat_vars)\n  e2 &lt;- wght_vars[1]*y1 + wght_vars[2]*y2\n  \n  # RMSE weights\n  yhat_rmses &lt;- unlist(map(list(y1, y2), function(e){sqrt((sum((ys-e)**2)/n))}))\n  wght_rmses &lt;- (1/yhat_rmses)/sum(1/yhat_rmses)\n  e3 &lt;- wght_rmses[1]*y1 + wght_rmses[2]*y2\n  \n  # Aggregate predictions and accuracy\n  data &lt;- data.frame(xs, ys, y1, y2, unif = e1, var = e2, rmse = e3)\n  plt_data &lt;- data %&gt;% \n    select(-xs) %&gt;% \n    pivot_longer(cols = c(y1, y2, unif, var, rmse)) %&gt;% \n    rename(y_pred = value) %&gt;% \n    # Calc RMSE\n    group_by(name) %&gt;%                \n    mutate(y_se = (ys - y_pred)**2) %&gt;% \n    summarise(y_rmse = sqrt(mean(y_se))) %&gt;% \n    ungroup() %&gt;% \n    mutate(mean1 = mean1,\n           mean2 = mean2,\n           var1 = var1,\n           var2 = var2)\n  \n  return(plt_data)\n}\n\n\nNext we’ll define the variables to examine in our computational experiment.\nWe can think about combining models that differ in accuracy (error mean) and precision (error variation). These differences can be are easier to think about visually. Here are the four “flavors” of model that we would like to combine to test all combinations of accuracy and precision.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nSpecifically, We’ll have one model that acting as a stable reference and vary the error of the other (y2). We’ll make a version of the y2 model that is accurate (the mean error is only shifted by 0.01) and one that is inaccurate (the mean error is shifted by 50). Then we’ll see what happens when these two go from being precise (variance around 0.01) to very imprecise (variance up to 100).\nIn R this is expressed as below. When the mean of the error (mean_shift) is near zero accuracy is high. When the variance of the error (var_shift) is near zero precision is high.\n\nparams &lt;- expand.grid(\n  mean_shift = seq(0.01, 50, length.out = 2),\n  var_shift = c(seq(0.01, 0.99, length.out = 50), seq(1, 100, length.out = 90))\n)\n\nThis results in quite a few (280) combinations. of parameters Let’s look at the first and last few:\n\n\n\n\n\n\nmean_shift\nvar_shift\n\n\n\n\n1\n0.01\n0.01000\n\n\n2\n50.00\n0.01000\n\n\n3\n0.01\n0.03000\n\n\n278\n50.00\n98.88764\n\n\n279\n0.01\n100.00000\n\n\n280\n50.00\n100.00000\n\n\n\n\n\n\n\nNow we’ll generate the results. This code may look confusing at first. Here’s what it’s doing. 1. run_sim executes the steps we did above, using the parameters we specified in params to generate 100,000 observations and calculate the expected RMSE of each approach. 1. map is a way of looping over some input and putting all of the results into a list. In this case we’re looping over all the the rows in params so we will end up with a list containing a data.frame for each of the 280 parameter combinations. 1. rbind will combine two data.frames, ‘stacking’ one on top of the other. However, it can’t use a list as input so… 1. we have to use do.call. It will iteratively apply a function (rbind) to the entries in a list so that all the simulation results end up in one big data.frame.\n\nsim_data &lt;- do.call(         # 4.\n  rbind,                     # 3\n  map(seq(1, nrow(params)),  # 2.\n      function(i){           \n        run_sim(             # 1.\n          n = 10000,\n          mean1 = 0,\n          mean2 = unlist(params[i, 'mean_shift']), \n          var1 = 1,\n          var2 = unlist(params[i, 'var_shift']))\n      })\n)\n\nOnce this runs we can look at the results. Let’s consider the high accuracy y2 first, starting where y2’s variance is less than or equal to y1’s variance (1).\n\n\n\n\n\n\n\n\n\nWhen y2’s variance is very small (&lt; ~0.2) it outperforms all other estimates (just like the previous simulation). As it increases it crosses the line for \\(rmse^{-1}\\) weighting (rmse, orange line) and then the other averaging schemes before converging with y1 (dashed blue line). Over the same span \\(rmse^{-1}\\) converges with \\(var^{-1}\\) (var, red line), and the simple average (unif, black line).\n\n\n\n\n\n\n\n\n\nAs y2 continues to worsen, every prediction (except those from y1) get worse and worse. What’s interesting is that this doesn’t happen at the same rate. Because \\(rmse^{-1}\\) weighting penalizes predictions from models based on accuracy its error grows much more slowly than \\(var^{-1}\\) weighting or uniform weighting.\nTo summarize – If two models are equally good (1 on the y axis) then using any of the averaging strategies here will be better than not averaging. If one is far and away better than the other then it’s best to ignore the worse one. In practice one might find they have models that are performing in the same ballpark of accuracy. These results would suggest that in that case one gets the best results by \\(rmse^{-1}\\) weighting.\nNow let’s add in the case where one model is highly inaccurate. In this case, as precision worsens y2 (top blue line) has higher error but this is hard to see given just how much error it has to begin with. Uniform weighting follows a similar trend (but lessened by half) while \\(var^{-1}\\) improves as y2 becomes more imprecise because this decreases it’s influence on the final prediction. Of the averages \\(rmse^{-1}\\) is the best by a country mile because it accounts for the inaccuracy of y2 right from the start."
  },
  {
    "objectID": "posts/DanielKick/230607_simulating_ensembles/index.html#what-if-models-err-in-different-directions",
    "href": "posts/DanielKick/230607_simulating_ensembles/index.html#what-if-models-err-in-different-directions",
    "title": "Simulation as a Super Power",
    "section": "What if models err in different directions?",
    "text": "What if models err in different directions?\nJust for fun, let’s add one more simulation. Let’s suppose we have two models that are equally precise but err in opposite directions. We can modify the code above like so to have some combinations that are equally accurate (just in oppostie directions) and with differing accuracies.\n\nshift_array = seq(0.01, 10, length.out = 40)\nparams &lt;- expand.grid(\n  mean_shift  =    shift_array,\n  mean_shift2 = -1*shift_array\n)\n\nLet’s consider the case where errors are equal and opposite. In the previous simulation, when model variances were equal (1) the performance of all the averages converged, so we might expect that to be the case here. We can see the models getting worse and worse, but can’t see what’s happening with the averages.\n\n\n\n\n\n\n\n\n\nIf we zoom in, it looks like our intuition is correct (ignoring some sampling noise).\n\n\n\n\n\n\n\n\n\nBut we also simulated combinations where one model was off by more than the other. Let’s plot all the combinations of mean1 and mean2 but instead of showing the error of each method like we’ve done above, let’s instead just show where each method produces the best results.\n\n\n\n\n\n\n\n\n\nConsistent with what we’ve seen, for most of these combinations \\(rmse^{-1}\\) performs best. We can get a little fancier by color coding each fo these cells by the best expected error (y_rmse) and color coding the border with the method that produced the best expected error (excepting \\(rmse^{-1}\\) since that accounts for so much of the space).\n\n\n\n\n\n\n\n\n\nIt looks like there’s a sort of saddle shape off the diagonal. We’ll re-plot these data in 3d so we can usethe z axis for y_RMSE and color code each point as above.\n\n\n\n\n\n\nThere we go. Just a little bit of scripting and plotting will let one answer a whole lot of questions about statistics."
  },
  {
    "objectID": "posts/DanielKick/230607_simulating_ensembles/index.html#footnotes",
    "href": "posts/DanielKick/230607_simulating_ensembles/index.html#footnotes",
    "title": "Simulation as a Super Power",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI added a fair bit more for the sake of this post↩︎\nFor simplicity we’re not using testing and training sets. In this simulation that shouldn’t be an issue, but one might consider cases where this could matter. For instance if one model was wildly over fit then its RMSE would not be predictive of its RMSE on newly collected data↩︎"
  },
  {
    "objectID": "posts/DanielKick/230913_filtering_to_save_88pr_space/index.html",
    "href": "posts/DanielKick/230913_filtering_to_save_88pr_space/index.html",
    "title": "Save only what you need",
    "section": "",
    "text": "[1] 7424.908\n\n\n[1] 735.05"
  },
  {
    "objectID": "posts/DanielKick/230913_filtering_to_save_88pr_space/index.html#how-we-got-to-this-point",
    "href": "posts/DanielKick/230913_filtering_to_save_88pr_space/index.html#how-we-got-to-this-point",
    "title": "Save only what you need",
    "section": "How we got to this point:",
    "text": "How we got to this point:\nCollecting data from many sites is expensive but using a biophysical model, many sites and years can be ‘harvested’ in minutes. I’m building a dataset with many cultivars planted across the united states. The problem is that I’m being greedy – I want to have the a day by day account of plant’s growth at ~1,300 locations from 1984-2022, varying cultivar, and planting date.\nIn my initial implementation the results from the model are written to a csv for each location…\n\n\n\nOh no.\n\n\nThis file has a boat load of data. It’s a table of 25,243,175 rows by 19 columns – 479,620,325 billion cells. By the end of the experiment much of my hard drive will be taken up by these."
  },
  {
    "objectID": "posts/DanielKick/230913_filtering_to_save_88pr_space/index.html#reclaiming-88-storage-space",
    "href": "posts/DanielKick/230913_filtering_to_save_88pr_space/index.html#reclaiming-88-storage-space",
    "title": "Save only what you need",
    "section": "Reclaiming 88% Storage Space",
    "text": "Reclaiming 88% Storage Space\nAn easy place to cut cells is from redundant or unneeded columns. These are produced by the simulation but the way I have the experiment structured, they aren’t needed after it’s done running.\n# YAGNI isn't just for code\ndf &lt;- df[, c(\n      # Indepenent Variables\n      'ith_lon', 'ith_lat', 'soils_i', 'SowDate', 'Genotype', 'Date',\n      \n      # Dependent Variables\n      'Maize.AboveGround.Wt', 'Maize.LAI', 'yield_Kgha'\n\n      # Redundant or not needed\n      #'X', 'ith_year_start', 'ith_year_end', 'factorial_file', 'CheckpointID', \n      #'SimulationID', 'Experiment', 'FolderName', 'Zone', 'Clock.Today'\n      )]\nThis is an easy way to get rid of over half the cells (down to 47.36%) (and really I should not have saved these in the first place) but we can do better still.\nMany of the rows represent times before planting without any data collected. All rows where Maize.AboveGround.Wt, Maize.LAI, and Maize.AboveGround.Wt are 0 can be dropped. Because so much of the year is out of the growing season this is quite helpful and cuts about half of the observations (20.09%).\nSplitting these data into two tables with independent variables or dependent variables (with a key) gets the total down to 10,602 + 53,530,975 = 53,541,577. Still a lot but only 11.16% of the starting size!\n\n\n\n\n\nData\nSize\nPercent Original\n\n\n\n\nOriginal\n479620325\n100.00\n\n\nSelect Cols.\n227188575\n47.37\n\n\nFilter Rows\n96355755\n20.09\n\n\nSplit Tables\n53541577\n11.16\n\n\n\n\n\n\n\nI could probably go even further, but now that each experiment takes up only 482 MB instead of 4.64 GB. Furhter optimization can wait for another day.\nWhile storage space is important (at this scale), another factor for the performance (and quality of life) is reading in the data. Using the basic read.csv function it takes 4 minutes 23 seconds to read in. Using the vroom library instead can read in these data in only 4.04 seconds."
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html",
    "href": "posts/DanielKick/230915_vnn_overview/index.html",
    "title": "Making a “Visible” Neural Network",
    "section": "",
    "text": "In most neural networks, neurons are not parametric in the same way that linear models are. In a image recognition model there may be neuron which functions to detects edges but when the model is set up initially one can’t point to a neuron and say what it will do or represent. This can make interpreting the weights in a model tricky.\nVisible neural networks (VNN) are one way to get around this problem by making the structure of a model reflect the process being modeled. In a VNN, influential sub-components may be interpreted as implicating the process they represent as being important. Within biology VNNs have been used by Ma et al. 2018 and Hilten et al. 2021 working in yeast and humans respectively (in the later mixed performance, seemingly based on trait complexity)."
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#whats-a-visual-neural-network",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#whats-a-visual-neural-network",
    "title": "Making a “Visible” Neural Network",
    "section": "",
    "text": "In most neural networks, neurons are not parametric in the same way that linear models are. In a image recognition model there may be neuron which functions to detects edges but when the model is set up initially one can’t point to a neuron and say what it will do or represent. This can make interpreting the weights in a model tricky.\nVisible neural networks (VNN) are one way to get around this problem by making the structure of a model reflect the process being modeled. In a VNN, influential sub-components may be interpreted as implicating the process they represent as being important. Within biology VNNs have been used by Ma et al. 2018 and Hilten et al. 2021 working in yeast and humans respectively (in the later mixed performance, seemingly based on trait complexity)."
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#a-hypothetical-gene-network",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#a-hypothetical-gene-network",
    "title": "Making a “Visible” Neural Network",
    "section": "A hypothetical gene network",
    "text": "A hypothetical gene network\nBefore scaling to representing gene networks, I built a simple test case and will walk through it below, with all the necessary code (but some of it hidden1 for brevity).\n\nHere we have a hypothetical network which involves two genes (a1_input, a2_input), variants of which affect some initial processes (b1, b2), which in turn affect a second set of processes (c1, c2). I’ll use these last processes to predict my trait of interest (y_hat).\nThis is a directed acyclic graph, meaning that processes have an order (the arrows) and there are no loops (c1 doesn’t some how change a1_input). The model I’d like to end up with is a neural network with a structure that mirrors this graph 2 with each node representing one or more layers of neurons.\nBeginning with the end in mind, I need a way to specify: 1. The data the graph operates on 1. The process graph and each node’s attributes 1. How to “move” through the graph"
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#the-data-itself",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#the-data-itself",
    "title": "Making a “Visible” Neural Network",
    "section": "1. The data itself",
    "text": "1. The data itself\nMy example trait, \\(y\\) is either 0 or 1 (plus a little noise). It’s controlled by two genes which are represented as tensor3 containing values for each possible nucleotide (ACGT) for each SNP measured in the gene. Conveniently, both genes either contain all 0’s or all 1’s and when there are only 0’s \\(y\\) will be around 0 (and the same for 1).\nThis of course means that in this population no nucleotides (all 0s) were observed or all nucleotides (all 1s) were simultaneously observed. Don’t ask me how this is possible 🤔. For real data these values would be probability of seeing a given nucelotide so “A” might be [1, 0, 0, 0]4.\n\nn_obs = 100 # 100 obs for each group\ny_true = torch.from_numpy(np.concatenate([\n        np.zeros((n_obs, )),\n        np.ones( (n_obs, ))], 0)) + .1* torch.rand(2*n_obs,)\n        \ninput_tensor_dict = {\n    'a1_input': torch.from_numpy(np.concatenate([\n        np.zeros((n_obs, 4, 3)),\n        np.ones( (n_obs, 4, 3))], 0)),\n    'a2_input': torch.from_numpy(np.concatenate([\n        np.zeros((n_obs, 4, 2)),  \n        np.ones( (n_obs, 4, 2))], 0))}\n\nx_list_temp = [input_tensor_dict[key].to(torch.float) for key in input_tensor_dict.keys()]\nx_list_temp\n# output\n                        # Probability of\n[tensor([[[0., 0., 0.], # A\n          [0., 0., 0.], # C\n          [0., 0., 0.], # G\n          [0., 0., 0.]],# T\n\n         ...,\n\n         [[1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.]]]),\n\n tensor([[[0., 0.],\n          [0., 0.],\n          [0., 0.],\n          [0., 0.]],\n\n         ...,\n\n         [[1., 1.],\n          [1., 1.],\n          [1., 1.],\n          [1., 1.]]])]\n\nThen this data can be packaged nicely in a DataLoader5. This will retrieve the trait (y) and SNPs for each gene (in x_list) for 20 observations at a time.\n\ntraining_dataloader = DataLoader(\n  ListDataset(\n    y = y_true[:, None].to(torch.float32), # Set as 32 bit float to match network\n    x_list = [e.to(torch.float32) for e in x_list_temp]),\n    batch_size = 20,\n    shuffle = True)"
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#defining-the-graph",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#defining-the-graph",
    "title": "Making a “Visible” Neural Network",
    "section": "2. Defining the graph",
    "text": "2. Defining the graph\nThe structure of a graph can be nicely represented as a python dictionary so I’ll begin with that:\n\nnode_connections = {\n  'y_hat':['c1', 'c2'],\n  'c1':['b1'],\n  'c2':['b2'],\n  'b1':['a1_input', 'b2'],\n  'b2':['a2_input'],\n  'a1_input': [],\n  'a2_input': []\n}\n\nEach node will have an input and output size stored in a dictionary. The output sizes are easy, all nodes will have the same size except for the last node, which has predicts y, which will have a size of 1.\n\nnode_list = list(node_connections.keys())\n\ndefault_output_size = 20\noutput_size_dict = dict(zip(node_list, \n                        [default_output_size for i in range(len(node_list))]))\noutput_size_dict['y_hat'] = 1 \noutput_size_dict\n# output\n{'a1_input': 20,\n 'a2_input': 20,\n 'b1': 20,\n 'b2': 20,\n 'c1': 20,\n 'c2': 20,\n 'y_hat': 1}\n\nThe input sizes are a little trickier. A node’s input should be the number of SNPs in a gene (if it’s an input node) or the sum of the outputs of the nodes on which it depends (e.g. y_hat’s input size is the sum of c1 and c2’s outputs). To do this, I’m going to copy the dictionary with all the connections between nodes, then swap the node names for their output sizes. Summing the list of these output values will be the required input size. Data nodes don’t depend on input from other nodes, so those will have an input shape of 0.\n\ninput_size_dict = node_connections.copy()\n\nno_dependants = [e for e in node_connections.keys() if node_connections[e] == []]\n\n# use the expected output sizes from `output_size_dict` to fill in the non-data sizes\ntensor_ndim = len(input_tensor_dict[list(input_tensor_dict.keys())[0]].shape)\nfor e in tqdm(input_size_dict.keys()):\n    # overwrite named connections with the output size of those connections\n    # if the entry is in no_dependants it's data so it's size needs to be grabbed from the input_tensor_dict\n    input_size_dict[e] = [\n        (list(input_tensor_dict[ee].shape)[1]*list(input_tensor_dict[ee].shape)[2]) \n        if ee in no_dependants\n        else output_size_dict[ee] for ee in input_size_dict[e]]\n\n# Now walk over entries and overwrite with the sum of the inputs\nfor e in tqdm(input_size_dict.keys()):\n    input_size_dict[e] = np.sum(input_size_dict[e])\n    \ninput_size_dict\n# output\n{'y_hat': 40,\n 'c1': 20,\n 'c2': 20,\n 'b1': 32,\n 'b2': 8,\n 'a1_input': 0.0,\n 'a2_input': 0.0}\n\nNow we can update the graph from above adding in the input/output sizes."
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#how-to-move-through-the-graph",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#how-to-move-through-the-graph",
    "title": "Making a “Visible” Neural Network",
    "section": "3. How to move through the graph",
    "text": "3. How to move through the graph\nTo calculate the prediction for an observation each node in the graph needs to be run after all it’s input nodes have been run. Specifically, I need a list of nodes, ordered such that each node comes after all the nodes on which it depends.\nThis takes little doing. Here I use some custom helper function to find the unique entries in a dictionary, the “top” nodes (those on which no other nodes depend).\n\n# start by finding the top level -- all those keys which are themselves not values\n# helper function to get all keys and all value from a dict. Useful for when keys don't have unique values.\ndef find_uniq_keys_values(input_dict):\n    all_keys = list(input_dict.keys())\n    all_values = []\n    for e in all_keys:\n        all_values.extend(input_dict[e])\n    all_values = list(set(all_values))\n\n    return({'all_keys': all_keys,\n           'all_values': all_values})\n\n# find the dependencies for run order from many dependencies to none\n# wrapper function to find the nodes that aren't any other nodes dependencies.\ndef find_top_nodes(all_key_value_dict):\n    return([e for e in all_key_value_dict['all_keys'] if e not in all_key_value_dict['all_values']])\n\nSimilar to how I calculated each node’s output size, here I copy the connection dictionary and then manipulate it. I repeatedly identify the top-most nodes in the graph, add them to a list, and then remove them from the dictionary. Repeating this “peels” of the top layer over and over until there are nodes left. The resulting list is ordered from top most to most basal, so reversing it is all that need be done to get the order nodes should be run in.\n\n# find the dependencies for run order from many dependencies to none\ntemp = node_connections.copy()\n\ndependancy_order = []\n# Then iterate\nfor ith in range(100): \n    top_nodes = find_top_nodes(all_key_value_dict = find_uniq_keys_values(input_dict = temp))\n    if top_nodes == []:\n        break\n    else:\n        dependancy_order += top_nodes    \n        # remove nodes from the graph that are at the 'top' level and haven't already been removed\n        for key in [e for e in dependancy_order if e in temp.keys()]:\n             temp.pop(key)\n\n                \n# reverse to get the order that the nodes should be called\ndependancy_order.reverse()                \ndependancy_order\n# output\n['a2_input', 'a1_input', 'b2', 'b1', 'c2', 'c1', 'y_hat']"
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#turn-the-graph-into-a-neural-network",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#turn-the-graph-into-a-neural-network",
    "title": "Making a “Visible” Neural Network",
    "section": "4. Turn the graph into a neural network",
    "text": "4. Turn the graph into a neural network\nSo far, we have the data in a useful format (training_dataloader), a description of what the network should look like (node_connections, input_size_dict, output_size_dict), and the order that nodes in the network should be run in (dependancy_order). With this, we can build the network. I’ll start by defining a node as a linear layer (nn.Linear) that is passed into a ReLU. By creating a function6 for making nodes, changing every node in the network is as easy as editing this function.\n\ndef Linear_block(in_size, out_size, drop_pr):\n    block = nn.Sequential(\n        nn.Linear(in_size, out_size),\n        nn.ReLU())\n    return(block)  \n\nNow, I can go through each node in order of it’s dependencies and have it return the data (if it’s an input node), process inputs with a Linear_block (if it’s not an input node or the output node), or use a linear function to predict the trait7.\n\n# fill in the list in dependency order. \nlayer_list = []\nfor key in dependancy_order:\n    if key in input_tensor_names:\n        layer_list += [\n            nn.Flatten()\n        ]\n    elif key != 'y_hat':\n        layer_list += [\n            Linear_block(in_size=example_dict_input_size[key], \n                         out_size=example_dict_output_size[key])\n                      ]\n    else:\n        layer_list += [\n            nn.Linear(example_dict_input_size[key], \n                      example_dict_output_size[key])\n                      ]"
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#double-checking-the-model-structure",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#double-checking-the-model-structure",
    "title": "Making a “Visible” Neural Network",
    "section": "Double checking the model structure",
    "text": "Double checking the model structure\nUsing the lovely library torchviz, we can visualize every computational step in this model.\n\nThis is a lot to look at, but if we compare it to the earlier graph we can spot the same loop."
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#the-moment-of-truth",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#the-moment-of-truth",
    "title": "Making a “Visible” Neural Network",
    "section": "The moment of truth…",
    "text": "The moment of truth…\nNow all that is left is to see if the model trains. Using the objects describing the graph, the names of the input tensors, and the order nodes should be run it I’ll initialze the network, train it for 200 epochs aaaaaannnnddd….\n\nmodel = NeuralNetwork(example_dict = node_connections, \n                      example_dict_input_size = input_size_dict,\n                      example_dict_output_size = output_size_dict,\n                      input_tensor_names = list(input_tensor_dict.keys()),\n                      dependancy_order = dependancy_order) \n\n\nmodel, loss_df = train_nn_yx(\n    training_dataloader,\n    training_dataloader, # For demo, the training and testing data are the same.\n    model,\n    learning_rate = 1e-3,\n    batch_size = 20,\n    epochs = 200\n)\n\nIt works!\n\nNow all that’s left is to scale it up to a full genome and all the connections between the genes in it 😅."
  },
  {
    "objectID": "posts/DanielKick/230915_vnn_overview/index.html#footnotes",
    "href": "posts/DanielKick/230915_vnn_overview/index.html#footnotes",
    "title": "Making a “Visible” Neural Network",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHave a look at the page source.↩︎\nThis will not be a graph neural network, although they may be effective here.↩︎\nA box of numbers that can have multiple dimensions. A matrix is a “rank-2” tensor.↩︎\nTechnically, with 4 possibilities you only need 3 binary digits where [0, 0, 0] would be 100% probability of the fourth nucleotide↩︎\nI’m using a custom Dataset subclass. See source for details.↩︎\nTechnically a method since it’s in a class.↩︎\nAs an aside, the first time I wrote this I had all non-input nodes be Linear_blocks. This resulted in fair bit frusterated debugging as the network would either train perfectly or fail to train depending on how the last ReLU was initialized🤦🏼‍♂️.↩︎"
  },
  {
    "objectID": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html",
    "href": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html",
    "title": "Worse is better and not doing things “right”",
    "section": "",
    "text": "“Worse is better” is an idea I get a lot of mileage out of. Here’s the crux of it:\nI find this is useful descriptively1 but also prescriptively as a way to spend less time doing work that doesn’t need to be done.\nIn brief the idea is that once you have something that works it’s often not worth altering it to make it faster, more efficient, or more elegant … at least initially. Optimization is important (example but what I’m talking about here premature optimization. Avoiding the urge to improve things that aren’t the priority can be difficult, especially when you conceptually know what you would change."
  },
  {
    "objectID": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html#simplified-example",
    "href": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html#simplified-example",
    "title": "Worse is better and not doing things “right”",
    "section": "Simplified Example",
    "text": "Simplified Example\nHere’s an example: I’m building a network that ‘compresses’ information. The key idea is that there’s a function, f(), takes in some number of values and outputs fewer values. We can use this function over and over again to compress the values more and more. Once they’re ‘compressed’ we can do the reverse procedure and get more values until we’re back at the starting number.\nThere’s a catch however, and that’s that the function can only output an integer number of values even if the number should be a fraction. It’s like this division function. If the numerator argument is the number of input values and it will return numerator/3 values.\n\ndiv_it &lt;- function(numerator, divisor = 3){\n  res = numerator/divisor\n  res = round(res)\n  return(res)\n}\n\ndiv_it(100)\n\n[1] 33\n\n\nBecause it can only return whole numbers, we can’t reverse this procedure and always get back the same number – sometimes we have to add or subtract a little bit.\n\ninv_div_it &lt;- function(numerator, divisor = 3){\n  return(numerator*divisor)\n}\n\ninv_div_it(33)\n\n[1] 99\n\ninv_div_it(33)+1\n\n[1] 100\n\n\nIf we want to really compress the input (f(X) |&gt; f(X) |&gt; f(X) |&gt; f(X) or f(f(f(f(X))))) then the number of values at each level would be:\n\nvals &lt;- c(100)\nfor(i in 1:4){\n  i_val &lt;- vals[length(vals)]\n  vals[length(vals)+1] &lt;- div_it(i_val) \n}\nvals\n\n[1] 100  33  11   4   1\n\n\nIdeally running the inverse procedure multiple times on the last output above (just one value) would output produce:\n\nvals_reverse &lt;- vals[length(vals):1]\nvals_reverse\n\n[1]   1   4  11  33 100\n\n\nBut using the inverse function defined above (inv_div_it()) we get:\n\nrecover_vals &lt;- c(1)\nfor(i in 1:4){\n  i_val &lt;- recover_vals[length(recover_vals)]\n  recover_vals[length(recover_vals)+1] &lt;- inv_div_it(i_val) \n}\nrecover_vals\n\n[1]  1  3  9 27 81\n\n\nTo get back to 100 values we need to add a new value (imagine appending a 1 to an array) sometimes, and drop a value others, or make no change to output other times.\n\nadd_vals &lt;- c(1, -1, 0, 1)\n\nrecover_vals &lt;- c(1)\nfor(i in 1:4){\n  i_val &lt;- recover_vals[length(recover_vals)]\n  print(add_vals[i])\n  recover_vals[length(recover_vals)+1] &lt;- inv_div_it(i_val) + add_vals[i]\n}\n\n[1] 1\n[1] -1\n[1] 0\n[1] 1\n\nrecover_vals\n\n[1]   1   4  11  33 100\n\n\nWe could keep track of the remainder each time f() is called and use that to figure out when to add or subtract 1. That would be the elegant and efficient solution. We know the desired output (100 values) and the number of times f() was called (4) so we could also try changing the numbers in add_vals until we have four numbers that. This solution would be inelegant but still effective.\nIf a piece of code only needs to be a few times then the cost of the time you’d spend optimizing it will probably be worth more than than cost of the time the computer spends running it (see also).\nIf the sloppy way to express what you want is good enough then don’t worry about it. Good enough now is often better than perfect later."
  },
  {
    "objectID": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html#example-in-context-python",
    "href": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html#example-in-context-python",
    "title": "Worse is better and not doing things “right”",
    "section": "Example in Context (python)",
    "text": "Example in Context (python)\nThe motivating problem behind this write up is that ‘compressing’ weather data (17 measurements for 365 days) into fewer values. I’m using a variation autoencoder with convolution layers which you can imagine as passing a sliding window over a 17x365 grid and summarizing each windowed chunk to get fewer values.\nTo check if the compression is effective, we have to compress 17x365 values down to something smaller (e.g. 17x23), and inflate them back to 17x365 so we can compare the input weather to the output weather. If we can get back the same 17x365 values (or something pretty close) then the comprssion is effective..\nFrom the input data’s length (days) you can calculate what a convolutional layer’s output length will be like so:\ndef L_out_conv1d(\n    L_in = 365, \n    kernel_size=3, stride = 2, padding=1, dilation = 1\n): return ((L_in +2*padding-dilation*(kernel_size-1)-1)/stride)+1\n\nL_out_conv1d(L_in = 365) # 183.0\nAnd the same for reversing the operation (with a transposed convolution).\ndef L_out_convT1d(\n    L_in = 183, \n    kernel_size=3, stride = 2, padding=1, output_padding=0, dilation = 1\n): return (L_in - 1)*stride-2*padding+dilation*(kernel_size-1)+output_padding+1\n\nL_out_convT1d(L_in = 183) # 365.0\nThe trouble is that if I stack convolution layers the output length can become a fraction, which is forced to an integer, and prevents the reverse operation from producing the right number. When I use 4 layers the length should be [365, 183.0, 92.0, 46.5, 23.75] which as integers is [365, 183, 92, 46, 23]. Reversing the operation produces [23, 45, 89, 177, 353].\nWe can get back to 365 days by increasing the output’s length in some of the transposed convolution layers by adding a non-zero output_padding. I don’t know how many layers will be best, so I can’t hard code these values. I could use the functions above to calculate what when the output_padding should be 0 and when it shouldn’t (the elegant solution), but that’s not what I did.\nInstead I made a simple disposable neural network just to check if I had the output_paddings right by tracking the lengths of the tensor after each layer.\n# input data. One observation, 17 measurements, 365 days of measurements. \n# It's all 0s because all I care about right now is the dimensions of the data.\nxin = torch.zeros((1, 17, 365))\n\n# Proposed output_padding for each layer in the decoder network\nlayer_output_padding = [0, 0, 0, 0, 0, 0, 0, 0]\n\n# encoder network\nne = nn.ModuleList([\n    nn.Sequential(nn.Conv1d(\n    17, out_channels=17, kernel_size= 3, stride= 2, padding  = 1), nn.BatchNorm1d(\n    17), nn.LeakyReLU())\n    for i in range(len(layer_output_padding))\n])\n\n# Decoder network\nnd = nn.ModuleList([\n    nn.Sequential(nn.ConvTranspose1d(\n    17, 17, \n    kernel_size=3, stride = 2, padding=1, output_padding=layer_output_padding[i]), nn.BatchNorm1d(\n    17), nn.LeakyReLU())\n    for i in range(len(layer_output_padding))\n])\nThen I can run this network …\n# list to store lengths\ntensor_Ls = []\n\n# add the input data's length (days)\ntensor_Ls += [list(xin.shape)[-1]] \n\n# encode data\nfor mod in ne:\n    xin = mod(xin)\n    tensor_Ls += [list(xin.shape)[-1]]\n\n# add the encoded data's \ntensor_Ls += [str(tensor_Ls[-1])]\n\n# decode data\nfor mod in nd:\n    xin = mod(xin)\n    tensor_Ls += [list(xin.shape)[-1]]\n… and look at the first and last value of the list of lengths (tensor_Ls) to see if the proposed output paddings will work.\ntensor_Ls[0] == tensor_Ls[-1]\n# False\ntensor_Ls\n# [365, 183, 92, 46, 23, 12, 6, 3, 2, '2', 3, 5, 9, 17, 33, 65, 129, 257]\nNext I need a way to systematically produce different paddings. For a decoder of four layers I would test paddings [0, 0, 0, 0], [1, 0, 0, 0], ... [1, 1, 1, 1] stopping at the first list that works. So I’ll write a function to increment [0, 0, 0, 0] to [1, 0, 0, 0].\ndef increment_list(\n    in_list = [0, 0, 0, 0],\n    min_value = 0,\n    max_value = 1):\n    # Check that all entries are within min/max\n    if False in [True if e &lt;= max_value else False for e in in_list]:\n        print('Value(s) above maximum!')\n    elif False in [True if e &gt;= min_value else False for e in in_list]:\n        print('Value(s) below minimum!')\n    elif [e for e in in_list if e != max_value] == []:\n        print('List at maximum value!')\n    else:    \n        # start cursor at first non-max value\n        for i in range(len(in_list)):\n            if in_list[i] &lt; max_value:\n                in_list[i] += 1\n                break\n            else:\n                in_list[i] = min_value\n    return(in_list)\n\nincrement_list()\n# [1, 0, 0, 0]\nThen we can loop through possible paddings until we find one that works or have tried all of them.\n# Proposed output_padding for each layer in the decoder network\nlayer_output_padding = [0, 0, 0, 0, 0, 0, 0, 0]\n\nwhile True:\n    # save a backup of the current padding\n    old_layer_output_padding = layer_output_padding.copy()\n    \n    \n    # ... define and run network here ...\n    \n    \n    # If it did work we're done\n    if True == (tensor_Ls[0] == tensor_Ls[-1]):\n        print('done!')\n        \n    # If the padding _didn't_ work change it\n    else:\n        layer_output_padding = increment_list(\n            in_list = layer_output_padding,\n            min_value = 0,\n            max_value = 1)\n    \n    # If the proposed new padding is the same as the backup, then we have tried all the possible paddings and will stop. \n    if layer_output_padding == old_layer_output_padding: \n        break\n\nAll together\nHere’s the full loop and its output:\n\n# Proposed output_padding for each layer in the decoder network\nlayer_output_padding = [0, 0, 0, 0, 0, 0, 0, 0]\n\nwhile True:\n    # save a backup of the current padding\n    old_layer_output_padding = layer_output_padding.copy()\n    \n    # input data. One observation, 17 measurements, 365 days of measurements. \n    # It's all 0s because all I care about right now is the dimensions of the data.\n    xin = torch.zeros((1, 17, 365))\n\n    # encoder network\n    ne = nn.ModuleList([\n        nn.Sequential(nn.Conv1d(\n        17, out_channels=17, kernel_size= 3, stride= 2, padding  = 1), nn.BatchNorm1d(\n        17), nn.LeakyReLU())\n        for i in range(len(layer_output_padding))\n    ])\n\n    # Decoder network\n    nd = nn.ModuleList([\n        nn.Sequential(nn.ConvTranspose1d(\n        17, 17, \n        kernel_size=3, stride = 2, padding=1, output_padding=layer_output_padding[i]), nn.BatchNorm1d(\n        17), nn.LeakyReLU())\n        for i in range(len(layer_output_padding))\n    ])\n    \n    # list to store lengths\n    tensor_Ls = []\n\n    # add the input data's length (days)\n    tensor_Ls += [list(xin.shape)[-1]] \n\n    # encode data\n    for mod in ne:\n        xin = mod(xin)\n        tensor_Ls += [list(xin.shape)[-1]]\n\n    # add the encoded data's \n    tensor_Ls += [str(tensor_Ls[-1])]\n\n    # decode data\n    for mod in nd:\n        xin = mod(xin)\n        tensor_Ls += [list(xin.shape)[-1]]    \n    \n    # If it did work we're done\n    if True == (tensor_Ls[0] == tensor_Ls[-1]):\n        print('done!')\n        \n    # If the padding _didn't_ work change it\n    else:\n        layer_output_padding = increment_list(\n            in_list = layer_output_padding,\n            min_value = 0,\n            max_value = 1)\n    \n    # If the proposed new padding is the same as the backup, then we have tried all the possible paddings and will stop. \n    if layer_output_padding == old_layer_output_padding: \n        break\n\n# done!\n\nlayer_output_padding  \n# [0, 1, 1, 0, 1, 1, 0, 0]\n\ntensor_Ls\n# [365, 183, 92, 46, 23, 12, 6, 3, 2, '2', 3, 6, 12, 23, 46, 92, 183, 365]\nThis may not be as not as elegant or as efficient as it could be, but it doesn’t matter. It only takes about 200ms so it’s not worth improving unless."
  },
  {
    "objectID": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html#footnotes",
    "href": "posts/DanielKick/231013_premature_optimiztion_worse_is_better/index.html#footnotes",
    "title": "Worse is better and not doing things “right”",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ne.g. If scientific manuscripts with embedded code are valuable for reproducibility, why haven’t they become the default? There’s a lot of energy needed to switch and all of your collaborators already know word. 🤷🏼‍♂ ↩︎"
  },
  {
    "objectID": "posts/DanielKick/240514_bash_jupyter_nb/index.html",
    "href": "posts/DanielKick/240514_bash_jupyter_nb/index.html",
    "title": "Quick Tip: Write notebooks, run scripts",
    "section": "",
    "text": "Much of what I write is in notebooks (thanks to the lovely nbdev library) but there are times where this is not convenient. For instance, while tuning hyperparameters or running other processes that can take a long time, it would be useful to detach a notebook from my IDE while it’s running. One dead simple way to do this on linux is with the “no hangup” command (nohup).\nAll we have to do is: 1. activate the enviroment:\n$ conda activate my_env\n\nuse jupyter to create a python script from the desired notebook: (my_env) $ jupyter nbconvert --to python my_notebook.ipynb\nrun the notebook (in our environment) with nohup so that the shell can be disconnected and & to run the command in the background (my_env) $ nohup python ./my_notebook &\n\nEt voilà! The process shows up on the GPU and we don’t have to worry about a bad internet connection or anything else stopping it before it’s finished.\n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A      1461      G   /usr/lib/xorg/Xorg                            4MiB |\n|    0   N/A  N/A    767952      C   python                                     3764MiB |\n+---------------------------------------------------------------------------------------+"
  },
  {
    "objectID": "posts/DanielKick/240624_linux_wsl_ln/index.html",
    "href": "posts/DanielKick/240624_linux_wsl_ln/index.html",
    "title": "Tip: Make your life easier with Symbolic Links in WSL",
    "section": "",
    "text": "Modern Windows machines can give you access to Linux command line tools via the Windows Subsystem for Linux. On launch the present working directory is set to the subsystem’s home rather than a location in the main Window’s file structure.\n$ pwd\n/home/&lt;user&gt;\nThe main file structure is in /mnt/ so if you’re for example, trying to pattern match and zip a few files getting there is a bit of a pain. The quick solution is to create symbolic links (shortcuts) between the two.\nThe syntax for symbolic links is ls -s source target.\n$ ln -s /mnt/c/Users/&lt;UserName&gt;/Desktop Desktop\n$ ln -s /mnt/c/Users/&lt;UserName&gt;/Documents Documents\n$ ln -s /mnt/c/Users/&lt;UserName&gt;/Downloads Downloads\n$ ls\nDesktop Documents  Downloads \nAfter creating these links we cd into them and end up in the main system and keep just a little more momentum than we otherwise would"
  },
  {
    "objectID": "posts/DanielKick/240905_postgres_via_parquet/index.html",
    "href": "posts/DanielKick/240905_postgres_via_parquet/index.html",
    "title": "SQL Strikes Back",
    "section": "",
    "text": "For additional context please see Part 1 Part 2\nFor those that want to jump straight to the resources here are the files discussed below:\nPreviously I generated an absolute boat load of simulated data for modeling. Even after taking steps to remove unneeded values and storing as a file type with compression (parquet) the full set is still a whopping 285G.\nWe have several competing goods here that we’d like to balance. Ideally the data would be\nAs detailed in “Worse is better case study 2” our current solution sacrifices availability of all the data and (initially) query speed to get a set that fits in memory and can be moved to an HPC. Ultimately, we want to add new simulated results to this set while training which isn’t possible.\nThe solution of course is to bite the bullet and set up a database then write a new dataloader to get minibatches from the database. This means we’ll be moving batches on/off the gpu but we can access any records and need not read in files over and over."
  },
  {
    "objectID": "posts/DanielKick/240905_postgres_via_parquet/index.html#the-plan",
    "href": "posts/DanielKick/240905_postgres_via_parquet/index.html#the-plan",
    "title": "SQL Strikes Back",
    "section": "The Plan:",
    "text": "The Plan:\nThe target workflow for this is to\n\nInitialize a database\nRead in metadata parquet files as separate tables (genotypes, ids)\nRead in each results parquet file, add the file name as a field, and append its values to a table (results)\n\nEasy. How long could it take?"
  },
  {
    "objectID": "posts/DanielKick/240905_postgres_via_parquet/index.html#learning-postgresql-on-the-way-down",
    "href": "posts/DanielKick/240905_postgres_via_parquet/index.html#learning-postgresql-on-the-way-down",
    "title": "SQL Strikes Back",
    "section": "Learning PostgreSQL On The Way Down:",
    "text": "Learning PostgreSQL On The Way Down:\nI considered using duckdb for it’s speed and simplicity but ultimately elected to use PostgreSQL since it’s been around longer (1996) and boasts limits way beyond what I need. Please don’t take the below as best practice or even recommendations per se as this is the first non-SQLite database I’ve administered. This is just notes on the stumbling blocks I encountered. The best thing one can do is read the official PostgreSQL tutorial. Several times I searched for the solution to a problem just to find it covered a few pages later.\nInstallation (in this case on ubuntu via apt) was easy but database creation takes some doing. PostgreSQL defines accounts that are separate from the host system. This includes a special account postgres with root permisions that is initially available. Using sudo -u postgres &lt;cmd&gt; one can run commands as if this account existed on the host machine.\nWe use this account to create the database and then login as root.\nsudo -u postgres createdb apsimxsim\nsudo -u postgres psql\nNext we create a non-root account and provide permissions1 for the apsimxsim database and public schema.\npostgres=# CREATE ROLE loremipsum LOGIN PASSWORD 'TotallyRealPassword';\nCREATE ROLE\npostgres=# ALTER ROLE loremipsum CREATEDB;\nALTER ROLE\npostgres=# GRANT ALL PRIVILEGES ON DATABASE apsimxsim TO loremipsum;\nGRANT\napsimxsim=# GRANT ALL ON SCHEMA public TO loremipsum;\nGRANT\nWe can check permissions using \\du (note that psql specific commands begin with \\ e.g., \\q).\npostgres=# \\du\n                             List of roles\n Role name |                         Attributes\n-----------+------------------------------------------------------------\n loremipsum     | Create DB\n postgres  | Superuser, Create role, Create DB, Replication, Bypass RLS\nWe’re not done with setup yet. We also need to change the search path for the loremipsum user so we can find our tables.\n$psql apsimxsim\napsimxsim=&gt; SET search_path TO public;\nNow we should be set. We can add / drop tables from psql but we’re going to use python’s SQLAlchemy library to do most of the heavy lifting including creating these tables.\nFast forwarding a little, here’s what we’re aiming at. Ultimately the loremipsum user will own three tables with the metadata (ids, genotypes) and simulated results (results). A final quirk I’ll mention here is that it’s possible to use capitalized names but it a bit of a hasle. We’d have to select them as public.\"Ids\" instead of public.ids. For this reason we’ll standardized all table and field names using only lowercase letters and underscores.\napsimxsim=&gt; SELECT schemaname, tablename, tableowner FROM pg_tables WHERE tableowner='loremipsum';\n schemaname | tablename | tableowner\n------------+-----------+------------\n public     | ids       | loremipsum\n public     | genotypes | loremipsum\n public     | results   | loremipsum\n(3 rows)"
  },
  {
    "objectID": "posts/DanielKick/240905_postgres_via_parquet/index.html#starting-with-the-end-in-mind",
    "href": "posts/DanielKick/240905_postgres_via_parquet/index.html#starting-with-the-end-in-mind",
    "title": "SQL Strikes Back",
    "section": "Starting With The End In Mind",
    "text": "Starting With The End In Mind\nBefore developing anything too complicated we should check if we can get easily get data from SQL to python. We can’t retrieve data without inserting it so in reality I’m showing things a little out of order. Bear with me and it’ll all fit together.\nThankfully getting data from SQL to python isn’t a new problem so there should be many workable solutions (e.g. discussion, asyncpg, warp_prism).\nWe’re going to start simple and use sqlalchemy with psycopg2 to connect to the database.\nUltimately we’ll have something like this:\nimport psycopg2\nfrom   sqlalchemy import create_engine, text\n\nengine = create_engine(\n        \"db_string_here\"\n        )\n\nwith engine.connect() as conn:\n    result = conn.execute(\n      text('SELECT * FROM public.ids LIMIT 1')\n      )\nOnce we have result we can transform it into a tensor and be set. There are likely more performant ways to get data out, but as long as we can get data out we can go to the next steps."
  },
  {
    "objectID": "posts/DanielKick/240905_postgres_via_parquet/index.html#dont-leak-passwords",
    "href": "posts/DanielKick/240905_postgres_via_parquet/index.html#dont-leak-passwords",
    "title": "SQL Strikes Back",
    "section": "Don’t Leak Passwords",
    "text": "Don’t Leak Passwords\nThat placeholder string up above, \"db_string_here\" will contain the username, password, host, and port, and database name. Even if our machine isn’t publicly accessible and this data isn’t sensitive, we still don’t want to make this information public. There are different ways to do this such as using storing this information as enviromental variables or keeping it in a separate file (excluded from version control in your .gitignore). The latter is what I have demonstrated here but if you’re working with more sensitive data please consult a security expert regarding your use case.\nHere, we’ll create a file psql_details.json that will hold the information needed for \"db_string_here\".\n{\n  \"user\": \"loremipsum\", \n  \"pass\": \"TotallyRealPassword\", \n  \"host\": \"localhost\", \n  \"port\": \"5432\", \n  \"name\": \"apsimxsim\"\n}\nNow we can read this as a dictionary and insert these values into the connection string.\nwith open('./psql_details.json', 'r') as f:\n    d = json.load(f)\n    \nengine = create_engine(\n        f\"postgresql+psycopg2://{d['user']}:{d['pass']}@{d['host']}:{d['port']}/{d['name']}\"\n        )"
  },
  {
    "objectID": "posts/DanielKick/240905_postgres_via_parquet/index.html#preparation-for-data-migration",
    "href": "posts/DanielKick/240905_postgres_via_parquet/index.html#preparation-for-data-migration",
    "title": "SQL Strikes Back",
    "section": "Preparation for Data Migration",
    "text": "Preparation for Data Migration\nThe workflow to insert data into the database is to read in the parquet file as a pandas dataframe, make any needed changes, then write that table to the database.\nThe pattern will look something like this.\nexample = pq.read_table(parquet_path+'example.parquet').to_pandas()\nexample = example.rename(columns={e:e.lower().replace('.', '_') for e in list(example)})\nexample.to_sql(name='example', con=engine, if_exists = 'append',  schema='public')\nHere the only change to the dataframe we’re making is to set all the column names to be lowercase without periods. As with using all lowercase table names, this consistency will make it easier to write SQL without worrying about quotes."
  },
  {
    "objectID": "posts/DanielKick/240905_postgres_via_parquet/index.html#data-migration-ad-hoc-parallelism",
    "href": "posts/DanielKick/240905_postgres_via_parquet/index.html#data-migration-ad-hoc-parallelism",
    "title": "SQL Strikes Back",
    "section": "Data Migration & Ad Hoc Parallelism",
    "text": "Data Migration & Ad Hoc Parallelism\nWhen writing to the database we want to append to the table if it already exists. This means that if the script is disrupted we run the risk of writing duplicate rows when the script runs again.\nIf we keep track of what files have been processed, it’s a short jump to enabling parallelism. The approach we’ll take is to…\n\nIdentify all files to be imported\nCheck if a file has already been processed\nCheck if a file is being worked on by another process\nWrite to a log which file is going to be processed\nProcess file\nWrite to a log that the file has finished being processed\n\nThese steps will be in the script parquet_to_psql.py so we can easily run this from the command line.\nWe’ll use os.listdir to find all the files in the target directory then filter the files with re to get those that follow a naming convention sim_######_######.parquet.\n# Identify files to add\nparquet_path = '/path/to/parquet_files/'\n\n# constrain to the parquets that exist in the path\nexisting_parquet = [e for e in os.listdir(parquet_path) if re.match('sim_\\d+_\\d+\\.parquet', e)]\nWe’ll use a simple list saved to a json file to track what files have been processed already and use it to exclude these entries from our queue.\nfinished_parquet = [] \nif os.path.exists('./finished_parquet.json'):\n    with open('./finished_parquet.json', 'r') as f:\n        finished_parquet = json.load(f)\n\nexisting_parquet = [e for e in existing_parquet if e not in finished_parquet]\nNow we’ll iterate over some number of the un-processed files but we’ll use the same trick to check if the parquet is being worked on by another instance of this script. The main difference between finished_parquet.json and reserved_parquet.json is that the latter we’ll update as soon as we confirm the parquet we’re considering (variable e) isn’t reserved. After processing we’ll log that the file has been processed.\nnum_parquets = 3 # How many files to process per run\nfor e in existing_parquet[0:num_parquets]: \n    # so that we can run this process in parallel we'll track which files have been reserved\n    reserved_parquet = []\n    if os.path.exists('./reserved_parquet.json'):\n        with open('./reserved_parquet.json', 'r') as f:\n            reserved_parquet = json.load(f)\n            \n    if e in reserved_parquet:\n        pass\n    else:\n        # log that this entry reserved\n        reserved_parquet.append(e)\n        with open('./reserved_parquet.json', 'w') as f:\n            json.dump(reserved_parquet, f)\n            \n        # Process File\n        # ...\n\n        # After completion, log that the file has been processed\n        finished_parquet.append(e)\n        with open('./finished_parquet.json', 'w') as f:\n            json.dump(finished_parquet, f)\n        print('\\n')  \nOne additional quirk is worth noting. There are a ton of records in some of these files – enough to cause the process to stall out. To get around this we’re going to break the dataframe into blocks of 1,000,000 rows and write each separately (There’s certainly a better way to achieve this, but hey, it works.)\nWith all that done we can run python ./parquet_to_psql.py and it’ll take care of the migration for us!"
  },
  {
    "objectID": "posts/DanielKick/240905_postgres_via_parquet/index.html#footnotes",
    "href": "posts/DanielKick/240905_postgres_via_parquet/index.html#footnotes",
    "title": "SQL Strikes Back",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee also: https://stackoverflow.com/questions/67276391/why-am-i-getting-a-permission-denied-error-for-schema-public-on-pgadmin-4↩︎"
  },
  {
    "objectID": "posts/DanielKick/241024_n_dim_arrays_R_v_np/index.html",
    "href": "posts/DanielKick/241024_n_dim_arrays_R_v_np/index.html",
    "title": "Sharing n-dimensional data between Python and R",
    "section": "",
    "text": "Storing multidimensional data is a pain. In R or python you can save the n-dimensional array to disk, but not in a way that is easily accessible by the other. Admittedly, this is a rare issue to want to solve (It’s taken 3 years of deep learning for it to come up) but it’s worth discussing regardless as it illustrates a stumbling block when working between these languages. Think of it as the advanced version of trying to get the 0th entry in a R vector.\nZooming out, if we have a 3d array of data (say observations by nucleotides by SNPs) we have a few options on how to save it.\n\nWe could save it in a format designed for n-dim arrays (e.g. numpy’s .npz format). I’m not aware of any options that would allow for reading into both R and python (there certainly could be some) so we’ll ignore this option.\nWe could reduce this to a set of 2d arrays and save the lot. Then we can use any format for tabular data – text files, sqlite databases, parquet files – all would be an option. The downside is we’d be working with (potentially) many files. This isn’t a deal breaker1 but it would be nice to avoid.\nWe could store the data and dimensions separately. We would end up with only two files each with a 1d array representing the values and dimensions. We could even put the dimensions in the name of the file if we wanted to and lets us use all the file types in option 2. Reshaping the data sounds nicer than reconstituting it so let’s go with that.\n\nI’m a fan of parquet files for storing large amounts of data, especially when they need to be accessible by multiple programming languages. So the plan will be to turn the n-dim array into a 1-dim array and store the original dimension separately. On the other end we’ll reshape this 1-dim array back into the right shape. Easy, right?"
  },
  {
    "objectID": "posts/DanielKick/241024_n_dim_arrays_R_v_np/index.html#motivating-problem",
    "href": "posts/DanielKick/241024_n_dim_arrays_R_v_np/index.html#motivating-problem",
    "title": "Sharing n-dimensional data between Python and R",
    "section": "",
    "text": "Storing multidimensional data is a pain. In R or python you can save the n-dimensional array to disk, but not in a way that is easily accessible by the other. Admittedly, this is a rare issue to want to solve (It’s taken 3 years of deep learning for it to come up) but it’s worth discussing regardless as it illustrates a stumbling block when working between these languages. Think of it as the advanced version of trying to get the 0th entry in a R vector.\nZooming out, if we have a 3d array of data (say observations by nucleotides by SNPs) we have a few options on how to save it.\n\nWe could save it in a format designed for n-dim arrays (e.g. numpy’s .npz format). I’m not aware of any options that would allow for reading into both R and python (there certainly could be some) so we’ll ignore this option.\nWe could reduce this to a set of 2d arrays and save the lot. Then we can use any format for tabular data – text files, sqlite databases, parquet files – all would be an option. The downside is we’d be working with (potentially) many files. This isn’t a deal breaker1 but it would be nice to avoid.\nWe could store the data and dimensions separately. We would end up with only two files each with a 1d array representing the values and dimensions. We could even put the dimensions in the name of the file if we wanted to and lets us use all the file types in option 2. Reshaping the data sounds nicer than reconstituting it so let’s go with that.\n\nI’m a fan of parquet files for storing large amounts of data, especially when they need to be accessible by multiple programming languages. So the plan will be to turn the n-dim array into a 1-dim array and store the original dimension separately. On the other end we’ll reshape this 1-dim array back into the right shape. Easy, right?"
  },
  {
    "objectID": "posts/DanielKick/241024_n_dim_arrays_R_v_np/index.html#the-curse-of-dimensionality-but-not-that-one",
    "href": "posts/DanielKick/241024_n_dim_arrays_R_v_np/index.html#the-curse-of-dimensionality-but-not-that-one",
    "title": "Sharing n-dimensional data between Python and R",
    "section": "The Curse of Dimensionality (but not that one)",
    "text": "The Curse of Dimensionality (but not that one)\nIt’s easiest to see the problem with an example. Suppose we want to “remake” 3d data of shape (2, 3, 4). We’re starting with an array of values (vals) which are the integers 0-23. We can reshape this array using .reshape and then look at at the 0th plane of the data.\n\n# original python\nimport numpy as np\narr_shape = (2,3,4)\nvals = np.arange(np.prod(arr_shape))\n\nx = np.array(vals).reshape(arr_shape)\nx[0, :, :]\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n\n\nWhen we repeat this in R (selecting the 1st plane because R is not 0 indexed) we don’t get the same values.\n\n# original R\narr_shape &lt;- c(2,3,4)\nvals &lt;- seq(0, prod(arr_shape)-1 )\n\nx &lt;- array(vals, dim = arr_shape)\nx[1, , ]\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    6   12   18\n[2,]    2    8   14   20\n[3,]    4   10   16   22"
  },
  {
    "objectID": "posts/DanielKick/241024_n_dim_arrays_R_v_np/index.html#whats-going-on-and-how-do-we-fix-it",
    "href": "posts/DanielKick/241024_n_dim_arrays_R_v_np/index.html#whats-going-on-and-how-do-we-fix-it",
    "title": "Sharing n-dimensional data between Python and R",
    "section": "What’s going on and how do we fix it?",
    "text": "What’s going on and how do we fix it?\nIt’s easiest to see what’s happening by looking for the slice that begins with 0.\n\nx[0, 0, ]\n\narray([0, 1, 2, 3])\n\n\n\nx[, 1, 1]\n\n[1] 0 1\n\n\nThese values are in the last dimension in python but the first in R! In effect, numpy and R ‘fill’ values from the opposite directions.\nSo how do we fix this? We have to change the shape of the array so that it fills properly and then “rotate” the dimensions until we have the right shape. We’ll do this by\n\nReversing the desired dimensions\nFilling the values into the array\nSwapping the axes (swapping the first and last, second and penulitmate, etc. )\n\nFinally, here’s how this would be done for either language:"
  },
  {
    "objectID": "posts/DanielKick/241024_n_dim_arrays_R_v_np/index.html#option-1-remaking-a-numpy-array-in-r",
    "href": "posts/DanielKick/241024_n_dim_arrays_R_v_np/index.html#option-1-remaking-a-numpy-array-in-r",
    "title": "Sharing n-dimensional data between Python and R",
    "section": "Option 1: Remaking a Numpy array in R",
    "text": "Option 1: Remaking a Numpy array in R\nHere’s the python we want to mimic:\n\n# original python\nimport numpy as np\narr_shape = (2,3,4)\nvals = np.arange(np.prod(arr_shape))\n\nx = np.array(vals).reshape(arr_shape)\nx[0, :, :]\n\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])\n\n\n\narr_shape &lt;- c(2,3,4)\nvals &lt;- seq(0, prod(arr_shape)-1 )\n\nx &lt;- array(vals, dim = rev(arr_shape)) # reverse the dims\nx &lt;- aperm(x, rev(seq_along(arr_shape)))    # transpose by permuting dims\nx[1, , ]\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    1    2    3\n[2,]    4    5    6    7\n[3,]    8    9   10   11"
  },
  {
    "objectID": "posts/DanielKick/241024_n_dim_arrays_R_v_np/index.html#option-2-remaking-a-r-array-in-numpy",
    "href": "posts/DanielKick/241024_n_dim_arrays_R_v_np/index.html#option-2-remaking-a-r-array-in-numpy",
    "title": "Sharing n-dimensional data between Python and R",
    "section": "Option 2: Remaking a R array in Numpy",
    "text": "Option 2: Remaking a R array in Numpy\nHere’s the R we want to mimic:\n\n# original R\narr_shape &lt;- c(2,3,4)\nvals &lt;- seq(0, prod(arr_shape)-1 )\n\nx &lt;- array(vals, dim = arr_shape)\nx[1, , ]\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    6   12   18\n[2,]    2    8   14   20\n[3,]    4   10   16   22\n\n\n\nimport numpy as np\narr_shape = (2,3,4)\nvals = np.arange(np.prod(arr_shape))\n\narr_shape = list(arr_shape)\narr_shape.reverse() # now is [4, 3, 2]\nx = np.array(vals).reshape(arr_shape)\n# we only want to iterate over the first half of the list.\n# Iterating over all of it will swap all axes and then swap them back\n# Coercing 1/2 the length to an int will round down values in the case of odd dims\nfor i in range( int(len(arr_shape) / 2) ): \n  j = (len(arr_shape)-1)-i\n  x = x.swapaxes(i, j)\nx[0, :, :]\n\narray([[ 0,  6, 12, 18],\n       [ 2,  8, 14, 20],\n       [ 4, 10, 16, 22]])"
  },
  {
    "objectID": "posts/DanielKick/241024_n_dim_arrays_R_v_np/index.html#footnotes",
    "href": "posts/DanielKick/241024_n_dim_arrays_R_v_np/index.html#footnotes",
    "title": "Sharing n-dimensional data between Python and R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOnce I split a file with genomes into several thousand files just to minimize what had to be read in.↩︎"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240501_PreMeeting/index.html",
    "href": "posts/DeepLearningCommunityofPractice/240501_PreMeeting/index.html",
    "title": "Summer Session: Pre-Meeting Preparation",
    "section": "",
    "text": "Hello Everyone!\nI think everyone who will fill out the survey has and it looks like Tuesdays from 11-12 fits the most people’s schedules. Our first meeting will be Tuesday May 7th at 11-12am.\nI’ve included information on preparing for the first session below and please let me know if you have any questions.\nBest, Daniel\nLogistics: I have reserved room 219a in Curtis Hall for those attending in person and there will be a videochat link in the invitation for those attending virtually. I’m looking into recording the meetings and making them internally available for anyone who can’t make it, but I can’t guarantee I’ll have this sorted.\nPreparing for our first meeting: Before we meet I would like everyone to write down your answers to the following questions.\n\nAre you interested in the low-level details of deep learning or mostly high-level application?\nWhere do you see yourself using deep learning? In the lab or for personal use?\nAfter learning these topics what goals will you be better situated to achieve?\n\nCloser to the date of I will send out some instructions on the software we’ll be using. Please follow these setup instructions.\nOur first meeting will be composed of three parts. First, I’ll give a brief introduction. Next, we’ll talk about everyone’s answers to the questions above. The goal of this is to help people with similar uses in mind and interests find each other – so over time we can grow from a learning group to a community of practice. Finally, we’ll talk about software and setup to fix any issues for the next meeting.\nFuture meetings: Beyond this first meeting we’ll have some lecture and exercises that we’ll complete in between sessions. In the sessions we’ll talk though the exercises and what people found challenging or interesting.\nThe material will initially focus on building networks from the ground up so when we start using research grade libraries how they work isn’t a black box. After a few sessions we’ll take stock and adjust course as needed."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240501_PreMeeting/index.html#introduction",
    "href": "posts/DeepLearningCommunityofPractice/240501_PreMeeting/index.html#introduction",
    "title": "Summer Session: Pre-Meeting Preparation",
    "section": "",
    "text": "Hello Everyone!\nI think everyone who will fill out the survey has and it looks like Tuesdays from 11-12 fits the most people’s schedules. Our first meeting will be Tuesday May 7th at 11-12am.\nI’ve included information on preparing for the first session below and please let me know if you have any questions.\nBest, Daniel\nLogistics: I have reserved room 219a in Curtis Hall for those attending in person and there will be a videochat link in the invitation for those attending virtually. I’m looking into recording the meetings and making them internally available for anyone who can’t make it, but I can’t guarantee I’ll have this sorted.\nPreparing for our first meeting: Before we meet I would like everyone to write down your answers to the following questions.\n\nAre you interested in the low-level details of deep learning or mostly high-level application?\nWhere do you see yourself using deep learning? In the lab or for personal use?\nAfter learning these topics what goals will you be better situated to achieve?\n\nCloser to the date of I will send out some instructions on the software we’ll be using. Please follow these setup instructions.\nOur first meeting will be composed of three parts. First, I’ll give a brief introduction. Next, we’ll talk about everyone’s answers to the questions above. The goal of this is to help people with similar uses in mind and interests find each other – so over time we can grow from a learning group to a community of practice. Finally, we’ll talk about software and setup to fix any issues for the next meeting.\nFuture meetings: Beyond this first meeting we’ll have some lecture and exercises that we’ll complete in between sessions. In the sessions we’ll talk though the exercises and what people found challenging or interesting.\nThe material will initially focus on building networks from the ground up so when we start using research grade libraries how they work isn’t a black box. After a few sessions we’ll take stock and adjust course as needed."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240501_PreMeeting/index.html#setup-instructions",
    "href": "posts/DeepLearningCommunityofPractice/240501_PreMeeting/index.html#setup-instructions",
    "title": "Summer Session: Pre-Meeting Preparation",
    "section": "Setup Instructions",
    "text": "Setup Instructions\nHello Everyone,\nHere are the setup instructions. It should take only a few minutes to get set up, but please go ahead and do this before our kickoff meeting next week. That way we can address any issues that arise.\nTo start out with we’ll use python through jupyter notebooks. If you’ve worked in the R ecosystem, these are similar to Rmarkdown or quarto documents that let you mix notes and equations in with your code. There are two options you can use. The easiest is to use “Google Collab” which will take care of the setup for you. The hardware your notebook will not be especially powerful but that is perfectly fine for starting out.\nOption 1: • Create a google account if you don’t have one. • Keep this url handy for next week https://colab.research.google.com/ Option 2: • Install Anaconda https://docs.anaconda.com/free/anaconda/install/ or miniconda https://docs.anaconda.com/free/miniconda/index.html • Using either the graphical interface or the command line https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html# create an environment and ensure that at least Jupiter is installed https://anaconda.org/anaconda/jupyter\nAssuming you opt for option 2 with the graphical user interface, select environments\n\nNext select create\n\nSelect python and choose a name for your environment. (Don’t worry about the version of python just now, we’ll change it when or if it becomes an issue)\n\nNow we need to install jupyter notebook. By default anaconda displays the installed packages so we’ll need to switch to all or uninstalled before searching for it. Find it, check the box and press apply below.\n\nYou may see a pop up requesting that you install notebook’s dependencies.\n\nAfter the installation if finished now you can right click on the play symbol and launch a jupyter notebook.\n\nThis should automatically launch in your default browser."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240520_Meeting1/index.html",
    "href": "posts/DeepLearningCommunityofPractice/240520_Meeting1/index.html",
    "title": "Summer Session: Meeting 1",
    "section": "",
    "text": "What are the key ideas you took from the video?\nDid you implement code from the video? How did it go?\nFor those who are beginning with python where there sticking points you encounter?\n\n“High level” class organization?\n“Low level” syntax?\nBoth?\nWould a python office hours next week be useful?\n\nHow long did you spend on the material?\n\nWas this too much for a two week period?\nWould video + take home challenge be a better pairing?"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240520_Meeting1/index.html#discussion",
    "href": "posts/DeepLearningCommunityofPractice/240520_Meeting1/index.html#discussion",
    "title": "Summer Session: Meeting 1",
    "section": "",
    "text": "What are the key ideas you took from the video?\nDid you implement code from the video? How did it go?\nFor those who are beginning with python where there sticking points you encounter?\n\n“High level” class organization?\n“Low level” syntax?\nBoth?\nWould a python office hours next week be useful?\n\nHow long did you spend on the material?\n\nWas this too much for a two week period?\nWould video + take home challenge be a better pairing?"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240520_Meeting1/index.html#key-ideas-from-main-session",
    "href": "posts/DeepLearningCommunityofPractice/240520_Meeting1/index.html#key-ideas-from-main-session",
    "title": "Summer Session: Meeting 1",
    "section": "Key Ideas From Main Session:",
    "text": "Key Ideas From Main Session:\n\nFunctions approximated by combining many non-linear functions.\nParameters of the functions nudged to decrease error.\n\n\n\n\nCredit Randall Munroe (“Machine Learning” XKCD)"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240520_Meeting1/index.html#key-ideas-from-homework",
    "href": "posts/DeepLearningCommunityofPractice/240520_Meeting1/index.html#key-ideas-from-homework",
    "title": "Summer Session: Meeting 1",
    "section": "Key Ideas From Homework",
    "text": "Key Ideas From Homework\n\nA computational graph allows us to track gradients through many operations\nGiven the derivative of each function in a graph we use backpropagation to determine derivative of each parameter with respect to the loss\nHaving the set of derivatives (the gradient) parameters can be nudged to reduce the loss"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240520_Meeting1/index.html#looking-forward",
    "href": "posts/DeepLearningCommunityofPractice/240520_Meeting1/index.html#looking-forward",
    "title": "Summer Session: Meeting 1",
    "section": "Looking Forward:",
    "text": "Looking Forward:\n\n\n\n\n\nflowchart TD\n    A[backpropagation]\n    B[Text encoding\\nand generation]\n    C[MLPs and logistics]\n    D[Advanced MLPs]\n    E[Advanced\\nbackpropagation]\n    F[Convolutional\\nnetworks]\n    G[Transformers]\n    \n    a(Learning Partners)\n    b(Learning Problem)\n    c(High Level Model)\n    or{or}\n    d(Low Level\\nModel)\n    e(Advanced\\nModel)\n    f(Advanced\\nInterpretation)\n    \n    A --&gt; B --&gt; C --&gt; D --&gt; E --&gt; F --&gt; G\n    a --&gt; b --&gt; c --&gt; or\n    or--&gt; d\n    or--&gt; e\n    or--&gt; f\n    \n    A --- a\n    D ~~~ d\n\n\n\n\n\n\n\nThe core mechanisms of deep learning apply across types of data and tasks.\n\nTasks (outputs)\nData types and shapes (inputs)"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240520_Meeting1/index.html#core-tasks",
    "href": "posts/DeepLearningCommunityofPractice/240520_Meeting1/index.html#core-tasks",
    "title": "Summer Session: Meeting 1",
    "section": "Core Tasks",
    "text": "Core Tasks\n\nClassification - Predict a discrete value (label)\nRegression - Predict a continuous value\nDensity - How similar are different populations of observations?"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240520_Meeting1/index.html#data-types",
    "href": "posts/DeepLearningCommunityofPractice/240520_Meeting1/index.html#data-types",
    "title": "Summer Session: Meeting 1",
    "section": "Data Types",
    "text": "Data Types\nReminder: Categories of data\n\n\n\n\nCategories\nRanked\nEvenly Spaced\nNatural 0\n\n\n\n\nNominal\nX\n\n\n\n\n\nOrdinal\nX\nX\n\n\n\n\nInterval\nX\nX\nX\n\n\n\nRatio\nX\nX\nX\nX\n\n\n\n\nThink about generalized linear models to get a non-linear response we pass a linear model into a link function. (How is this similar to a neuron?)\nTo flexibly use or predict different data types we change\n\nHow data is encoded (e.g. \\(Adenine \\rightarrow [1, 0, 0, 0]\\) )\nThe number of values output\nHow loss is calculated"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240520_Meeting1/index.html#data-shapes",
    "href": "posts/DeepLearningCommunityofPractice/240520_Meeting1/index.html#data-shapes",
    "title": "Summer Session: Meeting 1",
    "section": "Data Shapes",
    "text": "Data Shapes\nDimensions of data\n\n\n\nTensor Rank\nName\nExample\nExample Dims.*\n\n\n\n\n0\nScalar\nHeight\n\n\n\n1\nVector\nHeight for several obs.\nn\n\n\n2\nMatrix\nSequence on off\nn, c\n\n\n3\nCube\nPicture (bw)\nn, h, w\n\n\n\n\nSequence of nucleotides\nn, l, c\n\n\n4\n?\nPicture (rgb)\nn, c, h, w, l\n\n\n5\n?\nVideo (rgb)\nn, c, h, w, l\n\n\n\n*order changes based on conventions"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240520_Meeting1/index.html#for-next-meeting",
    "href": "posts/DeepLearningCommunityofPractice/240520_Meeting1/index.html#for-next-meeting",
    "title": "Summer Session: Meeting 1",
    "section": "For Next Meeting",
    "text": "For Next Meeting\n\nIf you are have not found a learning partner please reach out to at least one person who\n\nDoesn’t have a partner yet and\nhas a different programming comfort (the median is 3)\n\nThink about if there is a small dataset that you would enjoy working with\n\nRT-qPCR experiment?\nImages with a plant diseased/non?\nSequences of nucleotides for genes in two families?\n\nPlease watch at least the first 1h4m (strongly recommend the full 1h57) of Andrej Karpathy’s lecture here and follow along in your Jupyter Notebook."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240618_Meeting3/index.html",
    "href": "posts/DeepLearningCommunityofPractice/240618_Meeting3/index.html",
    "title": "Summer Session: Meeting 3",
    "section": "",
    "text": "This week we supplemented the lecture material with supplemental sides which can be found here. Specifically we discussed common considerations for defining training and testing splits (e.g. information leakage) and several common hyperparameter tuning libraries (with an example using Ax)"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240618_Meeting3/index.html#recap",
    "href": "posts/DeepLearningCommunityofPractice/240618_Meeting3/index.html#recap",
    "title": "Summer Session: Meeting 3",
    "section": "",
    "text": "This week we supplemented the lecture material with supplemental sides which can be found here. Specifically we discussed common considerations for defining training and testing splits (e.g. information leakage) and several common hyperparameter tuning libraries (with an example using Ax)"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240618_Meeting3/index.html#preparation-for-next-session",
    "href": "posts/DeepLearningCommunityofPractice/240618_Meeting3/index.html#preparation-for-next-session",
    "title": "Summer Session: Meeting 3",
    "section": "Preparation for Next Session",
    "text": "Preparation for Next Session\nBased on our conversation we will proceed with our current materials and I’ll aim to have an implementation in a high level library so we can see how the network might look if we weren’t working from scratch.\nThe lecture for next session is on the longer side (1h55m). If there are sections that one skip for the sake of time I’ll send an email noting the timestamps for these sections early next week.\nWe also discussed presentations of everyone’s learning projects. These will likely start towards the end of July or in August (or sooner if anyone is champing at the bit!). Don’t worry about presenting on a work in progress – this may be more helpful to your peers than seeing a fully polished model.\nCheers,\nDaniel"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240805_Meeting5/index.html",
    "href": "posts/DeepLearningCommunityofPractice/240805_Meeting5/index.html",
    "title": "Meeting 5",
    "section": "",
    "text": "This supplementary presentation from this week can be found here.\n\nLogistics\n\nWe’re running a new scheduling poll for the fall since we have members whose schedule may change with the new semester.\nBecause of this, our next meeting will be on the 20th and the one after that will be TBD.\n\nMLP in PyTorch\n\nWe discussed some of the logistics around building models in PyTorch including\nDataloaders\nMoving tensors to the GPU\nnn.Module classes"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240805_Meeting5/index.html#recap",
    "href": "posts/DeepLearningCommunityofPractice/240805_Meeting5/index.html#recap",
    "title": "Meeting 5",
    "section": "",
    "text": "This supplementary presentation from this week can be found here.\n\nLogistics\n\nWe’re running a new scheduling poll for the fall since we have members whose schedule may change with the new semester.\nBecause of this, our next meeting will be on the 20th and the one after that will be TBD.\n\nMLP in PyTorch\n\nWe discussed some of the logistics around building models in PyTorch including\nDataloaders\nMoving tensors to the GPU\nnn.Module classes"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240805_Meeting5/index.html#preparation-for-next-session",
    "href": "posts/DeepLearningCommunityofPractice/240805_Meeting5/index.html#preparation-for-next-session",
    "title": "Meeting 5",
    "section": "Preparation for Next Session",
    "text": "Preparation for Next Session\nThe content for the next two sessions will dovetail:\nWe’re going to look at transformers next so\n\nFor next session:\n\nWatch Grant Sanderson’s Attention in transformers (26m)\nBegin Let’s build GPT (1h56)\nWe’re planning to review Santurkar, et al. 2018 (“How Does Batch Normalization Help Optimization?”) for the folks that couldn’t be at Meeting 4. If you haven’t read it give it a read!\nDecide if you’re game to present a project or paper “show and tell” and what project you’re planning or working on that you.\n\nFor the session after that:\n\nFinish Let’s build GPT (1h56)\n\n\nCheers,\nDaniel"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240917_Fall_Preparation/index.html",
    "href": "posts/DeepLearningCommunityofPractice/240917_Fall_Preparation/index.html",
    "title": "Fall Session: Preparation and Schedule",
    "section": "",
    "text": "Edit: 9/26 Added to Schedule resources for\nEdit: 11/12 Updated schedule due to members conference/symposium and holiday travel."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240917_Fall_Preparation/index.html#welcome",
    "href": "posts/DeepLearningCommunityofPractice/240917_Fall_Preparation/index.html#welcome",
    "title": "Fall Session: Preparation and Schedule",
    "section": "Welcome!",
    "text": "Welcome!\nHello and welcome to our deep learning fall session! We are (re)iterating on the material we discussed over the summer so if this is the first time you’re joining us don’t worry about reading back through the older posts. If someone has directed you to this page and you’d like to be added to the mailing list, please get in touch with us."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240917_Fall_Preparation/index.html#what-to-expect",
    "href": "posts/DeepLearningCommunityofPractice/240917_Fall_Preparation/index.html#what-to-expect",
    "title": "Fall Session: Preparation and Schedule",
    "section": "What to Expect",
    "text": "What to Expect\nWe’ll be using python to go work from derivatives to generatively pretrained transformer models. Along the way we’ll talk about…\n\nWhat deep learning models do\nHow deep learning models are trained\nArchitectural decisions to improve performance\nTools used for model development and refinement\n\nAlong the way we’ll also talk a fair bit about python, the logistical challenges that surround model training, and some of the ways models can learn the wrong things.\nWe’re interested in building understanding and intuition so as to avoid cases where you only think your model is working well. We’re going to encourage twin prescriptions of skepticism and caution in working with models. Instead of “move fast and break things” think “move slow and build things”."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240917_Fall_Preparation/index.html#what-we-expect",
    "href": "posts/DeepLearningCommunityofPractice/240917_Fall_Preparation/index.html#what-we-expect",
    "title": "Fall Session: Preparation and Schedule",
    "section": "What We Expect",
    "text": "What We Expect\nWe recommend viewing this group not as a class but a convening of autodidactic learners. Initially we’ll have a presentations coving foundational topics which will give way to open discussions. Having a list of thoughts and questions will be valuable to make the most of these sessions.\nSince it’s really hard to train models on paper, we expect folks to be writing (and debugging) code. We recommend budgeting time for the videos and writing exploratory code on your own. It can be useful to write the code featured in the videos in parallel with them, which may substantially increase the amount of time the materi\nal requires (and one’s comfort and fluency with these topics). Futher, applying or extending what you have seen to small data sets is a great way to solidify understanding.\nFinally, we recommend identifying at least one peer that you can partner with. Explaining a new concept is a great way to solidify your understanding and having someone you can ask for a fresh perspective from is hugely useful. Although we’re interested in technical knowledge, we also want to increase connections between labs and the folks in them.\nIn summary:\n\nKeep an eye on the upcoming materials\nWrite code, make mistakes, debug code\nMake friends, ask for help, and teach others"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240917_Fall_Preparation/index.html#what-we-have-planned",
    "href": "posts/DeepLearningCommunityofPractice/240917_Fall_Preparation/index.html#what-we-have-planned",
    "title": "Fall Session: Preparation and Schedule",
    "section": "What We Have Planned",
    "text": "What We Have Planned\nBelow is our “working draft” for the fall. Based on feedback from the summer session we’re beginning with python so doesn’t add friction when we use it to build models. We may also make a few additions to these topics (such as working with images, sequences, or modeling distrbutions) or add presentations on folks planned or ongoing projects to talk about how these model go from the classroom to the lab.\n\n\n\n\n\nMonth\nDay\nBefore\nSession\nAfter\nDeep.Dive\n\n\n\n\nSept.\n17\n\n\n\n\n\n\nSept.\n24\nPre-Meeting Setup\nPython: Application\n\n\n\n\nOct.\n1\n\nPython: Fundamentals\n\n\n\n\nOct.\n8\n\nPython: Functions and Classes\n\n\n\n\nOct.\n15\n\nDeep Learning Overview\nTo Deliver You From the Preliminary Terrors, The Intuitive Notion of the Chain Rule, What is a Neural Network? 0h18 , Gradient descent 0h20 , Analyzing our neural network\n\n\n\nOct.\n22\nWhat is backpropagation really doing? 0h12 , Backpropagation calculus 0h10, Backpropagation 2h25\nDiscussion\n\n\n\n\nOct.\n29\nBackpropagation Cont.\nDiscussion\n\n\n\n\nNov.\n5\nBigram Language Model 1h57\nDiscussion\n\n\n\n\nNov.\n12\nBigram Model Cont.\nDiscussion\n\nMultilayer Perceptron Language Model 1h15\n\n\nNov.\n19\n\nNo Session\n\nJournal Club: BatchNorm Activations + BatchNorm 1h55\n\n\nNov.\n26\n\nNo Session (Thanksgiving week)\n\nBackpropagation Deep Dive 1h55\n\n\nDec.\n3\nBut what is a GTP 0h27, Visualizing Attention 0h26 , Generatively Pretrained Transformer 1h56\nDiscussion\nHow might LLMs store facts 0h22\nWaveNet 0h56\n\n\nDec.\n10\nTransformers Cont.\nDiscussion\n\n\n\n\nDec.\n17"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240917_Fall_Preparation/index.html#preparing-for-the-first-session",
    "href": "posts/DeepLearningCommunityofPractice/240917_Fall_Preparation/index.html#preparing-for-the-first-session",
    "title": "Fall Session: Preparation and Schedule",
    "section": "Preparing for the First Session:",
    "text": "Preparing for the First Session:\nTo get started you’ll want to have installed python and a way to edit jupyter notebooks1.\nFor Python I recommend installing Anaconda if you’re on Windows and micromamba if you’re on Linux. For help installing Anaconda, see this link.\nAlthough not strictly necessary I also recommend installing Visual Studio Code and Microsoft’s Live Share plugin. The former provides a nice editor for notebooks and the latter a way to collaboratively edit a code (think “google docs for code” or Teletype)."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/240917_Fall_Preparation/index.html#footnotes",
    "href": "posts/DeepLearningCommunityofPractice/240917_Fall_Preparation/index.html#footnotes",
    "title": "Fall Session: Preparation and Schedule",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese behave similarly to R markdown / Quarto files for those coming from the R ecosystem.↩︎"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/241001_Python_Fundamentals/index.html",
    "href": "posts/DeepLearningCommunityofPractice/241001_Python_Fundamentals/index.html",
    "title": "Python 2: Python Fundamentals",
    "section": "",
    "text": "The presentation slides can be found here.\nIf you do not have access to the video please contact Daniel."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/241001_Python_Fundamentals/index.html#materials",
    "href": "posts/DeepLearningCommunityofPractice/241001_Python_Fundamentals/index.html#materials",
    "title": "Python 2: Python Fundamentals",
    "section": "Materials",
    "text": "Materials\nThe files for today are provided here:\n\npython_fundamentals_worksheet.tar \npython_fundamentals.tar\n\nIf you can’t open the zipped files, download these files, changing the extension of the worksheet and solutions from .txt to .ipynb.\n\npython_fundamentals_worksheet.txt \npython_fundamentals.txt"
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/241022_Backprop/index.html",
    "href": "posts/DeepLearningCommunityofPractice/241022_Backprop/index.html",
    "title": "Deep Learning Discussion: Backpropagaton",
    "section": "",
    "text": "Next week will be a “catch-up” session since the backpropagation material is critical for understanding how networks are optimized and takes a fair bit of time to work through.\nTime permitting we may introduce the bigram model or dig deeper into dataloaders depending on the length of the discussion next week."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/241022_Backprop/index.html#for-next-week",
    "href": "posts/DeepLearningCommunityofPractice/241022_Backprop/index.html#for-next-week",
    "title": "Deep Learning Discussion: Backpropagaton",
    "section": "",
    "text": "Next week will be a “catch-up” session since the backpropagation material is critical for understanding how networks are optimized and takes a fair bit of time to work through.\nTime permitting we may introduce the bigram model or dig deeper into dataloaders depending on the length of the discussion next week."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/241022_Backprop/index.html#materials",
    "href": "posts/DeepLearningCommunityofPractice/241022_Backprop/index.html#materials",
    "title": "Deep Learning Discussion: Backpropagaton",
    "section": "Materials",
    "text": "Materials\n\nHere is the notebook I wrote during our meeting today (Demo Notebook) showing how how gradients are tracked in PyTorch and how to create a small multilayer perceptron (MLP)."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/241112_Bigram/index.html",
    "href": "posts/DeepLearningCommunityofPractice/241112_Bigram/index.html",
    "title": "Deep Learning Discussion: The Bigram Language Model",
    "section": "",
    "text": "Explore the Bigram Model – Try to improve on the Bigram model we wrote this week. How much is performance influenced by…\n\nOptimizer choice?\nLearning rate?\nNumber and size of hidden layers?\nBatch size?\nRandomness of training set\n…\n\nNext session we will be discussing attention and transformers. For next session:\n\nGet an introduction to transormers and attention with Grant Sanderson’s But what is a GTP 0h27 and Visualizing Attention 0h26.\nThen work through Generatively Pretrained Transformer 1h56 following along in a notebook.\nFinally (optionally) watch How might LLMs store facts 0h22."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/241112_Bigram/index.html#for-next-session",
    "href": "posts/DeepLearningCommunityofPractice/241112_Bigram/index.html#for-next-session",
    "title": "Deep Learning Discussion: The Bigram Language Model",
    "section": "",
    "text": "Explore the Bigram Model – Try to improve on the Bigram model we wrote this week. How much is performance influenced by…\n\nOptimizer choice?\nLearning rate?\nNumber and size of hidden layers?\nBatch size?\nRandomness of training set\n…\n\nNext session we will be discussing attention and transformers. For next session:\n\nGet an introduction to transormers and attention with Grant Sanderson’s But what is a GTP 0h27 and Visualizing Attention 0h26.\nThen work through Generatively Pretrained Transformer 1h56 following along in a notebook.\nFinally (optionally) watch How might LLMs store facts 0h22."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/241112_Bigram/index.html#materials",
    "href": "posts/DeepLearningCommunityofPractice/241112_Bigram/index.html#materials",
    "title": "Deep Learning Discussion: The Bigram Language Model",
    "section": "Materials",
    "text": "Materials\n\nHere is the notebook we finished writing today (Demo Notebook Part 2)."
  },
  {
    "objectID": "posts/DeepLearningCommunityofPractice/index.html",
    "href": "posts/DeepLearningCommunityofPractice/index.html",
    "title": "Deep Learning Community of Practice",
    "section": "",
    "text": "Spring Session: Preparation and Schedule\n\n\n\nSchedule\n\n\nDeep Learning\n\n\n\n\n\n\n\nDaniel Kick\n\n\nFeb 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning Discussion: The Bigram Language Model\n\n\n\nbeginner\n\n\ncode\n\n\nDeep Learning\n\n\n\n\n\n\n\nDaniel Kick\n\n\nNov 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning Discussion: The Bigram Language Model\n\n\n\nbeginner\n\n\ncode\n\n\nDeep Learning\n\n\n\n\n\n\n\nDaniel Kick\n\n\nNov 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning Discussion: Backpropagaton\n\n\n\nbeginner\n\n\ncode\n\n\nDeep Learning\n\n\n\n\n\n\n\nDaniel Kick\n\n\nOct 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython 2: Python Fundamentals\n\n\n\nPython\n\n\n\n\n\n\n\nDaniel Kick\n\n\nOct 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython 1: Data Visualization in Python\n\n\n\nPython\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFall Session: Preparation and Schedule\n\n\n\nSchedule\n\n\nDeep Learning\n\n\n\n\n\n\n\nDaniel Kick\n\n\nSep 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummer Session: Meeting 6 Preparation Notes\n\n\n\nbeginner\n\n\ncode\n\n\nDeep Learning\n\n\n\n\n\n\n\nDaniel Kick\n\n\nAug 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeeting 5\n\n\n\nbeginner\n\n\ncode\n\n\nDeep Learning\n\n\n\n\n\n\n\nDaniel Kick\n\n\nAug 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummer Session: Meeting 4\n\n\n\nbeginner\n\n\ncode\n\n\nDeep Learning\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJul 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummer Session: Meeting 3\n\n\n\nbeginner\n\n\ncode\n\n\nDeep Learning\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummer Session: Meeting 2\n\n\n\nbeginner\n\n\ncode\n\n\nDeep Learning\n\n\n\n\n\n\n\nDaniel Kick\n\n\nJun 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummer Session: Meeting 1\n\n\n\nbeginner\n\n\ncode\n\n\nDeep Learning\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummer Session: Kick Off Meeting\n\n\n\nbeginner\n\n\ncode\n\n\nDeep Learning\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummer Session: Kick Off Meeting\n\n\n\nbeginner\n\n\ncode\n\n\nDeep Learning\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummer Session: Pre-Meeting Preparation\n\n\n\nbeginner\n\n\ncode\n\n\nDeep Learning\n\n\n\n\n\n\n\nDaniel Kick\n\n\nMay 6, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "protocols/Drones/Pix4Dmapper_Stitch/index.html",
    "href": "protocols/Drones/Pix4Dmapper_Stitch/index.html",
    "title": "Stitching with Pix4Dmapper",
    "section": "",
    "text": "Pix4Dmapper is installed on the lambda2 in the dry lab (NOT the Linux lambda)\n\nPassword: █████████-███-███████-██████ (You put it in your password wallet right?😉)\n\nCheck what flights need to be stitched on the To Be Stitched List_2022: Teams &gt; UAV Missions &gt; Files\nLaunch Pix4Dmapper on desktop\nSelect New Project\n\nName file FlightDate(YYMMDD)_camera(m2pro/ANAFI)_FieldName_Pix4D\nCreate In: This PC &gt; Desktop &gt; Pix4D &gt; 202X Fields &gt; “Your Field Name” &gt; 1 Folder called FlightDate(YYMMDD)_drone(m2pro/RE-mx/ANAFI)_FieldName_Pix4D\n\nSelect Images, Click the Add Images… button, file explorer will open.\n\nNavigate to This PC &gt; wldata (Under Network locations) &gt; Field_Data_202X &gt; UAV_images_by_field_202X &gt; Select a Field &gt; Select a folder by Date (YYMMDD) and drone type (m2pro/ANAFI only)\nCtrl + A to select all of the JPGs in the folder, Click Open (images will be selected, green check will appear) then Click Next &gt;\n\nKeep default Image Properties (Pix4D uses the GPS info in drone images), Click Next &gt;\nKeep default settings of Select Output Coordinate System, Click Next &gt;\nProcessing Options Template will open, under Standard select 3D Maps, (Do not check the box next to Start Processing), Click Finish\nAfter the map loads, Import the GCPs\n\nSelect the Project tab\nSelect GCP/MTP Manager…, the GCP Manager Window will open\nIn the GCP Coordinate System click the Edit… button, the Select GCP Coordinate System window will open\n\nCheck the Advanced Coordinate Options box\nClick the From List… button under the Known Coordinate System [m] search bar, Coordinate System window will open\nSelect the following from the dropdown lists\n\nDatum: WGS 1984\nCoordinate System: WGS 84 (Top of list, look for the globe )\n\n\n\nIn the GCP/MTP Table click the Import GCPs… button, the Import Ground Control Points window will open\n\nCoordinates order: Longitude, Latitude, Altitude\nClick Browse…, navigate to This PC &gt; Desktop &gt; Pix4D &gt; 2022 Fields &gt; GCPs &gt; Select your field’s .txt file, Click Open, Click OK, Click OK. The GCPs will appear as blue crosses on the map\nSave Project!\n\n\nInitial Processing\n\nClick the Processing tab on the bottom left side of the window, check the box next to 1. Initial Processing (ensure other boxes are unchecked) )\nClick the Processing Options tab on the bottom left side of the window (gear shape icon), the Processing Options window will open, ensure Initial Processing step is clicked on\n\nClick the Matching tab, in the Matching Strategy header check the Use Geometrically Verified Matching box )\nClick the Calibration tab, under the Camera Optimization header use the Internal Parameters Optimization dropdown box and select All Prior, Click OK, window will close )\nAt the bottom of the screen Click the Start bottom to begin Initial Processing (This will take hours, just leave Pix4D running 😊) )\n\n\nMarking GCPs\n\nAfter initial processing switch to rayCloud view on the left side of the Pix4D Window, Click on the blue GCP marker and Images will open up on the right (I like to uncheck Cameras and Rays for a cleaner map) )\nYou can press hold down to move around the images and zoom in and out to find the GCP (they may not be in every image, that’s ok)\nClick on the center of the GCP to mark it, repeat on 8-15 images. Do this for each GCP.\n\nIn the Selection panel above the Image panel, Click the Apply button occasionally to update and mark the GCPs\nSelect the Process tab at the top of the window\n\nSelect Reoptimize (this will take about 10 minutes), a Warning will come up, Click OK\nGenerate a newQuality Report by Clicking the Process tab and Selecting Generate Quality Report (this will take about 15 minutes). Ensure there are green checks next to the 5 parameters in the Quality Check (if not, troubleshoot w/ help from the Pix4D website)\n\nSave Project!\n\nPoint Cloud and Mesh and DSM, Orthomosaic and Index\n\nCheck the box next to 2. Point Cloud and Mesh and 3. DSM, Orthomosaic and Index, in the processing window (be sure to uncheck the previous step so Pix4D doesn’t rerun and take even longer), Click Start and accept default Processing Options (this will take hours, leave Pix4D running 😊)\nSave Project!\nTo see your 3D map, in the Ray Cloud viewer check the box next to Point Graphs (let it load) and then check the box next to Triangle Meshes. Enjoy!\nUpdate the To Be Stitched List_202X on Teams &gt; UAV Missions &gt; Files\n\nOpen orthomosaic in QGIS.\n\nCreate a directory within the flight directory with the name of the flight directory plus “_QGIS.”\n\nOpen QGIS\nSave project in newly created QGIS directory with flight name plus “_QGIS”.\nGo to Layer -&gt; Add Layer -&gt; Add Raster Layer.\nFind the orthomosaic “.tif” file just created by Metashape. And add it.\nGo to Project -&gt; Properties. Select CRS from the side tab. Change the Coordinate Reference System to “WGS 84 / UTM zone 15N” Authority ID: “EPSG:32615” if needed.\nRotate the image using the controls on the bottom taskbar if needed. Rows should be vertical and ranges horizontal.\n\n\nImport reference grid.\n\nCopy the Reference_grid file from the main field directory to the QGIS directory.\nRename the copied file replacing the field name and year with the flight name. (e.g. “Reference_grid_210717_m2pro_Gen7.gpkg”\nGo to Layer -&gt; Add Layer -&gt; Add Vector Layer.\nSelect the new Reference_Grid file you just renamed and click Open. Then click Add. Then Close.\nRight Click on the newly added layer and go to Properties. Select Symbology in the left hand pane. Select Simple Fill. Change the Fill Color to Transparent Fill. Click ok.\nCheck that the grid lines up properly with the field data. Make any minor adjustments that may be needed."
  },
  {
    "objectID": "protocols/Drones/Pix4Dmapper_Stitch/index.html#instructions",
    "href": "protocols/Drones/Pix4Dmapper_Stitch/index.html#instructions",
    "title": "Stitching with Pix4Dmapper",
    "section": "",
    "text": "Pix4Dmapper is installed on the lambda2 in the dry lab (NOT the Linux lambda)\n\nPassword: █████████-███-███████-██████ (You put it in your password wallet right?😉)\n\nCheck what flights need to be stitched on the To Be Stitched List_2022: Teams &gt; UAV Missions &gt; Files\nLaunch Pix4Dmapper on desktop\nSelect New Project\n\nName file FlightDate(YYMMDD)_camera(m2pro/ANAFI)_FieldName_Pix4D\nCreate In: This PC &gt; Desktop &gt; Pix4D &gt; 202X Fields &gt; “Your Field Name” &gt; 1 Folder called FlightDate(YYMMDD)_drone(m2pro/RE-mx/ANAFI)_FieldName_Pix4D\n\nSelect Images, Click the Add Images… button, file explorer will open.\n\nNavigate to This PC &gt; wldata (Under Network locations) &gt; Field_Data_202X &gt; UAV_images_by_field_202X &gt; Select a Field &gt; Select a folder by Date (YYMMDD) and drone type (m2pro/ANAFI only)\nCtrl + A to select all of the JPGs in the folder, Click Open (images will be selected, green check will appear) then Click Next &gt;\n\nKeep default Image Properties (Pix4D uses the GPS info in drone images), Click Next &gt;\nKeep default settings of Select Output Coordinate System, Click Next &gt;\nProcessing Options Template will open, under Standard select 3D Maps, (Do not check the box next to Start Processing), Click Finish\nAfter the map loads, Import the GCPs\n\nSelect the Project tab\nSelect GCP/MTP Manager…, the GCP Manager Window will open\nIn the GCP Coordinate System click the Edit… button, the Select GCP Coordinate System window will open\n\nCheck the Advanced Coordinate Options box\nClick the From List… button under the Known Coordinate System [m] search bar, Coordinate System window will open\nSelect the following from the dropdown lists\n\nDatum: WGS 1984\nCoordinate System: WGS 84 (Top of list, look for the globe )\n\n\n\nIn the GCP/MTP Table click the Import GCPs… button, the Import Ground Control Points window will open\n\nCoordinates order: Longitude, Latitude, Altitude\nClick Browse…, navigate to This PC &gt; Desktop &gt; Pix4D &gt; 2022 Fields &gt; GCPs &gt; Select your field’s .txt file, Click Open, Click OK, Click OK. The GCPs will appear as blue crosses on the map\nSave Project!\n\n\nInitial Processing\n\nClick the Processing tab on the bottom left side of the window, check the box next to 1. Initial Processing (ensure other boxes are unchecked) )\nClick the Processing Options tab on the bottom left side of the window (gear shape icon), the Processing Options window will open, ensure Initial Processing step is clicked on\n\nClick the Matching tab, in the Matching Strategy header check the Use Geometrically Verified Matching box )\nClick the Calibration tab, under the Camera Optimization header use the Internal Parameters Optimization dropdown box and select All Prior, Click OK, window will close )\nAt the bottom of the screen Click the Start bottom to begin Initial Processing (This will take hours, just leave Pix4D running 😊) )\n\n\nMarking GCPs\n\nAfter initial processing switch to rayCloud view on the left side of the Pix4D Window, Click on the blue GCP marker and Images will open up on the right (I like to uncheck Cameras and Rays for a cleaner map) )\nYou can press hold down to move around the images and zoom in and out to find the GCP (they may not be in every image, that’s ok)\nClick on the center of the GCP to mark it, repeat on 8-15 images. Do this for each GCP.\n\nIn the Selection panel above the Image panel, Click the Apply button occasionally to update and mark the GCPs\nSelect the Process tab at the top of the window\n\nSelect Reoptimize (this will take about 10 minutes), a Warning will come up, Click OK\nGenerate a newQuality Report by Clicking the Process tab and Selecting Generate Quality Report (this will take about 15 minutes). Ensure there are green checks next to the 5 parameters in the Quality Check (if not, troubleshoot w/ help from the Pix4D website)\n\nSave Project!\n\nPoint Cloud and Mesh and DSM, Orthomosaic and Index\n\nCheck the box next to 2. Point Cloud and Mesh and 3. DSM, Orthomosaic and Index, in the processing window (be sure to uncheck the previous step so Pix4D doesn’t rerun and take even longer), Click Start and accept default Processing Options (this will take hours, leave Pix4D running 😊)\nSave Project!\nTo see your 3D map, in the Ray Cloud viewer check the box next to Point Graphs (let it load) and then check the box next to Triangle Meshes. Enjoy!\nUpdate the To Be Stitched List_202X on Teams &gt; UAV Missions &gt; Files\n\nOpen orthomosaic in QGIS.\n\nCreate a directory within the flight directory with the name of the flight directory plus “_QGIS.”\n\nOpen QGIS\nSave project in newly created QGIS directory with flight name plus “_QGIS”.\nGo to Layer -&gt; Add Layer -&gt; Add Raster Layer.\nFind the orthomosaic “.tif” file just created by Metashape. And add it.\nGo to Project -&gt; Properties. Select CRS from the side tab. Change the Coordinate Reference System to “WGS 84 / UTM zone 15N” Authority ID: “EPSG:32615” if needed.\nRotate the image using the controls on the bottom taskbar if needed. Rows should be vertical and ranges horizontal.\n\n\nImport reference grid.\n\nCopy the Reference_grid file from the main field directory to the QGIS directory.\nRename the copied file replacing the field name and year with the flight name. (e.g. “Reference_grid_210717_m2pro_Gen7.gpkg”\nGo to Layer -&gt; Add Layer -&gt; Add Vector Layer.\nSelect the new Reference_Grid file you just renamed and click Open. Then click Add. Then Close.\nRight Click on the newly added layer and go to Properties. Select Symbology in the left hand pane. Select Simple Fill. Change the Fill Color to Transparent Fill. Click ok.\nCheck that the grid lines up properly with the field data. Make any minor adjustments that may be needed."
  },
  {
    "objectID": "protocols/Drones/QGIS_First_Grid/index.html",
    "href": "protocols/Drones/QGIS_First_Grid/index.html",
    "title": "Gridding the Reference Flight",
    "section": "",
    "text": "Open QGIS and open a “New Project”\nGo to the “Layer” tab at the top, then “Add Raster Layer” - where it says browse, go to the Pix4D folder for the date and field being gridded and find the orthomosaic\nWhen the orthomosaic shows up on the screen, go to the “Rotation” bar in the bottom right corner and change the rotation angle until the orthomosaic is straight (As straight as it can be, doesn’t have to be perfect)\nTo create the origin point where the grid will start from:\n\nNavigate to the “Layer” tab at the top of toolbar then “Create Layer” then “New Shapefile Layer”\nSave with the name “origin”\nClick the “Toggle Editing” and then “Add Feature” - make the point in the lower left corner above the GCP in between the plants in the first experimental plot\nWhen the labeling window comes up, name the point “1”\n\nTo get to the processing toolbox:\n\nGo to “Plugins” then “Manage and Install Plugins”\nRe-check the box for “Processing”\nClose the tab and click on the “Setting” icon that looks like a gear and the toolbox will appear\n\nClick on the “R” option and in the drop down menu click “Draw Plots from points” - a window should appear with all of the options for inputting the range and row measurements. To create the grid:\n\nBased on information from the field map given, enter the number of ranges in the box for “Number of ranges per block” and enter the number of rows in the box for “Number of rows per block”\nIn the angle of rotation, enter the opposite (+/-) number of the rotation the orthomosaic is set at, which can be seen in the bottom right corner of the screen\nMake the measurement unit for plot size “feet” and the ID format “serpentine”\nStart the numbering plots should be “bottom left”\nThe “Full plot height” and “Data plot height should be the length of each plot in feet and the full plot width and data plot width should be the width of each plot in feet\nSet the Field containing ID to “123 id” (a common error with the script happens when this setting isn’t correct)\n\nGo to the log tab at the top of the window to see if the script runs successfully\nWhen the script is finished running, the grid will generate on top of the orthomosaic with the boxes colored in. To make the boxes transparent:\n\nClick on the layer in the “Layers” Menu on the left side of the screen\nIn the dropdown menu, go to “Properties” then “Symbology” on the left side of the window\nChange the fill to the red outline\nClick on the “Simple Line” below “Fill” and change the line width to “Hairline”\nClick “Ok”\n\nGo through each row of the grid checking that each box contains one plot - the plots on the end of each row and range don’t need a box because they’re border - so when the grid is used for future flights the plots will, for the most part, already be lined up (This step will probably take the longest)"
  },
  {
    "objectID": "protocols/Drones/QGIS_First_Grid/index.html#instructions",
    "href": "protocols/Drones/QGIS_First_Grid/index.html#instructions",
    "title": "Gridding the Reference Flight",
    "section": "",
    "text": "Open QGIS and open a “New Project”\nGo to the “Layer” tab at the top, then “Add Raster Layer” - where it says browse, go to the Pix4D folder for the date and field being gridded and find the orthomosaic\nWhen the orthomosaic shows up on the screen, go to the “Rotation” bar in the bottom right corner and change the rotation angle until the orthomosaic is straight (As straight as it can be, doesn’t have to be perfect)\nTo create the origin point where the grid will start from:\n\nNavigate to the “Layer” tab at the top of toolbar then “Create Layer” then “New Shapefile Layer”\nSave with the name “origin”\nClick the “Toggle Editing” and then “Add Feature” - make the point in the lower left corner above the GCP in between the plants in the first experimental plot\nWhen the labeling window comes up, name the point “1”\n\nTo get to the processing toolbox:\n\nGo to “Plugins” then “Manage and Install Plugins”\nRe-check the box for “Processing”\nClose the tab and click on the “Setting” icon that looks like a gear and the toolbox will appear\n\nClick on the “R” option and in the drop down menu click “Draw Plots from points” - a window should appear with all of the options for inputting the range and row measurements. To create the grid:\n\nBased on information from the field map given, enter the number of ranges in the box for “Number of ranges per block” and enter the number of rows in the box for “Number of rows per block”\nIn the angle of rotation, enter the opposite (+/-) number of the rotation the orthomosaic is set at, which can be seen in the bottom right corner of the screen\nMake the measurement unit for plot size “feet” and the ID format “serpentine”\nStart the numbering plots should be “bottom left”\nThe “Full plot height” and “Data plot height should be the length of each plot in feet and the full plot width and data plot width should be the width of each plot in feet\nSet the Field containing ID to “123 id” (a common error with the script happens when this setting isn’t correct)\n\nGo to the log tab at the top of the window to see if the script runs successfully\nWhen the script is finished running, the grid will generate on top of the orthomosaic with the boxes colored in. To make the boxes transparent:\n\nClick on the layer in the “Layers” Menu on the left side of the screen\nIn the dropdown menu, go to “Properties” then “Symbology” on the left side of the window\nChange the fill to the red outline\nClick on the “Simple Line” below “Fill” and change the line width to “Hairline”\nClick “Ok”\n\nGo through each row of the grid checking that each box contains one plot - the plots on the end of each row and range don’t need a box because they’re border - so when the grid is used for future flights the plots will, for the most part, already be lined up (This step will probably take the longest)"
  },
  {
    "objectID": "protocols/DryLab/Rovers/RoverUseInField/index.html",
    "href": "protocols/DryLab/Rovers/RoverUseInField/index.html",
    "title": "Operating Rovers in the Field",
    "section": "",
    "text": "How to Start a Mission in the Field\nBattery Installation\n\nWARNING: LiPo batteries are extremely dangerous! Before using LiPo batteries, make sure you have read the Battery procedures SOP and been trained.\nRemove the Rover from its travel case.\nOpen the compartment on the top of the rover by sliding it over.\nThere is a wire connector inside of the battery bay, as well as a digital battery monitor.\nConnect the battery to the battery monitor and review what is displayed on the digital reader\nYou should plug the four prongs into the far left side of the digital monitor (there are 4 holes)\n\n\n\nIt should display a total combined voltage of ~16.8V and display the voltage contained in each cell, ~4.2V each.\nIf any of the cells are below there is a difference of greater than\n\n0.3 between cells, do not use this battery and instead use a different one.\n\nPlace the messed-up battery in its fireproof case, log it in your notebook, and report it to Harper or Jacob.\nThe Rover contains a silver button on its aft side. Press this button to boot it up.\nIt will take 5-10 minutes for the rover to start up; when it is nearly started, you will hear a fan turn on inside of it.\nWatch the tablet loading screen; it should display a rover symbol in the upper right hand of the screen when she is ready. If you do not see this or hear the fan turn on, reboot the Rover by pressing the silver button.\n\nBeginning Mission\n1. As the rover is powering up, record your name, date and time, and battery number in the notebook.\n\nManually drive the rover to the beginning point of the mission (first or last row)\nWhen you are at the starting point, go to mission on the tablet and select the field you will be driving through.\nWhen you are ready, press the record button and select the correct row and column you will be traveling down.\nThe rover will automatically begin driving on its own; attempt to use autopilot as much as possible (this may be difficult when there are gaps or passing through rows as it tends to veer off on its own)\nWhen not using autopilot, use the sliders to control the direction and speed of rover (it is not a race, drive it slowly).\nContinue down the row until you come into contact with the orange flag at the end. At this point, press the record button again to collect the data from that row by selecting the same row, but different column.\nJump to the next row by driving manually (skipping a “lane”); there should not be a flag where you begin.\nPress the record button again and select the correct row and column (new row, same column) and repeat 4 down\n\n• Some times mislabeling of recordings occurs. If this happens, proceed as normal, but write it down in the notebook to make the correction during data upload back at the lab.\n• Any issues you have with the Rover take note of it.\n• You will likely have to change the battery during the mission. You will know you need to change the battery when you hear a loud beeping noise. When the noise goes off, finish the row you are on and power down Natasha at the end of the row (after you collect/save the recorded data).\no Replace the battery with a new one; place used battery in fireproof bag. Reboot Natasha (takes 5-10 minutes), continue mission\no Log where you stopped and started again in the log book\n• Occasionally the wifi between Natasha and the tablet will disconnect and you cannot control her. Wait 2 minutes to see if she reconnects – this is faster than a reboot. Simply turn her off and reboot her. Record this in the log; you will likely need to redo the row.\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "protocols/DryLab/Rovers/RoverUseInField/index.html#instructions",
    "href": "protocols/DryLab/Rovers/RoverUseInField/index.html#instructions",
    "title": "Operating Rovers in the Field",
    "section": "",
    "text": "How to Start a Mission in the Field\nBattery Installation\n\nWARNING: LiPo batteries are extremely dangerous! Before using LiPo batteries, make sure you have read the Battery procedures SOP and been trained.\nRemove the Rover from its travel case.\nOpen the compartment on the top of the rover by sliding it over.\nThere is a wire connector inside of the battery bay, as well as a digital battery monitor.\nConnect the battery to the battery monitor and review what is displayed on the digital reader\nYou should plug the four prongs into the far left side of the digital monitor (there are 4 holes)\n\n\n\nIt should display a total combined voltage of ~16.8V and display the voltage contained in each cell, ~4.2V each.\nIf any of the cells are below there is a difference of greater than\n\n0.3 between cells, do not use this battery and instead use a different one.\n\nPlace the messed-up battery in its fireproof case, log it in your notebook, and report it to Harper or Jacob.\nThe Rover contains a silver button on its aft side. Press this button to boot it up.\nIt will take 5-10 minutes for the rover to start up; when it is nearly started, you will hear a fan turn on inside of it.\nWatch the tablet loading screen; it should display a rover symbol in the upper right hand of the screen when she is ready. If you do not see this or hear the fan turn on, reboot the Rover by pressing the silver button.\n\nBeginning Mission\n1. As the rover is powering up, record your name, date and time, and battery number in the notebook.\n\nManually drive the rover to the beginning point of the mission (first or last row)\nWhen you are at the starting point, go to mission on the tablet and select the field you will be driving through.\nWhen you are ready, press the record button and select the correct row and column you will be traveling down.\nThe rover will automatically begin driving on its own; attempt to use autopilot as much as possible (this may be difficult when there are gaps or passing through rows as it tends to veer off on its own)\nWhen not using autopilot, use the sliders to control the direction and speed of rover (it is not a race, drive it slowly).\nContinue down the row until you come into contact with the orange flag at the end. At this point, press the record button again to collect the data from that row by selecting the same row, but different column.\nJump to the next row by driving manually (skipping a “lane”); there should not be a flag where you begin.\nPress the record button again and select the correct row and column (new row, same column) and repeat 4 down\n\n• Some times mislabeling of recordings occurs. If this happens, proceed as normal, but write it down in the notebook to make the correction during data upload back at the lab.\n• Any issues you have with the Rover take note of it.\n• You will likely have to change the battery during the mission. You will know you need to change the battery when you hear a loud beeping noise. When the noise goes off, finish the row you are on and power down Natasha at the end of the row (after you collect/save the recorded data).\no Replace the battery with a new one; place used battery in fireproof bag. Reboot Natasha (takes 5-10 minutes), continue mission\no Log where you stopped and started again in the log book\n• Occasionally the wifi between Natasha and the tablet will disconnect and you cannot control her. Wait 2 minutes to see if she reconnects – this is faster than a reboot. Simply turn her off and reboot her. Record this in the log; you will likely need to redo the row.\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "protocols/Logistics/Container_Singularity_PortForward/index.html",
    "href": "protocols/Logistics/Container_Singularity_PortForward/index.html",
    "title": "Testing (and using) your Singularity Container with Port Forwarding",
    "section": "",
    "text": "Note: this how to guide assumes that you have a working container with jupyter installed. See this page for details on setting up the gpu.sif container used here.\nThere are two steps to access jupyter remotely. First, start jupyter in the container. This can be done like so\nsingularity exec --nv gpu.sif jupyter notebook --no-browser --port=8887\nor can be started within a shell in the container if desired.\nuser.name$ singularity shell --nv gpu.sif\nSingularity&gt; jupyter notebook --no-browser --port=8887\nNote that here the --nv flag is not needed if gpu access is not desired. Further we’re specifying a port to be used for the notebook.\nIf the above is being run on a remote machine that allows port forwarding then we can open a ssh session from the local machine to connect the remote port (8887) to a local port. Here we use the same port number but if that one is busy another could be used.\nssh -N -f -Y -L 8887:localhost:8887 labmember\\@10.206.28.81\nAfter this session is active, then you can interact with jupyter as you normally would."
  },
  {
    "objectID": "protocols/Logistics/Do_Nothing_Scripting/index.html",
    "href": "protocols/Logistics/Do_Nothing_Scripting/index.html",
    "title": "Working Towards Automation: Do Nothing Scripting",
    "section": "",
    "text": "“Do Nothing Scripting” is a great practice. In brief the idea is to begin with a script that prints the instructions for a task but does not execute the instructions. Rahter it waits until the user presses a key or otherwise confirms that the task has been completed. Think of it like a check list – it contains the instructions and will display them as you complete them.\nIn our lab the main languages we use are python, R, and bash. In each of these languages the following functions would suspend the script until the user is ready to proceed.\n# python\ndef wait_for_enter():\n    input(\"Press a key to continue.\")\n    \nwait_for_enter()\n# R\nwait_for_enter &lt;- function(){\n  readline(prompt = \"Press a key to continue.\")\n}\n\nwait_for_enter()\n# bash\nwait_for_enter(){\n echo \"Press a key to continue.\"\n read -n 1 INVALUE\n}\n\nwait_for_enter\nDo nothing scripting can allow for steps that may change to be dynamically updated as in this example of in the rootbot sorting script. The jupyter notebook prints out instructions for the user to complete externally.\nprint(\"\"\"Instructions:\n1. Go to \"\"\"+path_base+'Experiments/'+experiment+\"\"\"\n2. Run rsync -azv --files-from=./send_files.txt ./ ../dest/\n   or if remote -azv --files-from:=./send_files.txt ./ ../dest/\n\"\"\")\nIt also allows for steps in that process to be automated over time. A step can begin as a list of instructions and ultimately be replaced by a function or external script."
  },
  {
    "objectID": "protocols/Logistics/HPC_Moving_Data/index.html",
    "href": "protocols/Logistics/HPC_Moving_Data/index.html",
    "title": "Moving Data from Computer to Computer",
    "section": "",
    "text": "Globus is the recommended way to move data from your local computer to the cluster. It is a service run from uchicago and allows you to move data using a web browser. Refer to SciNet’s install instructions.\nOnce installed you’ll need to link any remote storage. Atlas’ collection name is msuhpc2#Atlas-dtn. Linking it will require logging in with  with your Ceres password. \n\nOnce linked, you’ll be able to move data between linked drives."
  },
  {
    "objectID": "protocols/Logistics/HPC_Moving_Data/index.html#globus",
    "href": "protocols/Logistics/HPC_Moving_Data/index.html#globus",
    "title": "Moving Data from Computer to Computer",
    "section": "",
    "text": "Globus is the recommended way to move data from your local computer to the cluster. It is a service run from uchicago and allows you to move data using a web browser. Refer to SciNet’s install instructions.\nOnce installed you’ll need to link any remote storage. Atlas’ collection name is msuhpc2#Atlas-dtn. Linking it will require logging in with  with your Ceres password. \n\nOnce linked, you’ll be able to move data between linked drives."
  },
  {
    "objectID": "protocols/Logistics/HPC_Moving_Data/index.html#rsync-and-scp",
    "href": "protocols/Logistics/HPC_Moving_Data/index.html#rsync-and-scp",
    "title": "Moving Data from Computer to Computer",
    "section": "Rsync and scp",
    "text": "Rsync and scp\nCommand line tools like rsync and scp can be used to move files between workstations in the lab or to a cluster. To move data to or from Atlas one would use the following and then provide your password and authentication code.\nscp ./file &lt;SCINet UserID&gt;@Atlas-dtn.hpc.msstate.edu:/path/to/destination\nNote that the computer you’re connecting to is Atlas-dtn.hpc.msstate.edu instead of Atlas-Login.hpc.msstate.edu. This will access a data transfer node leaving the login nodes free for others to use.\nIf you need to move a directory, use the recursive flag -r or use rsync. With the latter this might look like:\nrsync -azv --progress ./file &lt;SCINet UserID&gt;@Atlas-dtn.hpc.msstate.edu:/path/to/destination\nThe options here ensure the file permissions are maintained (-a archive), files are zipped before transfer (-z zip), information is written to standard output (-v verbose), and transfer progress is provided (--progress)."
  },
  {
    "objectID": "protocols/Logistics/HPC_Singularity_and_Open_OnDemand/index.html",
    "href": "protocols/Logistics/HPC_Singularity_and_Open_OnDemand/index.html",
    "title": "Using your Singularity Container with Open OnDemand",
    "section": "",
    "text": "In this how-to we’ll make a singularity container accessible through Open OnDemand on Atlas. This allows for code on the HPC to be run in the same environment used for development locally. To do this, we’ll specify a new jupyter kernel that Open OnDemand will connect to."
  },
  {
    "objectID": "protocols/Logistics/HPC_Singularity_and_Open_OnDemand/index.html#setup",
    "href": "protocols/Logistics/HPC_Singularity_and_Open_OnDemand/index.html#setup",
    "title": "Using your Singularity Container with Open OnDemand",
    "section": "Setup",
    "text": "Setup\nThis document assumes you have a pre-built Singularity container (.sif) on Atlas. Using a custom container is recommended, but there is at least one container on Atlas that could be used (e.g. /reference/containers/jupyter-tensorflow/jupyter-tensorflow-2.2.0.sif). Note that your container should include Jupyter.\n\nCreate a symbolic link between your home directory and the project data directory.\n\ncd ~ \nln -s /projects/the_labs_project/your_directory\n\nCreate a directory to hold your container(s)\n\nmkdir ipython_containers\n\nUse Globus to transfer your container to this folder. In this example, my container is called tensorflow-21.07-tf2-py3.sif.\nSetup a kernel for your container. This will create configuration files that we will customize. They will be in a folder in .local/share/jupyter/kernels/.\n\nmodule load python\npython3 -m ipykernel install --user --name singularity-tf2py3 --display-name \"Python 3 Tensorflow 2\"\n\nCustomize kernel configuration. Open the new kernel configuration json in in your preferred text editor.\n\nvim .local/share/jupyter/kernels/singularity-tf2py3/kernel.json\n\nEdit the existing file so that it looks like the below. Be sure to edit the container path so that it has your container and username and the display_name so that your container is recognizable.\n\n\n{\n \"argv\": [\n  \"/apps/singularity-3/singularity-ce-3.11.0/bin/singularity\",\n  \"exec\",\n  \"--nv\",\n  \"-B\",\n  \"/project\",\n  \"-B\",\n  \"/90daydata\",\n  \"-B\",\n  \"/reference\",\n  \"-B\",\n  \"/local/scratch\",\n  \"/home/user.name/ipykernel_containers/tensorflow-21.07-tf2-py3.sif\",\n  \"/usr/bin/python3\",\n  \"-m\",\n  \"ipykernel\",\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"interrupt_mode\": \"message\",\n \"display_name\": \"Python 3 Tensorflow 2 Singularity\",\n \"language\": \"python\"\n}"
  },
  {
    "objectID": "protocols/Logistics/HPC_Singularity_and_Open_OnDemand/index.html#usage",
    "href": "protocols/Logistics/HPC_Singularity_and_Open_OnDemand/index.html#usage",
    "title": "Using your Singularity Container with Open OnDemand",
    "section": "Usage",
    "text": "Usage\n\nLog into Open OnDemand and begin a Jupyter session.\nSelect “Kernel” and “Change kernel”. Select your newly defined kernel.\n\nOnce the kernel starts all your code will be executed in the container!"
  },
  {
    "objectID": "protocols/Logistics/HPC_Singularity_and_Open_OnDemand/index.html#other-considerations",
    "href": "protocols/Logistics/HPC_Singularity_and_Open_OnDemand/index.html#other-considerations",
    "title": "Using your Singularity Container with Open OnDemand",
    "section": "Other Considerations",
    "text": "Other Considerations\nSome containers (see below) use a python other than /usr/bin/python3 as the default python. This will cause ipykernel to not be found when executed. The solution is to check the location of the default python in the container like so:\nsingularity exec container.sif which python\nNote the location and correct the path in kernel.json. In the case of the container created with the below .def file the path is /opt/conda/bin/python.\nBootstrap: docker\nFrom: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime\n\n%post\n    apt-get update\n    apt-get upgrade -y\n    ldconfig\n    apt-get install -y python3-pip\n    pip3 install --upgrade pip\n    echo 'ipykernel==6.15.1\nipython==8.4.0\nipython-genutils==0.2.0\nipywidgets==7.7.1\njupyter==1.0.0\njupyter-client==7.3.4\njupyter-console==6.4.4\njupyter-core==4.11.1\njupyter-server==1.18.1\njupyterlab==3.4.4\njupyterlab-pygments==0.2.2\njupyterlab-server==2.15.0\njupyterlab-widgets==1.1.1\nnbclassic==0.4.3\nnbclient==0.6.6\nnbconvert==6.5.0\nnbformat==5.4.0\nnotebook==6.4.12' &gt; requirements.txt\n    pip3 install -r requirements.txt"
  },
  {
    "objectID": "protocols/Logistics/HPC_Singularity_and_Open_OnDemand/index.html#references",
    "href": "protocols/Logistics/HPC_Singularity_and_Open_OnDemand/index.html#references",
    "title": "Using your Singularity Container with Open OnDemand",
    "section": "References",
    "text": "References\nThis was inspired by this MSI article and done with the help of James Huston Rogers at MSU. For cases where port forwarding is allowed (remote machines and some HPCs ) that will easier."
  },
  {
    "objectID": "protocols/Operations/Ops_QRCodeGen/index.html",
    "href": "protocols/Operations/Ops_QRCodeGen/index.html",
    "title": "QR Code Generation",
    "section": "",
    "text": "QR Codes for Easier Record Keeping\nFor certain tasks in the lab we use QR code to label plants, samples, or other materials. Generating these can take a bit of doing, so we have a few simple scripts to do this. Ideally, we want to make our labels both machine and human readable so we will produce labels with text alongside the QR code.\n\nlibrary(readxl)\nlibrary(qrcode)\nlibrary(tidyverse)\n\nmk_qr_label &lt;- function(\n    plot = 1,\n    field = 1,\n    pedigree = 'CM48',\n    max_len = 12,\n    output_dir = './output/'\n){\n  # clean inputs\n  plot     &lt;- as.character(plot)\n  field    &lt;- as.character(field)\n  pedigree &lt;- as.character(pedigree)\n  \n  # info to be embedded\n  label &lt;- paste(c(field, '_', pedigree), collapse='')\n  \n  # Set up save name\n  savename  &lt;- paste(plot, field, stringr::str_replace_all(label,  '/', '\\\\'), \n                     sep = '_')\n  save_path &lt;- paste(output_dir, savename, \".svg\", sep = '')\n  \n  # create qr code\n  code  &lt;- qr_code(label)\n  \n  # reshape for ggplot\n  code_long &lt;- pivot_longer(mutate(\n    X = seq(1, nrow(as.data.frame(code))),\n    as.data.frame(code)), \n    cols = starts_with('V')) %&gt;% \n    mutate(Y = -1*as.numeric(str_remove(name, 'V')))\n  \n  # create text for ggplot\n  align_label &lt;- paste(\n    c(plot, \n      paste0(rep(' ', max(c(0, (max_len - stringr::str_length(pedigree)))))),\n      field, \n      paste0(rep(' ', max(c(0, (max_len - stringr::str_length(field)))))),\n      pedigree), collapse = '')\n  \n  # make plot\n  plt &lt;- ggplot(code_long)+\n    geom_tile(aes(X, Y, fill = value))+\n    annotate(geom = \"text\", \n             x = min(filter(code_long, value)$X), \n             y = -1, \n             label = align_label,\n             hjust = 0, \n             size = 11\n    )+\n    scale_fill_manual(values = c('white', 'black'))+\n    theme_void()+\n    theme(legend.position = '')+\n    coord_equal()\n  \n  return(list('qr'=plt, 'path'=save_path))  \n}\n\nUse of this function is straight forward. It accepts numeric or text data for the plot, field, and pedigree of a plant. To ensure these attributes are positioned in a way that is easy to read, max_len adds spaces after each (up to the number specified). This results in text that is separated cleanly. The function returns a plot of the QR and the expected location for it to be saved. This will enable future extensions such as writing a batch of qr codes to a word document with officer for easy printing.\n\noutput_list &lt;- mk_qr_label(\n    plot       = 1,\n    field      = 1,\n    pedigree   = 'CM48',\n    max_len    = 12,\n    output_dir = './output/'\n)\n\nprint(output_list$path)\n\n[1] \"./output/1_1_1_CM48.svg\"\n\noutput_list$qr\n\n\n\n\n\n\n\n\nUse for batch processing is simple. We need to read in a spreadsheet with the label information and ensure that there are no missing values.Then we can loop over the entries and pass the output of mk_qr_label into ggsave and it will save them to the output directory.\n\nlabels &lt;- read_excel('./Grain_Labels_Demo.xlsx')\n\n# overwrite key fields with character version and add in missing labels for NAs\nwalk(c(\"Plot\", \"Field\", \"Pedigree\"), function(e){\n  labels[[e]] &lt;&lt;- as.character(labels[[e]])\n  labels[is.na(labels[[e]]), e] &lt;&lt;- 'MISSING'\n})\n\nfor(i in seq(1, nrow(labels))){\n  output_list &lt;- mk_qr_label(\n    plot       = labels[i, 'Plot'],\n    field      = labels[i, 'Field'],\n    pedigree   = labels[i, 'Pedigree'],\n    max_len    = 12,\n    output_dir = './output/'\n  )\n  ggsave(filename = output_list$path,\n         plot     = output_list$qr\n  )\n}"
  },
  {
    "objectID": "protocols/Rootbot/Restarting_Image_Script_On_Pi/index.html",
    "href": "protocols/Rootbot/Restarting_Image_Script_On_Pi/index.html",
    "title": "Restarting the Rootbot Imaging Script over SSH",
    "section": "",
    "text": "There’s an unresolved memory leak in the imaging script on the Rootbot. Overtime it consumes ever more ram until the script is killed. Ideally we should rewrite the script or monitor it using systemd so that the script is automatically restarted when it crashes. This has not risen to the top of the todo list so the current work around is as follows:\n\nAbout half way through an experiment, ssh into the rootbot’s pi.\nCheck that pictures aren’t currently being taken by looking at the timestamps in the pictures directory (ls -lh ~/Pictures)\nFind the process id number (PID) for the script (\nrootBotPhotoScript_23Feb2021.py\n) (e.g. with top or htop)\nEnd the process with kill $pid where $pid is the value from step 3.\ngo to the folder with the script (cd \\~/rootbot/jupyter_notebooks).\nRestart the rootbot script (rootBotPhotoScript_23Feb2021.py). Don’t be fooled by the shebang line, python3 must be explicitly used. The command to do this is nohup python3 rootBotPhotoScript_23Feb2021.py &&. nohup and && ensure the process continues in the background after the terminal is closed. python3 ensures the system’s default python (2) isn’t used. The full path need not be provided for the script because the present working directory is the /jupyter_notebooks folder."
  }
]